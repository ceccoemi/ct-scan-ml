{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from model import build_encoder, build_autoencoder\n",
    "from data import example_to_tensor, normalize, add_channel_axis, train_test_split\n",
    "from utils import duplicate_iterator, plot_slice, plot_animated_volume\n",
    "from config import allocate_gpu_memory_only_when_needed, data_root_dir, seed\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "#\n",
    "# policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "##policy = mixed_precision.Policy(\"float32\")\n",
    "# mixed_precision.set_policy(policy)\n",
    "# print(\"Compute dtype: %s\" % policy.compute_dtype)\n",
    "# print(\"Variable dtype: %s\" % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "downscaling = 2\n",
    "if downscaling == 4:\n",
    "    input_shape = (24, 128, 128, 1)\n",
    "    neg_tfrecord_glob = \"covid-neg-0.25/*.tfrecord\"\n",
    "    pos_tfrecord_glob = \"covid-pos-0.25/*.tfrecord\"\n",
    "elif downscaling == 2:\n",
    "    input_shape = (48, 256, 256, 1)\n",
    "    neg_tfrecord_glob = \"covid-neg-0.25/*.tfrecord\"\n",
    "    pos_tfrecord_glob = \"covid-pos-0.25/*.tfrecord\"\n",
    "elif downscaling == 1:\n",
    "    input_shape = (96, 512, 512, 1)\n",
    "    neg_tfrecord_glob = \"covid-neg-0.25/*.tfrecord\"\n",
    "    pos_tfrecord_glob = \"covid-pos-0.25/*.tfrecord\"\n",
    "else:\n",
    "    raise RuntimeError(\"Downscaling not supported\")\n",
    "\n",
    "encoder_num_filters = [32, 64, 128]\n",
    "epochs = 1000\n",
    "patience = 20\n",
    "batch_size = 2\n",
    "learning_rate = 0.00001  # very low learning rate for transfer learning\n",
    "val_perc = 0.1  # percentage from the already splitted training test\n",
    "test_perc = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative samples: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: (None, None, None, 1), types: tf.float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tfrecord_fnames = [str(p) for p in Path(data_root_dir).glob(neg_tfrecord_glob)]\n",
    "neg_x = (\n",
    "    tf.data.TFRecordDataset(neg_tfrecord_fnames)\n",
    "    .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(add_channel_axis, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "num_neg = sum(1 for _ in neg_x)\n",
    "print(f\"Number of negative samples: {num_neg}\")\n",
    "neg_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: (None, None, None, 1), types: tf.float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tfrecord_fnames = [str(p) for p in Path(data_root_dir).glob(pos_tfrecord_glob)]\n",
    "pos_x = (\n",
    "    tf.data.TFRecordDataset(pos_tfrecord_fnames)\n",
    "    .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(add_channel_axis, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "num_pos = sum(1 for _ in pos_x)\n",
    "print(f\"Number of positive samples: {num_pos}\")\n",
    "pos_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, None, None, 1), (1,)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_y = tf.data.Dataset.from_tensors(tf.constant([0], dtype=tf.int8)).repeat(num_neg)\n",
    "neg_dataset = tf.data.Dataset.zip((neg_x, neg_y))\n",
    "neg_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, None, None, 1), (1,)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_y = tf.data.Dataset.from_tensors(tf.constant([1], dtype=tf.int8)).repeat(num_pos)\n",
    "pos_dataset = tf.data.Dataset.zip((pos_x, pos_y))\n",
    "pos_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((2, 48, 256, 256, 1), (2, 1)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = neg_dataset.concatenate(pos_dataset)\n",
    "dataset, test_dataset = train_test_split(\n",
    "    dataset,\n",
    "    test_perc=test_perc,\n",
    "    cardinality=None,\n",
    "    seed=seed,\n",
    ")\n",
    "test_dataset = test_dataset.padded_batch(1, (input_shape, (1,)))\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    dataset,\n",
    "    test_perc=val_perc,\n",
    "    cardinality=None,\n",
    "    seed=seed,\n",
    ")\n",
    "val_dataset = (\n",
    "    val_dataset.padded_batch(batch_size, (input_shape, (1,)), drop_remainder=True)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.padded_batch(batch_size, (input_shape, (1,)), drop_remainder=True)\n",
    "    .cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=64, reshuffle_each_iteration=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 26, 1: 22})\n",
      "Counter({1: 2, 0: 2})\n",
      "Counter({1: 5, 0: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(label.numpy()[0] for _, label in train_dataset.unbatch()))\n",
    "print(Counter(label.numpy()[0] for _, label in val_dataset.unbatch()))\n",
    "print(Counter(label.numpy()[0] for _, label in test_dataset.unbatch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 48, 256, 256, 1)] 0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 48, 256, 256, 64)  1792      \n",
      "_________________________________________________________________\n",
      "alpha_dropout (AlphaDropout) (None, 48, 256, 256, 64)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 24, 128, 128, 64)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 24, 128, 128, 128) 221312    \n",
      "_________________________________________________________________\n",
      "alpha_dropout_1 (AlphaDropou (None, 24, 128, 128, 128) 0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 12, 64, 64, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 12, 64, 64, 256)   884992    \n",
      "_________________________________________________________________\n",
      "alpha_dropout_2 (AlphaDropou (None, 12, 64, 64, 256)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 6, 32, 32, 256)    0         \n",
      "=================================================================\n",
      "Total params: 1,108,096\n",
      "Trainable params: 1,108,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained = False\n",
    "if pretrained:\n",
    "    encoder = keras.models.load_model(\n",
    "        \"models/autoencoder-20201023-112618.h5\"\n",
    "    ).get_layer(\"encoder\")\n",
    "    encoder.trainable = False  # free the pre-trained encoder\n",
    "else:\n",
    "    encoder = build_encoder(input_shape)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Functional)         (None, 6, 32, 32, 256)    1108096   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1572864)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               805306880 \n",
      "_________________________________________________________________\n",
      "alpha_dropout_3 (AlphaDropou (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 806,415,489\n",
      "Trainable params: 806,415,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = keras.Sequential(\n",
    "    [\n",
    "        encoder,\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(\n",
    "            512,\n",
    "            kernel_initializer=\"lecun_normal\",\n",
    "            bias_initializer=\"lecun_normal\",\n",
    "            activation=\"selu\",\n",
    "        ),\n",
    "        keras.layers.AlphaDropout(0.3),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"cnn\",\n",
    ")\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "      2/Unknown - 0s 208ms/step - loss: 6.4859 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0117s vs `on_train_batch_end` time: 0.4037s). Check your callbacks.\n",
      "     24/Unknown - 9s 395ms/step - loss: 5.2714 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0094s vs `on_test_batch_end` time: 0.0810s). Check your callbacks.\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50000, saving model to models/20201023-144533.h5\n"
     ]
    }
   ],
   "source": [
    "cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        # keras.metrics.TruePositives(name=\"tp\"),\n",
    "        # keras.metrics.FalsePositives(name=\"fp\"),\n",
    "        # keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "        # keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "        keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        # keras.metrics.Precision(name=\"precision\"),\n",
    "        # keras.metrics.Recall(name=\"recall\"),\n",
    "        # keras.metrics.AUC(name=\"auc\"),\n",
    "    ],\n",
    ")\n",
    "monitor_metric = \"val_accuracy\"\n",
    "\n",
    "start_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "best_checkpoint = f\"models/{'pretrained-' if pretrained else ''}{start_time}.h5\"\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    best_checkpoint, monitor=monitor_metric, mode=\"max\", verbose=1, save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=patience, mode=\"max\"\n",
    ")\n",
    "log_dir = f\"logs/{'pretrained-' if pretrained else ''}{start_time}\"\n",
    "file_writer = tf.summary.create_file_writer(log_dir)\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=f\"logs/{'pretrained-' if pretrained else ''}{start_time}\",\n",
    "    histogram_freq=1,\n",
    "    write_graph=False,\n",
    "    profile_batch=0,\n",
    ")\n",
    "cnn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb],\n",
    ")\n",
    "with file_writer.as_default():\n",
    "    tf.summary.text(\n",
    "        \"Hyperparameters\",\n",
    "        f\"{downscaling=}; \"\n",
    "        f\"{encoder_num_filters=}; \"\n",
    "        f\"{epochs=}; \"\n",
    "        f\"{patience=}; \"\n",
    "        f\"{batch_size=}; \"\n",
    "        f\"{learning_rate=}; \"\n",
    "        f\"{unsupervised_val_perc=}\",\n",
    "        step=0,\n",
    "    )\n",
    "cnn = keras.models.load_model(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.models.load_model(\"models/20201021-213411.h5\")\n",
    "cnn.evaluate(test_dataset, verbose=0, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.models.load_model(\"models/pretrained-20201021-230015.h5\")\n",
    "cnn.evaluate(test_dataset, verbose=0, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_dataset.skip(6)))\n",
    "prediction = cnn(x, training=False)\n",
    "print(f\"real: {y.numpy()}, prediction: {prediction.numpy()}\")\n",
    "plot_animated_volume(x[0, :], fps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
