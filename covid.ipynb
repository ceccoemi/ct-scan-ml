{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "from data import (\n",
    "    tfrecord_labeled_dataset,\n",
    "    tfrecord_dataset,\n",
    "    normalize,\n",
    "    normalize_labeled,\n",
    "    scaler,\n",
    "    train_test_split,\n",
    "    kfolds,\n",
    ")\n",
    "from layers import SeluConv3D, SeluDense\n",
    "from plot import plot_volume_animation, plot_loss_history, plot_regression_results\n",
    "from config import (\n",
    "    CT_0_TFRECORD,\n",
    "    CT_1_TFRECORD,\n",
    "    CT_2_TFRECORD,\n",
    "    CT_3_TFRECORD,\n",
    "    CT_4_TFRECORD,\n",
    "    COVID_NEG_TFRECORD,\n",
    "    COVID_POS_TFRECORD,\n",
    "    SCAN_SHAPE,\n",
    "    LIDC_NUM_NODULES_TFRECORD,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 7]\n",
    "matplotlib.rcParams.update({\"font.size\": 15})\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIDC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the LIDC dataset. Its elements are of the form:\n",
    "    (scan, number_of_nodules)\n",
    "\n",
    "Where scan is a 3D volume of size (64, 192, 224, 1)\n",
    "\"\"\"\n",
    "lidc_dataset = tfrecord_labeled_dataset(LIDC_NUM_NODULES_TFRECORD)\n",
    "lidc_dataset = lidc_dataset.map(\n",
    "    normalize_labeled, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "# lidc_samples = sum(1 for _ in tqdm(lidc_dataset))\n",
    "lidc_samples = 293\n",
    "# lidc_samples = 797\n",
    "print(f\"{lidc_samples = }\")\n",
    "lidc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this to have an idea of the elements of the lidc_dataset.\n",
    "\"\"\"\n",
    "scan, num_nodules = next(iter(lidc_dataset))\n",
    "print(f\"{num_nodules = }\")\n",
    "print(scan.shape)\n",
    "plot_volume_animation(scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regression_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.InputLayer(SCAN_SHAPE, name=\"input_layer\"),\n",
    "            SeluConv3D(filters=32, kernel_size=3, name=\"selu_conv3d_1\"),\n",
    "            keras.layers.MaxPool3D(2, name=\"maxpool3d_1\"),\n",
    "            SeluConv3D(filters=64, kernel_size=3, name=\"selu_conv3d_2\"),\n",
    "            keras.layers.MaxPool3D(2, name=\"maxpool3d_2\"),\n",
    "            SeluConv3D(filters=128, kernel_size=3, name=\"selu_conv3d_3\"),\n",
    "            keras.layers.MaxPool3D(2, name=\"maxpool3d_3\"),\n",
    "            SeluConv3D(filters=256, kernel_size=3, name=\"selu_conv3d_4\"),\n",
    "            keras.layers.MaxPool3D(2, name=\"maxpool3d_4\"),\n",
    "            keras.layers.Flatten(name=\"flatten\"),\n",
    "            keras.layers.Dense(1, name=\"final_dense\"),\n",
    "        ],\n",
    "        name=\"3d_cnn\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "m = build_regression_model()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_perc = 0.2\n",
    "learning_rate = 1e-5\n",
    "batch_size = 8\n",
    "patience = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_test_split(\n",
    "    lidc_dataset, test_perc=val_perc, cardinality=lidc_samples\n",
    ")\n",
    "val_dataset = (\n",
    "    val_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a regression model for the number of nodules on the LIDC dataset.\n",
    "Save the final model in models/lidc-num-nodules-3dcnn.h5\n",
    "\"\"\"\n",
    "cnn = build_regression_model()\n",
    "cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    ")\n",
    "history = cnn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=1000,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "cnn.save(\"models/lidc-num-nodules-3dcnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Plot the train and validation loss of the previous training\"\n",
    "plot_loss_history(history)\n",
    "# plt.savefig(\"num-nodules-regression-loss.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn = build_regression_model()\n",
    "train_real_values = np.float32(\n",
    "    [label.numpy()[0] for _, label in train_dataset.unbatch()]\n",
    ")\n",
    "train_real_values = (train_real_values - train_real_values.min()) / (\n",
    "    train_real_values.max() - train_real_values.min()\n",
    ")\n",
    "\n",
    "train_predicted_values = np.float32(\n",
    "    [\n",
    "        cnn(x, training=False).numpy()[0][0]\n",
    "        for x, _ in train_dataset.unbatch().batch(1).as_numpy_iterator()\n",
    "    ]\n",
    ")\n",
    "train_predicted_values = (train_predicted_values - train_predicted_values.min()) / (\n",
    "    train_predicted_values.max() - train_predicted_values.min()\n",
    ")\n",
    "\n",
    "val_real_values = np.float32([label.numpy()[0] for _, label in val_dataset.unbatch()])\n",
    "val_real_values = (val_real_values - val_real_values.min()) / (\n",
    "    val_real_values.max() - val_real_values.min()\n",
    ")\n",
    "\n",
    "val_predicted_values = np.float32(\n",
    "    [\n",
    "        cnn(x, training=False).numpy()[0][0]\n",
    "        for x, _ in val_dataset.unbatch().batch(1).as_numpy_iterator()\n",
    "    ]\n",
    ")\n",
    "val_predicted_values = (val_predicted_values - val_predicted_values.min()) / (\n",
    "    val_predicted_values.max() - val_predicted_values.min()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Print the number of nodules and its regression prediction on the validation set\"\n",
    "# cnn = keras.models.load_model(\"models/\")\n",
    "for x, y in val_dataset.unbatch().batch(1).as_numpy_iterator():\n",
    "    print(f\"real: {y}\")\n",
    "    print(f\"predicted: {cnn(x, training=False).numpy()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Covid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(x):\n",
    "    def numpy_transform(x):\n",
    "        qt = QuantileTransformer()\n",
    "        raw = qt.fit_transform(x.reshape((-1, 1))).reshape(x.shape)\n",
    "        return 1 / (1 + np.exp(-(raw - 0.7) / 0.2))\n",
    "\n",
    "    scaled_x = tf.numpy_function(numpy_transform, [x], tf.float32)\n",
    "    scaled_x.set_shape([64, SCAN_SHAPE[1], SCAN_SHAPE[2], SCAN_SHAPE[3]])\n",
    "    return scaled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_samples = 254\n",
      "pos_samples = 856\n",
      "covid_samples = 1110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ConcatenateDataset shapes: ((None, None, None, None), (1,)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_x = tfrecord_dataset([CT_0_TFRECORD]).map(\n",
    "    normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "# neg_samples = sum(1 for _ in tqdm(neg_x))\n",
    "neg_samples = 254  # CT-0\n",
    "print(f\"{neg_samples = }\")\n",
    "neg_dataset = tf.data.Dataset.zip(\n",
    "    (neg_x, tf.data.Dataset.from_tensor_slices(np.int8([[0]])).repeat(neg_samples))\n",
    ")\n",
    "# assert sum(1 for _ in tqdm(neg_dataset)) == neg_samples\n",
    "\n",
    "pos_x = tfrecord_dataset(\n",
    "    [CT_1_TFRECORD, CT_2_TFRECORD, CT_3_TFRECORD, CT_4_TFRECORD]\n",
    ").map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# pos_samples = sum(1 for _ in tqdm(pos_x))\n",
    "pos_samples = 856  # CT-1 + CT-2 + CT-3 + CT-4\n",
    "# pos_samples = 127  # CT-2 + CT-3 + CT-4\n",
    "print(f\"{pos_samples = }\")\n",
    "pos_dataset = tf.data.Dataset.zip(\n",
    "    (pos_x, tf.data.Dataset.from_tensor_slices(np.int8([[1]])).repeat(pos_samples))\n",
    ")\n",
    "# assert sum(1 for _ in tqdm(pos_dataset)) == pos_samples\n",
    "\n",
    "covid_dataset = neg_dataset.concatenate(pos_dataset)\n",
    "covid_samples = neg_samples + pos_samples\n",
    "# assert sum(1 for _ in tqdm(covid_dataset)) == covid_samples\n",
    "print(f\"{covid_samples = }\")\n",
    "covid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this to have an idea of the elements of the covid_dataset.\n",
    "At this point, covid_dataset is not shuffled, so the elements are\n",
    "in order CT-0, CT-1, CT-2, CT-3, CT-4.\n",
    "\n",
    "For example, to see a CT-2 scan you must run .skip(950)\n",
    "\"\"\"\n",
    "scan, label = next(iter(covid_dataset.skip(950)))\n",
    "print(f\"{label = }\")\n",
    "#plot_volume_animation(scan, axis=\"y\")\n",
    "plot_slice(scan, axis=\"y\", index=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_3d_cnn():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.InputLayer(SCAN_SHAPE, name=\"input_layer\"),\n",
    "            SeluConv3D(filters=32, kernel_size=3, name=\"selu_conv3d_1\"),\n",
    "            keras.layers.MaxPool3D(2, name=\"maxpool3d_1\"),\n",
    "            SeluConv3D(filters=64, kernel_size=3, name=\"selu_conv3d_2\"),\n",
    "            keras.layers.MaxPool3D(2, name=\"maxpool3d_2\"),\n",
    "            SeluConv3D(filters=128, kernel_size=3, name=\"selu_conv3d_3\"),\n",
    "            keras.layers.MaxPool3D(2, name=\"maxpool3d_3\"),\n",
    "            SeluConv3D(filters=256, kernel_size=3, name=\"selu_conv3d_4\"),\n",
    "            keras.layers.MaxPool3D(2, name=\"maxpool3d_4\"),\n",
    "            keras.layers.Flatten(name=\"flatten\"),\n",
    "            keras.layers.Dense(1, activation=\"sigmoid\", name=\"final_dense\"),\n",
    "        ],\n",
    "        name=\"3d_cnn\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "m = build_3d_cnn()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pretrained_3d_cnn(freezing=True):\n",
    "    pretrained_3d_cnn = keras.models.load_model(\"models/lidc-num-nodules-3dcnn.h5\")\n",
    "    pretrained_3d_cnn.pop()  # remove last dense layer\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.InputLayer(SCAN_SHAPE, name=\"pretrained_input\"),\n",
    "            pretrained_3d_cnn,\n",
    "            keras.layers.Dense(1, activation=\"sigmoid\", name=\"final_dense\"),\n",
    "        ],\n",
    "        name=\"pretrained_3d_cnn\",\n",
    "    )\n",
    "    pretrained_3d_cnn.trainable = not freezing\n",
    "    return model\n",
    "\n",
    "\n",
    "m = build_pretrained_3d_cnn()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perc = 0.1\n",
    "val_perc = 0.1\n",
    "learning_rate = 1e-5\n",
    "batch_size = 8\n",
    "patience = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_dataset, test_dataset = train_test_split(\n",
    "    covid_dataset, test_perc=test_perc, cardinality=covid_samples\n",
    ")\n",
    "train_dataset, val_dataset = train_test_split(trainval_dataset, test_perc=val_perc)\n",
    "test_dataset = test_dataset.batch(1)\n",
    "val_dataset = (\n",
    "    val_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Train a model to predict covid/non-covid\"\n",
    "cnn = build_3d_cnn()\n",
    "# cnn = build_pretrained_3d_cnn()\n",
    "cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        keras.metrics.TruePositives(name=\"tp\"),\n",
    "        keras.metrics.FalsePositives(name=\"fp\"),\n",
    "        keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "        keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "        keras.metrics.AUC(name=\"auc\"),\n",
    "        keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "    ],\n",
    ")\n",
    "history = cnn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=1000,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn.evaluate(test_dataset, return_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 5\n",
    "fracs = (1, 2, 5, 7, 10)\n",
    "test_perc = 0.1\n",
    "val_perc = 0.1\n",
    "learning_rate = 1e-5\n",
    "batch_size = 8\n",
    "patience = 10\n",
    "num_epochs = 1000\n",
    "metrics = [\n",
    "    keras.metrics.AUC(name=\"auc\", num_thresholds=1000),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model from scratch and a pre-trained model on the covid_dataset.\n",
    "\n",
    "The models are trained on 1/`fracs` of the original covid_dataset.\n",
    "The process is repeated r `rounds`.\n",
    "\"\"\"\n",
    "data = defaultdict(list)\n",
    "wo_pt_histories = []\n",
    "w_pt_histories = []\n",
    "w_pt_freezing_histories = []\n",
    "for r in tqdm(range(rounds)):\n",
    "    print(f\" {r = } \".center(50, \"=\"))\n",
    "    full_train_dataset, test_dataset = train_test_split(\n",
    "        covid_dataset, test_perc=test_perc, cardinality=covid_samples\n",
    "    )\n",
    "    # print(f\"Test size: {sum(1 for _ in test_dataset)}\")\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    full_train_dataset, val_dataset = train_test_split(\n",
    "        full_train_dataset, test_perc=val_perc\n",
    "    )\n",
    "    # print(f\"Val size: {sum(1 for _ in val_dataset)}\")\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    wo_pt_frac_histories = []\n",
    "    w_pt_frac_histories = []\n",
    "    w_pt_freezing_frac_histories = []\n",
    "    for frac in fracs:\n",
    "        print(f\" {frac = } \".center(25, \"=\"))\n",
    "        train_dataset = (\n",
    "            full_train_dataset.shuffle(1024, reshuffle_each_iteration=False)\n",
    "            .shard(num_shards=frac, index=0)\n",
    "            .cache()  # must be called before shuffle\n",
    "            .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        )\n",
    "        # print(f\"Train size: {sum(1 for _ in train_dataset.unbatch())}\")\n",
    "\n",
    "        cnn = build_3d_cnn()\n",
    "        cnn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate),\n",
    "            loss=keras.losses.BinaryCrossentropy(),\n",
    "            metrics=metrics,\n",
    "        )\n",
    "        history = cnn.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=num_epochs,\n",
    "            verbose=0,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    patience=patience,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        wo_pt_frac_histories.append(history)\n",
    "        test_metrics = cnn.evaluate(test_dataset, return_dict=True, verbose=False)\n",
    "        wo_pt_auc = test_metrics[\"auc\"]\n",
    "        print(f\"{wo_pt_auc = }\")\n",
    "        data[\"nn_type\"].append(\"w/o pre-training\")\n",
    "        data[\"train_size_fraction\"].append(frac)\n",
    "        data[\"metric\"].append(\"AUC\")\n",
    "        data[\"metric_value\"].append(wo_pt_auc)\n",
    "\n",
    "        cnn = build_pretrained_3d_cnn(freezing=False)\n",
    "        cnn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate),\n",
    "            loss=keras.losses.BinaryCrossentropy(),\n",
    "            metrics=metrics,\n",
    "        )\n",
    "        history = cnn.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=num_epochs,\n",
    "            verbose=0,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    patience=patience,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        w_pt_frac_histories.append(history)\n",
    "        test_metrics = cnn.evaluate(test_dataset, return_dict=True, verbose=False)\n",
    "        w_pt_auc = test_metrics[\"auc\"]\n",
    "        print(f\"{w_pt_auc = }\")\n",
    "        data[\"nn_type\"].append(\"w/ pre-training (w/o freezing)\")\n",
    "        data[\"train_size_fraction\"].append(frac)\n",
    "        data[\"metric\"].append(\"AUC\")\n",
    "        data[\"metric_value\"].append(w_pt_auc)\n",
    "\n",
    "    wo_pt_histories.append(wo_pt_frac_histories)\n",
    "    w_pt_histories.append(w_pt_frac_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Plot the results of the previous trainings\"\n",
    "g = sns.catplot(\n",
    "    x=\"train_size_fraction\",\n",
    "    y=\"metric_value\",\n",
    "    col=\"metric\",\n",
    "    hue=\"nn_type\",\n",
    "    kind=\"point\",\n",
    "    data=pd.DataFrame(data),\n",
    ")\n",
    "# for ax in g.axes[0]:\n",
    "#    ax.axhline(color=\"r\", ls=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_index = 0\n",
    "frac_index = 0\n",
    "plt.plot(\n",
    "    wo_pt_histories[round_index][frac_index].history[\"loss\"],\n",
    "    \"--\",\n",
    "    label=\"w/o pre-training - train loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    wo_pt_histories[round_index][frac_index].history[\"val_loss\"],\n",
    "    label=\"w/o pre-training - val loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    w_pt_histories[round_index][frac_index].history[\"loss\"],\n",
    "    \"--\",\n",
    "    label=\"w/ pre-training, w/o conv freezing - train loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    w_pt_histories[round_index][frac_index].history[\"val_loss\"],\n",
    "    label=\"w/ pre-training, w/o conv freezing - val loss\",\n",
    ")\n",
    "# plt.plot(\n",
    "#    w_pt_freezing_histories[round_index][frac_index].history[\"loss\"],\n",
    "#    \"--\",\n",
    "#    label=\"w/ pre-training, w/ conv freezing - train loss\",\n",
    "# )\n",
    "# plt.plot(\n",
    "#    w_pt_freezing_histories[round_index][frac_index].history[\"val_loss\"],\n",
    "#    label=\"w/ pre-training, w/ conv freezing - val loss\",\n",
    "# )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = defaultdict(list)\n",
    "wo_pt_histories = []\n",
    "w_pt_histories = []\n",
    "for _ in tqdm(range(rounds)):\n",
    "    trainval_dataset, test_dataset = train_test_split(\n",
    "        covid_dataset, test_perc=test_perc, cardinality=covid_samples\n",
    "    )\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    full_train_dataset, val_dataset = train_test_split(\n",
    "        trainval_dataset, test_perc=val_perc\n",
    "    )\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    wo_pt_frac_histories = []\n",
    "    w_pt_frac_histories = []\n",
    "    for frac in fracs:\n",
    "        train_dataset = (\n",
    "            full_train_dataset.shuffle(1024, reshuffle_each_iteration=False)\n",
    "            .shard(num_shards=frac, index=0)\n",
    "            .cache()  # must be called before shuffle\n",
    "            .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "        cnn = build_3d_cnn()\n",
    "        cnn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate),\n",
    "            loss=keras.losses.BinaryCrossentropy(),\n",
    "            metrics=metrics,\n",
    "        )\n",
    "        history = cnn.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=num_epochs,\n",
    "            verbose=0,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    patience=patience,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        wo_pt_frac_histories.append(history)\n",
    "        test_metrics = cnn.evaluate(test_dataset, return_dict=True, verbose=False)\n",
    "        data[\"nn_type\"].append(\"w/o pretraining\")\n",
    "        data[\"train_size_fraction\"].append(frac)\n",
    "        data[\"metric\"].append(\"AUC\")\n",
    "        data[\"metric_value\"].append(test_metrics[\"auc\"])\n",
    "\n",
    "        cnn = build_pretrained_3d_cnn(True)\n",
    "        cnn.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate),\n",
    "            loss=keras.losses.BinaryCrossentropy(),\n",
    "            metrics=metrics,\n",
    "        )\n",
    "        history = cnn.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=num_epochs,\n",
    "            verbose=0,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    patience=patience,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        w_pt_frac_histories.append(history)\n",
    "        test_metrics = cnn.evaluate(test_dataset, return_dict=True, verbose=False)\n",
    "        data[\"nn_type\"].append(\"w/ pretraining (conv freezing)\")\n",
    "        data[\"train_size_fraction\"].append(frac)\n",
    "        data[\"metric\"].append(\"AUC\")\n",
    "        data[\"metric_value\"].append(test_metrics[\"auc\"])\n",
    "    wo_pt_histories.append(wo_pt_frac_histories)\n",
    "    w_pt_histories.append(w_pt_frac_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    x=\"train_size_fraction\",\n",
    "    y=\"metric_value\",\n",
    "    hue=\"nn_type\",\n",
    "    col=\"metric\",\n",
    "    kind=\"point\",\n",
    "    data=pd.DataFrame(data),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_index = 1\n",
    "frac_index = 0\n",
    "plt.plot(\n",
    "    wo_pt_histories[round_index][frac_index].history[\"loss\"],\n",
    "    \"--\",\n",
    "    label=\"w/o pre-training - train loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    wo_pt_histories[round_index][frac_index].history[\"val_loss\"],\n",
    "    label=\"w/o pre-training - val loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    w_pt_histories[round_index][frac_index].history[\"loss\"],\n",
    "    \"--\",\n",
    "    label=\"w/ pre-training, w/ conv freezing - train loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    w_pt_histories[round_index][frac_index].history[\"val_loss\"],\n",
    "    label=\"w/ pre-training, w/ conv freezing - val loss\",\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "val_perc = 0.1\n",
    "learning_rate = 1e-5\n",
    "batch_size = 8\n",
    "patience = 10\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_pt_histories = []\n",
    "wo_pt_metrics = [\n",
    "    keras.metrics.TruePositives(name=\"tp\"),\n",
    "    keras.metrics.FalsePositives(name=\"fp\"),\n",
    "    keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "    keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "    keras.metrics.Precision(name=\"precision\"),\n",
    "    keras.metrics.Recall(name=\"recall\"),\n",
    "    keras.metrics.AUC(name=\"auc\", num_thresholds=1000),\n",
    "    keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "]\n",
    "w_pt_histories = []\n",
    "w_pt_metrics = [\n",
    "    keras.metrics.TruePositives(name=\"tp\"),\n",
    "    keras.metrics.FalsePositives(name=\"fp\"),\n",
    "    keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "    keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "    keras.metrics.Precision(name=\"precision\"),\n",
    "    keras.metrics.Recall(name=\"recall\"),\n",
    "    keras.metrics.AUC(name=\"auc\", num_thresholds=1000),\n",
    "    keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "]\n",
    "for fold_id, (train_dataset, test_dataset) in tqdm(\n",
    "    enumerate(kfolds(k, covid_dataset, cardinality=covid_samples)), total=k\n",
    "):\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "    train_dataset, val_dataset = train_test_split(train_dataset, test_perc=val_perc)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    train_dataset = (\n",
    "        train_dataset.cache()  # must be called before shuffle\n",
    "        .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    cnn = build_3d_cnn()\n",
    "    cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "    )\n",
    "    history = cnn.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=num_epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=patience,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    wo_pt_histories.append(history)\n",
    "    for test_x, test_y in test_dataset.as_numpy_iterator():\n",
    "        pred_y = cnn(test_x, training=False)\n",
    "        for metric in wo_pt_metrics:\n",
    "            metric.update_state(test_y, pred_y)\n",
    "\n",
    "    cnn = build_pretrained_3d_cnn(True)\n",
    "    cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-5),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "    )\n",
    "    history = cnn.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=num_epochs,\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=patience,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    w_pt_histories.append(history)\n",
    "    for test_x, test_y in test_dataset.as_numpy_iterator():\n",
    "        pred_y = cnn(test_x, training=False)\n",
    "        for metric in w_pt_metrics:\n",
    "            metric.update_state(test_y, pred_y)\n",
    "\n",
    "print(\" total \".center(50, \"=\"))\n",
    "print(\"Without pretraining: \")\n",
    "for metric in wo_pt_metrics:\n",
    "    print(f\"{metric.name}: {metric.result()}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"With pretraining (w/ conv freezing): \")\n",
    "for metric in w_pt_metrics:\n",
    "    print(f\"{metric.name}: {metric.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "plt.plot(\n",
    "    wo_pt_histories[index].history[\"loss\"], \"--\", label=\"w/o pre-training - train loss\"\n",
    ")\n",
    "plt.plot(\n",
    "    wo_pt_histories[index].history[\"val_loss\"], label=\"w/o pre-training - val loss\"\n",
    ")\n",
    "plt.plot(\n",
    "    w_pt_histories[index].history[\"loss\"],\n",
    "    \"--\",\n",
    "    label=\"w/ pre-training, w/o conv freezing - train loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    w_pt_histories[index].history[\"val_loss\"],\n",
    "    label=\"w/ pre-training, w/o conv freezing - val loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    w_pt_conv_histories[index].history[\"loss\"],\n",
    "    \"--\",\n",
    "    label=\"w/ pre-training, w/ conv freezing - train loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    w_pt_conv_histories[index].history[\"val_loss\"],\n",
    "    label=\"w/ pre-training, w/ conv freezing - val loss\",\n",
    ")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
