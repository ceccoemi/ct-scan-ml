{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from statistics import mean\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data import tfrecord_dataset, classification_dataset, train_test_split\n",
    "from train import train_model, get_best_num_epochs\n",
    "from layers import SeluConv3D, SeluDense\n",
    "from plot import plot_slice, plot_volume_animation\n",
    "from config import (\n",
    "    LIDC_SMALL_NEG_TFRECORD,\n",
    "    LIDC_BIG_NEG_TFRECORD,\n",
    "    LIDC_SMALL_POS_TFRECORD,\n",
    "    LIDC_BIG_POS_TFRECORD,\n",
    "    LIDC_SMALL_UNLABELED_TFRECORD,\n",
    "    SPIE_SMALL_NEG_TFRECORD,\n",
    "    SPIE_BIG_NEG_TFRECORD,\n",
    "    SPIE_SMALL_POS_TFRECORD,\n",
    "    SPIE_BIG_POS_TFRECORD,\n",
    "    SMALL_PATCH_SHAPE,\n",
    "    BIG_PATCH_SHAPE,\n",
    "    SEED,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1139"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_dataset = tfrecord_dataset(\n",
    "    [LIDC_SMALL_NEG_TFRECORD, LIDC_SMALL_POS_TFRECORD, LIDC_SMALL_UNLABELED_TFRECORD]\n",
    ")\n",
    "unlabeled_dataset = tf.data.Dataset.zip((unlabeled_dataset, unlabeled_dataset))\n",
    "unlabeled_samples = sum(1 for _ in unlabeled_dataset)\n",
    "unlabeled_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "val_perc = 0.1\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, None, None, None, None), (None, None, None, None, None)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset = train_test_split(\n",
    "    unlabeled_dataset,\n",
    "    test_perc=val_perc,\n",
    "    cardinality=unlabeled_samples,\n",
    ")\n",
    "val_dataset = (\n",
    "    val_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder():\n",
    "    encoder = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.InputLayer(SMALL_PATCH_SHAPE, name=\"encoder_input\"),\n",
    "            SeluConv3D(filters=32, kernel_size=3, name=\"encoder_selu_conv3d_1\"),\n",
    "            keras.layers.MaxPooling3D((1, 2, 2), name=\"maxpool_1\"),\n",
    "            SeluConv3D(filters=64, kernel_size=3, name=\"encoder_selu_conv3d_2\"),\n",
    "            keras.layers.MaxPooling3D((1, 2, 2), name=\"maxpool_2\"),\n",
    "            SeluConv3D(filters=128, kernel_size=3, name=\"encoder_selu_conv3d_3\"),\n",
    "            keras.layers.MaxPooling3D((1, 2, 2), name=\"maxpool_3\"),\n",
    "            SeluConv3D(filters=256, kernel_size=3, name=\"encoder_selu_conv3d_4\"),\n",
    "        ],\n",
    "        name=\"encoder\",\n",
    "    )\n",
    "    decoder = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.InputLayer(encoder.output_shape[1:], name=\"decoder_input\"),\n",
    "            SeluConv3D(filters=256, kernel_size=3, name=\"decoder_selu_conv3d_1\"),\n",
    "            keras.layers.UpSampling3D((1, 2, 2), name=\"upsampling_2\"),\n",
    "            SeluConv3D(filters=128, kernel_size=3, name=\"decoder_selu_conv3d_2\"),\n",
    "            keras.layers.UpSampling3D((1, 2, 2), name=\"upsampling_3\"),\n",
    "            SeluConv3D(filters=64, kernel_size=3, name=\"decoder_selu_conv3d_3\"),\n",
    "            keras.layers.UpSampling3D((1, 2, 2), name=\"upsampling_4\"),\n",
    "            SeluConv3D(filters=32, kernel_size=3, name=\"decoder_selu_conv3d_4\"),\n",
    "            keras.layers.Dense(1, activation=\"sigmoid\", name=\"decoder_final_dense\"),\n",
    "        ],\n",
    "        name=\"decoder\",\n",
    "    )\n",
    "\n",
    "    autoencoder = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.InputLayer(SMALL_PATCH_SHAPE, name=\"autoencoder_input\"),\n",
    "            encoder,\n",
    "            decoder,\n",
    "        ],\n",
    "        name=\"autoencoder\",\n",
    "    )\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 30\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "65/65 [==============================] - 2s 36ms/step - loss: 0.0805 - val_loss: 0.0375\n",
      "Epoch 2/1000\n",
      "65/65 [==============================] - 2s 28ms/step - loss: 0.0503 - val_loss: 0.0328\n",
      "Epoch 3/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0468 - val_loss: 0.0300\n",
      "Epoch 4/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0447 - val_loss: 0.0287\n",
      "Epoch 5/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0435 - val_loss: 0.0278\n",
      "Epoch 6/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0427 - val_loss: 0.0272\n",
      "Epoch 7/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0421 - val_loss: 0.0269\n",
      "Epoch 8/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0417 - val_loss: 0.0262\n",
      "Epoch 9/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0413 - val_loss: 0.0259\n",
      "Epoch 10/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0410 - val_loss: 0.0255\n",
      "Epoch 11/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0407 - val_loss: 0.0253\n",
      "Epoch 12/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0405 - val_loss: 0.0252\n",
      "Epoch 13/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0403 - val_loss: 0.0249\n",
      "Epoch 14/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0401 - val_loss: 0.0248\n",
      "Epoch 15/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0400 - val_loss: 0.0250\n",
      "Epoch 16/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0399 - val_loss: 0.0246\n",
      "Epoch 17/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0397 - val_loss: 0.0244\n",
      "Epoch 18/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0396 - val_loss: 0.0242\n",
      "Epoch 19/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0394 - val_loss: 0.0244\n",
      "Epoch 20/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0394 - val_loss: 0.0242\n",
      "Epoch 21/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0392 - val_loss: 0.0241\n",
      "Epoch 22/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0391 - val_loss: 0.0239\n",
      "Epoch 23/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0390 - val_loss: 0.0241\n",
      "Epoch 24/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0390 - val_loss: 0.0238\n",
      "Epoch 25/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0389 - val_loss: 0.0237\n",
      "Epoch 26/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0388 - val_loss: 0.0236\n",
      "Epoch 27/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0387 - val_loss: 0.0236\n",
      "Epoch 28/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0387 - val_loss: 0.0235\n",
      "Epoch 29/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0386 - val_loss: 0.0236\n",
      "Epoch 30/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0386 - val_loss: 0.0234\n",
      "Epoch 31/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0385 - val_loss: 0.0233\n",
      "Epoch 32/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0384 - val_loss: 0.0233\n",
      "Epoch 33/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0384 - val_loss: 0.0234\n",
      "Epoch 34/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0383 - val_loss: 0.0233\n",
      "Epoch 35/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0384 - val_loss: 0.0231\n",
      "Epoch 36/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0382 - val_loss: 0.0231\n",
      "Epoch 37/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0381 - val_loss: 0.0231\n",
      "Epoch 38/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0381 - val_loss: 0.0230\n",
      "Epoch 39/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0381 - val_loss: 0.0231\n",
      "Epoch 40/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0381 - val_loss: 0.0230\n",
      "Epoch 41/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0380 - val_loss: 0.0230\n",
      "Epoch 42/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0379 - val_loss: 0.0229\n",
      "Epoch 43/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0379 - val_loss: 0.0230\n",
      "Epoch 44/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0379 - val_loss: 0.0231\n",
      "Epoch 45/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0379 - val_loss: 0.0232\n",
      "Epoch 46/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0378 - val_loss: 0.0228\n",
      "Epoch 47/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0378 - val_loss: 0.0229\n",
      "Epoch 48/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0377 - val_loss: 0.0228\n",
      "Epoch 49/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0377 - val_loss: 0.0227\n",
      "Epoch 50/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0377 - val_loss: 0.0227\n",
      "Epoch 51/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0377 - val_loss: 0.0226\n",
      "Epoch 52/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0377 - val_loss: 0.0228\n",
      "Epoch 53/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0376 - val_loss: 0.0226\n",
      "Epoch 54/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0375 - val_loss: 0.0225\n",
      "Epoch 55/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0375 - val_loss: 0.0226\n",
      "Epoch 56/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0375 - val_loss: 0.0225\n",
      "Epoch 57/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0375 - val_loss: 0.0225\n",
      "Epoch 58/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0374 - val_loss: 0.0225\n",
      "Epoch 59/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0375 - val_loss: 0.0224\n",
      "Epoch 60/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0374 - val_loss: 0.0224\n",
      "Epoch 61/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0383 - val_loss: 0.0248\n",
      "Epoch 62/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0378 - val_loss: 0.0224\n",
      "Epoch 63/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0373 - val_loss: 0.0224\n",
      "Epoch 64/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0373 - val_loss: 0.0223\n",
      "Epoch 65/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0373 - val_loss: 0.0223\n",
      "Epoch 66/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0372 - val_loss: 0.0223\n",
      "Epoch 67/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0372 - val_loss: 0.0222\n",
      "Epoch 68/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0372 - val_loss: 0.0223\n",
      "Epoch 69/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0372 - val_loss: 0.0222\n",
      "Epoch 70/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0372 - val_loss: 0.0222\n",
      "Epoch 71/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0372 - val_loss: 0.0223\n",
      "Epoch 72/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0371 - val_loss: 0.0222\n",
      "Epoch 73/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0371 - val_loss: 0.0222\n",
      "Epoch 74/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0371 - val_loss: 0.0222\n",
      "Epoch 75/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0371 - val_loss: 0.0221\n",
      "Epoch 76/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0371 - val_loss: 0.0221\n",
      "Epoch 77/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0370 - val_loss: 0.0221\n",
      "Epoch 78/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0370 - val_loss: 0.0221\n",
      "Epoch 79/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0370 - val_loss: 0.0221\n",
      "Epoch 80/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0370 - val_loss: 0.0220\n",
      "Epoch 81/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0370 - val_loss: 0.0221\n",
      "Epoch 82/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0369 - val_loss: 0.0220\n",
      "Epoch 83/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0369 - val_loss: 0.0220\n",
      "Epoch 84/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0369 - val_loss: 0.0221\n",
      "Epoch 85/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0369 - val_loss: 0.0219\n",
      "Epoch 86/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0369 - val_loss: 0.0221\n",
      "Epoch 87/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0369 - val_loss: 0.0220\n",
      "Epoch 88/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0369 - val_loss: 0.0219\n",
      "Epoch 89/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0369 - val_loss: 0.0219\n",
      "Epoch 90/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0368 - val_loss: 0.0220\n",
      "Epoch 91/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0368 - val_loss: 0.0219\n",
      "Epoch 92/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0368 - val_loss: 0.0218\n",
      "Epoch 93/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0367 - val_loss: 0.0219\n",
      "Epoch 94/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0367 - val_loss: 0.0218\n",
      "Epoch 95/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0367 - val_loss: 0.0220\n",
      "Epoch 96/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0368 - val_loss: 0.0218\n",
      "Epoch 97/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0367 - val_loss: 0.0218\n",
      "Epoch 98/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0367 - val_loss: 0.0218\n",
      "Epoch 99/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0366 - val_loss: 0.0218\n",
      "Epoch 100/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0367 - val_loss: 0.0219\n",
      "Epoch 101/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0367 - val_loss: 0.0218\n",
      "Epoch 102/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0366 - val_loss: 0.0217\n",
      "Epoch 103/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0366 - val_loss: 0.0217\n",
      "Epoch 104/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0366 - val_loss: 0.0217\n",
      "Epoch 105/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0366 - val_loss: 0.0217\n",
      "Epoch 106/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0365 - val_loss: 0.0218\n",
      "Epoch 107/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0365 - val_loss: 0.0217\n",
      "Epoch 108/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0365 - val_loss: 0.0217\n",
      "Epoch 109/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0365 - val_loss: 0.0217\n",
      "Epoch 110/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0365 - val_loss: 0.0216\n",
      "Epoch 111/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0365 - val_loss: 0.0216\n",
      "Epoch 112/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0365 - val_loss: 0.0217\n",
      "Epoch 113/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0365 - val_loss: 0.0216\n",
      "Epoch 114/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0365 - val_loss: 0.0216\n",
      "Epoch 115/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0364 - val_loss: 0.0216\n",
      "Epoch 116/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0364 - val_loss: 0.0216\n",
      "Epoch 117/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0364 - val_loss: 0.0215\n",
      "Epoch 118/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0364 - val_loss: 0.0215\n",
      "Epoch 119/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0364 - val_loss: 0.0216\n",
      "Epoch 120/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0365 - val_loss: 0.0217\n",
      "Epoch 121/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0364 - val_loss: 0.0215\n",
      "Epoch 122/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0363 - val_loss: 0.0215\n",
      "Epoch 123/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0364 - val_loss: 0.0215\n",
      "Epoch 124/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0364 - val_loss: 0.0215\n",
      "Epoch 125/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0363 - val_loss: 0.0214\n",
      "Epoch 126/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0363 - val_loss: 0.0215\n",
      "Epoch 127/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0365 - val_loss: 0.0214\n",
      "Epoch 128/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0363 - val_loss: 0.0214\n",
      "Epoch 129/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0216\n",
      "Epoch 130/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0363 - val_loss: 0.0214\n",
      "Epoch 131/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0214\n",
      "Epoch 132/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0214\n",
      "Epoch 133/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0214\n",
      "Epoch 134/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0214\n",
      "Epoch 135/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0214\n",
      "Epoch 136/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0215\n",
      "Epoch 137/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0214\n",
      "Epoch 138/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0214\n",
      "Epoch 139/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0361 - val_loss: 0.0214\n",
      "Epoch 140/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0215\n",
      "Epoch 141/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0213\n",
      "Epoch 142/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0361 - val_loss: 0.0214\n",
      "Epoch 143/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0213\n",
      "Epoch 144/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0361 - val_loss: 0.0213\n",
      "Epoch 145/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0361 - val_loss: 0.0213\n",
      "Epoch 146/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0361 - val_loss: 0.0214\n",
      "Epoch 147/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0213\n",
      "Epoch 148/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0361 - val_loss: 0.0213\n",
      "Epoch 149/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0361 - val_loss: 0.0213\n",
      "Epoch 150/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0213\n",
      "Epoch 151/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0214\n",
      "Epoch 152/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0213\n",
      "Epoch 153/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0213\n",
      "Epoch 154/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0213\n",
      "Epoch 155/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0213\n",
      "Epoch 156/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0212\n",
      "Epoch 157/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0212\n",
      "Epoch 158/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0361 - val_loss: 0.0213\n",
      "Epoch 159/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0213\n",
      "Epoch 160/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0212\n",
      "Epoch 162/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0212\n",
      "Epoch 163/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0212\n",
      "Epoch 164/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0212\n",
      "Epoch 165/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0212\n",
      "Epoch 166/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0212\n",
      "Epoch 167/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0212\n",
      "Epoch 168/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0211\n",
      "Epoch 169/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0360 - val_loss: 0.0211\n",
      "Epoch 170/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0212\n",
      "Epoch 171/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0211\n",
      "Epoch 172/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0211\n",
      "Epoch 173/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0211\n",
      "Epoch 174/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0211\n",
      "Epoch 175/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0211\n",
      "Epoch 176/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0213\n",
      "Epoch 177/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0211\n",
      "Epoch 178/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0212\n",
      "Epoch 179/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0212\n",
      "Epoch 180/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0211\n",
      "Epoch 181/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0211\n",
      "Epoch 182/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0213\n",
      "Epoch 183/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0359 - val_loss: 0.0212\n",
      "Epoch 184/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0211\n",
      "Epoch 185/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0211\n",
      "Epoch 186/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0210\n",
      "Epoch 187/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0210\n",
      "Epoch 188/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0210\n",
      "Epoch 189/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 190/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0211\n",
      "Epoch 191/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0210\n",
      "Epoch 192/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 193/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 194/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0358 - val_loss: 0.0210\n",
      "Epoch 195/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 196/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0211\n",
      "Epoch 197/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0362 - val_loss: 0.0211\n",
      "Epoch 198/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 199/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 200/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 201/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 202/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 203/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 204/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 205/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 206/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0210\n",
      "Epoch 207/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0210\n",
      "Epoch 208/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 209/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0210\n",
      "Epoch 210/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0357 - val_loss: 0.0209\n",
      "Epoch 211/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 212/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 213/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 214/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 215/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0210\n",
      "Epoch 216/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 217/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 218/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 219/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 220/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 221/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 222/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 223/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0210\n",
      "Epoch 224/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 225/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0211\n",
      "Epoch 226/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 227/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0209\n",
      "Epoch 228/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 229/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 230/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 231/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 232/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 233/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0211\n",
      "Epoch 234/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0208\n",
      "Epoch 235/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 236/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 237/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 238/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0356 - val_loss: 0.0208\n",
      "Epoch 239/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 240/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 241/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 242/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0208\n",
      "Epoch 243/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 244/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0208\n",
      "Epoch 245/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0208\n",
      "Epoch 246/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0208\n",
      "Epoch 247/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0208\n",
      "Epoch 248/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0208\n",
      "Epoch 249/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0208\n",
      "Epoch 250/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0210\n",
      "Epoch 251/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0208\n",
      "Epoch 252/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 253/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0208\n",
      "Epoch 254/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 255/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 256/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 257/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0209\n",
      "Epoch 258/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 259/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0209\n",
      "Epoch 260/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 261/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 262/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 263/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 264/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0210\n",
      "Epoch 265/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 266/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 267/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 268/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0207\n",
      "Epoch 269/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0207\n",
      "Epoch 270/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 271/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0207\n",
      "Epoch 272/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0208\n",
      "Epoch 273/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0207\n",
      "Epoch 274/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 275/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 276/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0211\n",
      "Epoch 277/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0207\n",
      "Epoch 278/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 279/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0208\n",
      "Epoch 280/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 281/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0210\n",
      "Epoch 282/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0355 - val_loss: 0.0207\n",
      "Epoch 283/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 284/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 285/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 286/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 287/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 288/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 289/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0208\n",
      "Epoch 290/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 291/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 292/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 293/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 294/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 295/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 296/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 297/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0211\n",
      "Epoch 298/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0354 - val_loss: 0.0212\n",
      "Epoch 299/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 300/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0208\n",
      "Epoch 301/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 302/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 303/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 304/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0207\n",
      "Epoch 305/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0207\n",
      "Epoch 306/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0207\n",
      "Epoch 307/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0207\n",
      "Epoch 308/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0207\n",
      "Epoch 309/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 310/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 311/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0207\n",
      "Epoch 312/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0207\n",
      "Epoch 313/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 314/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0207\n",
      "Epoch 315/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 316/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0207\n",
      "Epoch 317/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0208\n",
      "Epoch 318/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 319/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 320/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 321/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 322/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0207\n",
      "Epoch 323/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0207\n",
      "Epoch 324/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 325/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0207\n",
      "Epoch 326/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 327/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 328/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 329/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 330/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 331/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0208\n",
      "Epoch 332/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0208\n",
      "Epoch 333/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 334/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 335/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 336/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 337/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 338/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 339/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 340/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 341/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 342/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 343/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 344/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 345/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0207\n",
      "Epoch 346/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 347/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 348/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 349/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 350/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 351/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 352/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 353/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 354/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0352 - val_loss: 0.0206\n",
      "Epoch 355/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 356/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 357/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 358/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 359/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 360/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 361/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 362/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 363/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0207\n",
      "Epoch 364/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 365/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 366/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 367/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 368/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0207\n",
      "Epoch 369/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 370/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 371/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 372/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 373/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0205\n",
      "Epoch 374/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 375/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 376/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 377/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 378/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0205\n",
      "Epoch 379/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0208\n",
      "Epoch 380/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 381/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 382/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0205\n",
      "Epoch 383/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 384/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0205\n",
      "Epoch 385/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 386/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 387/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 388/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0206\n",
      "Epoch 389/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 390/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 391/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0205\n",
      "Epoch 392/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0207\n",
      "Epoch 393/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 394/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 395/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 396/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 397/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0206\n",
      "Epoch 398/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 399/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 400/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 401/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 402/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 403/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 404/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 405/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0206\n",
      "Epoch 406/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0205\n",
      "Epoch 407/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0206\n",
      "Epoch 408/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0205\n",
      "Epoch 409/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 410/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 411/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 412/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 413/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0206\n",
      "Epoch 414/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 415/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 416/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 417/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 418/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 419/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 420/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 421/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 422/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 423/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0206\n",
      "Epoch 424/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 425/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 426/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 427/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0206\n",
      "Epoch 428/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 429/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 430/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 431/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0206\n",
      "Epoch 432/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 433/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 434/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 435/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 436/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 437/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 438/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 439/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 440/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 441/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 442/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 443/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 444/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0211\n",
      "Epoch 445/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0351 - val_loss: 0.0205\n",
      "Epoch 446/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 447/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 448/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0206\n",
      "Epoch 449/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 450/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 451/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 452/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 453/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 454/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 455/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 456/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 457/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 458/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 459/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 460/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 461/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 462/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 463/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 464/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 465/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 466/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 467/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 468/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 469/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 470/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 471/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 472/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 473/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 474/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 475/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 476/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 477/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 478/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 479/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 480/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 481/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 482/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 483/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 484/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 485/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 486/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 487/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 488/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 489/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 490/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 491/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 492/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 493/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 494/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 495/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 496/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 497/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 498/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 499/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 500/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 501/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 502/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 503/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 504/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 505/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 506/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 507/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 508/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 509/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 510/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 511/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 512/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 513/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 514/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 515/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 516/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 517/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 518/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 519/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 520/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 521/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 522/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 523/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 524/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 525/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 526/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0350 - val_loss: 0.0205\n",
      "Epoch 527/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 528/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 529/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 530/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 531/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 532/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 533/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 534/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 535/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 536/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 537/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 538/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 539/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 540/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 541/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 542/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 543/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 544/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 545/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 546/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 547/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 548/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 549/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 550/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 551/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 552/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 553/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 554/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 555/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 556/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 557/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 558/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 559/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 560/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 561/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 562/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0206\n",
      "Epoch 563/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 564/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 565/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 566/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0207\n",
      "Epoch 567/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 568/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 569/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 570/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 571/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 572/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 573/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 574/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 575/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 576/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0205\n",
      "Epoch 577/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 578/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 579/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 580/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 581/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 582/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 583/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 584/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 585/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 586/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0205\n",
      "Epoch 587/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0204\n",
      "Epoch 588/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 589/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 590/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 591/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0204\n",
      "Epoch 592/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 593/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 594/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 595/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 596/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 597/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 598/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 599/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 600/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 601/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 602/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 603/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 604/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 605/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 606/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 607/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 608/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 609/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 610/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 611/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 612/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 613/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 614/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 615/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 616/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 617/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 618/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 619/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0208\n",
      "Epoch 620/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0348 - val_loss: 0.0205\n",
      "Epoch 621/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0349 - val_loss: 0.0205\n",
      "Epoch 622/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 623/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 624/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 625/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 626/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 627/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 628/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 629/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 630/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 631/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 632/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 633/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 634/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 635/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 636/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 637/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 638/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 639/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 640/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 641/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 642/1000\n",
      "65/65 [==============================] - 2s 29ms/step - loss: 0.0347 - val_loss: 0.0204\n",
      "Epoch 00642: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "611"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = build_autoencoder()\n",
    "autoencoder.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    ")\n",
    "num_epochs = get_best_num_epochs(\n",
    "    autoencoder,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    patience,\n",
    "    \"val_loss\",\n",
    "    verbose_training=1,\n",
    "    verbose_early_stopping=1,\n",
    ")\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, None, None, None, None), (None, None, None, None, None)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = (\n",
    "    unlabeled_dataset.cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for more epochs because the training set will be bigger\n",
    "extra_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = build_autoencoder()\n",
    "autoencoder.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    ")\n",
    "\n",
    "model_fname = f\"models/autoencoder-lidc.h5\"\n",
    "log_dir = f\"logs/autoencoder-lidc\"\n",
    "autoencoder = train_model(\n",
    "    autoencoder,\n",
    "    train_dataset,\n",
    "    num_epochs + extra_epochs,\n",
    "    model_fname,\n",
    "    log_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"1080\" height=\"504\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAB/n21kYXQAAAKvBgX//6vcRem9\n",
       "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTU3IHIyOTgwIDM0YzA2ZDEgLSBILjI2NC9NUEVHLTQg\n",
       "QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE5IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n",
       "eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n",
       "MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n",
       "PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n",
       "b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9MTYgbG9v\n",
       "a2FoZWFkX3RocmVhZHM9MiBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxh\n",
       "Y2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHly\n",
       "YW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3\n",
       "ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTEwIHNjZW5lY3V0PTQwIGludHJhX3JlZnJl\n",
       "c2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAg\n",
       "cXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAAUzmWI\n",
       "hAA///73aJ8Cm1pDeoDklcUl20+B/6tncHyP6QMAAAMAAAMAAAMAAAMCQuonBnQB4Z0dFAAAAwAA\n",
       "R0ABljwz+CzVxAT/Onqh126Rs0xPCh6tQSc7w74zIyPYWIj8kwRNEhI0v5/9gNUQttZllcAfJ6+8\n",
       "5aIspH3RC5GjrCFvedJqJwu/ej6q0S+oMGcHBp3mk2wiAwbnASSwi1tsC4cP+5q3TFb3DU88hBMt\n",
       "yU7KL0GgOuXYVuXVHjl0R88zpMEgAuAiviLDCgwZ85jKqugllAAAAwB5iCB6puPRVMq1E9B1z9LJ\n",
       "7Er2v9XULmPcGWVndDUTX+PZFCmGHX2lf08d4SYo7HQE3lEVg76e37BpCj3QILf+S/4ZetSs8YSk\n",
       "iC+83LKl9T7a0iO7RXDHWRX8+kmJhi7zHnrNHmUP0ln/J1sf9U8/fEyDU9Rj0jKnAD5JBWsupbGF\n",
       "11Iat4T6JVhsL8nyzbQkKVLgbHtl/qCewCH0rEnLhuzVwc/0JHeBwDYEQ1C7VM3juxdNTevVWF8L\n",
       "mP5tuXoZkduRwJYDUjyxGRPxqz4aXnfAVVSq/uAwAAAIy4FMMjFtKVong97jR+FJFpkR+7evp9oo\n",
       "iGbloaUPOoKvXMKKPxYtRtzcv7vXp/70fmza1E/8OylDAKi33jUjmcZXlHQEG9I3+R51snh5JfJu\n",
       "Ql6IY56HswxtFmceEWXs5YU6yyBUVcGUyD1Vx9BGipcal4fyWxdmAAADALqrsAGmx3dVuJpNjk0s\n",
       "IFVrjf9jpHAhhAfgoXskdhCEprVoTVA2YZWG1BLbV2AhZZ8gkDiAq39uRtQr3Osy9kJqLUJVWfc+\n",
       "RONjrjNuUjQhXDDAT2DpeTgrz6wS16f9sbVB9RIMRn12hYBiyAAmCZK5MAZOoIjTn/u7IF/asw9O\n",
       "Ah5+nj4siUeZxEvBEgAAAwAABcWfCeDrv8MOpqKvzvYhqgdJUc6dNz2UybnflU4YV7hhFXhD0MXh\n",
       "VGzbrwi/kLtIEGqdnUk9d5H8yajcvEi2dTxqH9U2dUuTfd6zCwT1JYWIiD8VNX//kzw45jbWagDO\n",
       "ofXTQLV5mx6yYczfwtl2SoO4VN6v0QkVu03KscHDnZcfl5TzbNW0UJSJgrDshmgmHzC6sPw7yQeo\n",
       "AAADAp2akHpTo092Oe+j5QvBil4VwWYo82z/KqTkHo38G7Md0oPlH7OY9BfAkkvRV853u2+rkOCV\n",
       "S5+zI4AAmPTcGXmXnCNq9ibOaYjj0TZHVRThqTf/WDNp1iBe/n8PFvHrOhcBxW4YQ1T7gNStu/Ks\n",
       "1q2jdkl9XfyVNP6ZBbrBDZ+VZprs3B0tjpVW58bhErbX4kv+iXZW3BlKm23uN/4CPhra0mEQ7rzd\n",
       "weBrPJNyDu1wSEkcuFa50tCsTIOwajOx3CdLffjWCNHOKghF+aBF2gI9ffsV1kNyAADxZeJiWDUL\n",
       "ARaTgeJ2cAwuzRNtVfyrpw0BGZ/Fh2piyjHt3+syhy+2leumjRK/UcqBhnMoNXeiJBH7kmpwKtbK\n",
       "s7ZFSuBja0a85rhXUbtuank61gN06HKfO0hHakMkwgw3f/becTIbCcsrN+9EI3htyKPUc/A7pzny\n",
       "jzFHAB1naKBgSy82RuocVSbuLtHLMzLxmDW/OqfdD4oAAAMAAAZbg6ggjDOGUr/Bv+2lmA1n8fm8\n",
       "Qx+WARUfD5SH98IrwJTbGLDmpGQ5M85rUOx1p/22n1HI+TsukyGFATshFPOfNZJiTbJRCNeGkOzq\n",
       "876aParx2r9oqEWzhSwBnCtsnKigewdr7ejHB984gbnQmxdnvQvxWhJtTtgV5u6i2QAAAwAAktYX\n",
       "lVWKtCfgL+/kROTs2BnhJt6dOERYHWpc0baNIYnUxysZ6/gOqSHdJpxd/re8NDhSv8/T4O7aPgBT\n",
       "DPf7uE9H73qEMWu2muEdGXiUdXDY/GVxs2G9u6VZDMoKEZ3MdxlJP/1w2IKWGw7PscKWlHn6UNjL\n",
       "oPvlPOQvqVHlz2bZ5Vx12kn8xamtsjWDp5wYcSA3Ia4nUN4G5WLtvy1gmfx21UMS1cZhqS8aEq03\n",
       "EKyVmliB+yRGWNf19WOtban+yrA0YN6qEAdgGqF9bPamNXA/sEJKV+Qf1uJXNEfLcGv9fDFxMxFf\n",
       "jETEfpZacHerK87HjuzGz5EzltBr2Yql9+3+lxgaR6AACKK9vJdEiinP+V2tvSLvGcMAjWo2V0o+\n",
       "tgPVil2HVc/V11XDocv/g+O2DJcH2zx0hvU2mLs7xx7X/i2mm1l2t/oNWCbWvgMRTqhdiVDk21li\n",
       "JagEqZA+ZN1HLC7+D1aGvOrU4oBeokbdzNjsdXYA4jsUtHnNb1Uvk86Yv3E5r0gWdNknQo1arHae\n",
       "J/xiAWbPrgM7NyN1Yd7h6/BP7IkJw1dJ7TPysAAAPjty5/9XrtKWjuXL0Q/aQ/iAspGNzynGynfK\n",
       "dDk6mquof4hwtfGAyQoZ265WyAqhlHF1iWj/+bVhykjU+Z2QG+8m+46h/qkpelYao4c7UkRAPo7b\n",
       "idL0aDr0LVrG83NGcf7zGkLbT6+h3Ll5byi16aaoxvq4Mg9+DKs9ovTr4lpzW1GBEBLJ/00BUxmU\n",
       "/RxEAAAJW+KXdFb1sRkFZrPz6vV7eANYRk7203uL4o9ripAQs6Nnj+pPZeVG3CL6rISPVf53Dznw\n",
       "7bOOmdNbOHk7/DdbCZrOGy6gWbVn94pEjsCmuGqZnCwQolbR2PIZjr0Xo2VvyE2fy32AJ4ss6Nh7\n",
       "BWKrTll0goqeBUURl4GwCCgl4wnFZVxdTETvkTxEu9+4fidj2sQtzEYLXKsbg8ts8hYOaRWRqqhV\n",
       "wWOLWJGeTAJ90c8yFtp/OAgTgRkSC1IbnTYAG4GRb37vZvy52fRQObjoX2rX83loL/Nb5UbKgvri\n",
       "9wf14tU1I+E/qYO6TfQxaHThG7v1dDWjNQYyQfcWyKKKfjF69ndZjf4uzYp859DR9IeN75+vWfU6\n",
       "/WBIgu/RRmFU7ElGfIqt86QfWv8jLpIeH5832yZoBFpcfgiXtKaxe2qufuEcMNW+4q7DwbMoF9QI\n",
       "eATYxSZekStRrtS8JdFy2G3KhLNYrOFwMhXi9qeySD4NZYjbDiahCTGtLWy5rf+QG9mqlxBIe+FU\n",
       "dxrOfCYJcK0/mk+R/RdKrmoNG84dqZ8jlh7ow/EUUXwk1D3hW4kc6Dn51hPD/utT7dIt6qH7CCq+\n",
       "6jND2zt2yB9UUVNYhNe3HydsaOyOv8vzinfqpJGxGHcBUAAARUz7bIvK1hAE86oVgcJgxWI/CnhH\n",
       "buZQNNV89tLYrlZr8drUGJL3Txv/tjz5+lqsx+E7Knzmjivj2V7SN4wtBzfB2KE5N/NRpTlSfI19\n",
       "e2ZFIl2B/XsUB3LseiI35Z3u/dRkX+a4PaHqKQGg+XZy/iPq9MlTe9WK5mWvnUh36H4/1MfYWlOP\n",
       "1egwCwWuY/vn/9oFrybjaFca0JPMeCRc9EfJGF0tXw6wa31QEMwqyfwIuDd+L6H2CVU/D5hdojsA\n",
       "zuF1uXND+IhThMAoGLEJ5+FkAAqgrf4Nfimg/6ZoEPdDnZmOTOBD3rx6vKWgYi+mzgr3iOZ0MuNS\n",
       "r5nWuaBaHnrmG24uCyImRlorbwE64ja9bCRdfwJz2SKwebllCxNzCP3GholK9tiBfZuG9+ojIh3k\n",
       "SE/37wnd+lWw3C2TnKa20cSggvwAmZvBvwuId9C27Y/+EsUbeUk8w28X1Fq/2ygyv5cGlRYEuSSG\n",
       "tJ8XX4SYcBxGsdvFK2fGm4ZiV/w36kUYdEVx58zJVNC4I1/k8qS5EErG+RjZfFnTrAKFze64NJYy\n",
       "iCF0ptbSsKfeMzyQ5jRCbQePedfLTqRuTqbrs0dD9bHD7KiLHgbOz1GB2UlZjlzO9/P8Fxv1PDrd\n",
       "1QvzU7cIlOeR2NGSRgpnErR2h7hMlFkrC5sAesvfhHfCU5TnwAAAzVswi/uIDoqFjZp02iP8DuYI\n",
       "GwioaPJPX1NFCJDJ/pRgXlRmmcRO8pnoeHIZC7tfAZfVuQuYcOJcxlDroitDqhGfQEcgJCKN4xKg\n",
       "sFcVmKl4BHnXQFGrM9CA5Ta6gIhJE8Il//B8gsVdmXchy6HA7ir09GV8mWQW4Xb4TxilMjAgAeDA\n",
       "Uf6zDuRDyw+0w7PW0hMPoMH1GCFrow36RRVf6VnB3vbjmbxuGh+9yt+vUHDJABc6QtcAM4ibKpvG\n",
       "h6pDG7gKPKMgWhC/5SpjVjJh40hMowruDtUGs6hWz+fs7vJS1nHN1nGaBW9jOvx6EtPm4c8TpvEj\n",
       "UsjUycdjM/oVtNjLFFtc35BkpuMmdY/JmGMmHXdX7IFhJeHZ8uIZ7LpFUPrd9z7KoVmm9NVgdXLk\n",
       "ilMVW/GI2B+rnw9gyrXFT2B3tmdwg+TXICHgTjjzGO+tWzWMleWl9y4Y5gwawAA8e5FYXjyztzcq\n",
       "QyDDWHvyAuPN4+Emg/c9XnAJUkLBHaOKe+8bAdu4EwibxFRpbxFlJfy5+vo93Am/ROZkQOYhnbOL\n",
       "WYAPKJ5NX9xhhltgTZFYtQoalPM4J6YhUJmSmfuYgBDvQ/IgQLGNx9+yFejta2TX93/ii0rbrNlz\n",
       "t6oN4cASu49mzkAXV5WwI4hOazuL1CdEAAADAOJGpv/VTszQL0prN7/MZ3psq8IxSg5t3CBLts7I\n",
       "4h0b3mcjoDBrFuO41yxy3ke41IGOyUKNrKqFGct0PKEz1GctFvrrodZiPvaJXkLrQsvnwPX3roK9\n",
       "pK+wBpgLC62Et31pWAL3dSb0LIJBctae/0bw7xaOWjsmh9zBZYnDl04cWSeVpoQ4HB0YLmnx0suI\n",
       "Xi6FopOBm89uZAygCVCp38BzegQUyVJr26wYztFpgX1RGuaFTtGQqb2W0nQLJB5bsHK1x/7zTwpS\n",
       "g6c7VX0PgOksfbEGNPmpybMHRomvyBbRKAAAAwACp4Q9k89GChsYwbu8prO41zzCRcl1b5nE+TNm\n",
       "EZisV/UsrAlbf10B6dsp1qHLXkPrqab3G9WyTFV3nAf9tzeA0+aGD/9n+NAeYBfWmbsI23sZZtNK\n",
       "gMw717xZg1q8ihbfz+8pCRO7KOWR08WHqiKcsjfrzcCMEcCL6Ni5lvDvwVN1VTzj8xtRpQCyqzsw\n",
       "73P7wAD3bUIkWKwZY2jU4UYJdiM+qmH4ry/YWFLRtZIWUtjBwB1Lq9n3p+FCADfBR66+BH2aXnyJ\n",
       "H8ujqQfQK8dcYWSU6JgCeOKkKgaOz08qedItiqc81nWWjrccDqa1Ss5szSG6H+UfOlsnQMOqYqY9\n",
       "aslPy0QxHPVY9lKcMTW5VSbzDdxkpvXI64TIwzyHJoVHO3m3cc6b8a+wXRVykaBTMFxHksp6W1Gn\n",
       "26fwwtcN5pNUEtweb//GdW63LSny+TOQKG6OojwE5dTP3PqK81qMAAAMgG+lrzNq/Hwr6670Gb+K\n",
       "Be5knwuSHTOvG7GgQ2T7JRTUAZBDBtkFw4zYn3FQQKqL5FHaCQXlgYrtCPGyqBj8IJZWi2QmqLuF\n",
       "fOqiex8hmFCxcfHcZo4BDM8B6BpP/pVOC+fb1pzg09Lc3ErYv1kRBGDysFq9mqPfcQggeDPeaxEm\n",
       "wkrsfi3JyfYQBn5aFUq6zz3ECELaXC1ZvRxQAAAGL/jdSmSzb0Xxk83G1XWWlZrUppWR61JB34So\n",
       "gSsZRNc7IWXxehUAutMmkAu8FpRJlrNRon903NqPimEHcdO1G3M6oGCPrqhFRH5j5xpDPlObAUz2\n",
       "dWGqZoFHgQ0MDgfIQ5I8mEKUDxWi3I96nfYXqvG8uLkyoB/ZWGecNnvbwv+c4UBZ+mS8WPvYRWfT\n",
       "T/7QE4tBGE/Zm8WRdgetJb43+FQgzI/Hb0Zx5ZiaVbMUwk0g6RIseFBOSU5bHH6uejcmcFRqCrvJ\n",
       "69ivHWSHBoyQZJeFQgzWLwDQKxowbLQ6hOYcqwGNhH7WEOVIGu7G8Ccgn0haVfIAWi4IKx9lFc4B\n",
       "XL3LHzn8Nr0MW8KPsCTx0v8ScDz/KbySr9PPlr+vkSJqasIqLK+Hv0Ql0bI2TIsKz2pNTSRLUbqN\n",
       "hXtFy6P62Umy6qZPKH/MrQ33b8XUEtvCGWilyGsT9hI3gQAAxJkXMioEhv54vaY0CkugMRaCiyzM\n",
       "Ym1kuOxY9b8XXeP5NbQP5sIJSO1oAecSrx+nYAL4WRqDQrFs66GpS5CKOi4LjmkJ2lS8d6UWn4Sc\n",
       "2IWArNEDLf5ClNx+ma1m89VnTTECX5XTqhTc3HZTFk3LW5oBjyeS+LrPzDZLuqGSN8VuDhjlw/EO\n",
       "x6jlL3avVLGKTEhAQek0vtS3fHSy5o6Yg+o6+hlKoGHLJSShndpYTQL/wmtvQa8kp/V6puUaT551\n",
       "/+3Qu3KvswZZInz/9nARf5l1VejzqeCLY3eMJWICjv9s727Ddu/XbqfHgLxjob/6oL9//Feeg/qf\n",
       "6handjOa32h9/YopjhMrTTzQ2eGzoSGQ1mNN33hCIVpL+D9vFss4SUgA3HAABw9IeSgYwtvVy93S\n",
       "YJ4dLMWBwNbIbzJqnaF9NE6ifmacpP6AxqT7/q9+Xq0zvVXN5K2dbm7TfSf9B/0w6E/oDJzBSMUf\n",
       "duJ57LbLM7OKBbh7pQwWFA5YBoivcnXTQfZdHAhwrNcXnEt3/EQf3AxEmrapDZtdUs8F/dTDSvK7\n",
       "w1FBlO0b41fsGKGbnjsbLquPjNfIjfYgUdy0906Pvzoeipl3H3u7lg/KGfosx22vz4dQULC5BjwE\n",
       "OSZDiRQyqlunGm+O2YnnNp/es19fbMwVueJ2d4yHRoAklmDkf1c2YOyWYKzL9L4Wq3tIAkVVwoDH\n",
       "Y7sKJCos3Rd5z557OQMk41D8htRi35+zfUN4xR+zzKgHouRF2SgoVQt4ev8oKXEmw9puyw2sIKt0\n",
       "NVQHFFgtPfAxkPFdmBSpT5r3M7VfDBIpoXkestqJymWMDgAR7kMwOMrG87lAZI5bZSA8l3Yrg4wx\n",
       "IEnr2j/nw+/Nn/0KnYXcZqCizV26h+E2vEf52EDvSnhQdHVEcXTep7RO+TdfL+wq0lhfx7MvvWUR\n",
       "geeNINkqjSSo9lj900qDfB9h6zBnqxMMBqzCyFsSSJ+W79LmAI2rw0x95UAlIocG1Dx7e5RtdZAh\n",
       "TXnpDR18AM8AQcAAAAMAAAMAAAMACPkAAA8rQZohbEP//qmWAAlPzEzheE6ACUCIlyt//Q2iZ3cn\n",
       "Ot38d+u+tPCbAwFka3npYyY+a91wU3/GDiNq1SyaRM9qduQAzKypbUGx6XtRPAuPkK7mWCj3STsi\n",
       "11FB7FoypXg4MsHSYYxiT/BKps/6CFvMcR2VfTfDxadc0woFVEfd5UhOiQUAI0xSp4sHhdVV6jcW\n",
       "LFb8RVdn5z3HLIOhHgpqtBPE480UHy2TFP3+P96gVCVRsqiFICpDR1R/3LG3Miszk6XngbIrgkKm\n",
       "0p4P3JmM8c1F/fAi0xbxnIDUbpR77mJY80xmAxrq82I4kreaSfdiKxg52uMol1RZ9G869oPXI8XD\n",
       "1pOA87l2V8EnMJPmp0AiCiVUq8CWsKBj6YIxk2VO0k+FiVu+ueUEDRXpc2FJ5cql023Sxl2JoCGQ\n",
       "NAGS6sGMeq+HZAT7/NDzCvN/CxEYKiUq/dLmTTEJsQveIJnpOOGj2bojqEw3ouV6feXUq4pY7oX/\n",
       "bnQ1eH7BmRzvnuzn334MWAxzHhwG323d7fCANDC1WdmBUgWyuXu78y6vT5l9jRQhM2EVn9hKFmab\n",
       "fDpCAHsWuOeUP7zcGsn36wxr53l/wo+qH1kgQngRKaz5MZwDiuVxIQaP1eayGf1JR6skF6AmglVM\n",
       "187yBPYShVfq4IO6goL6Y820+bPIIkfn1Gm8fhoOGfiXN/zO0m9U2URe9yXiiO7lhuVOn0SeEhYi\n",
       "lbfTf/VgLO0kVZd2to8HTdVHDG6UlEEFp3mZ3HetOfZpcHAPO8lZtoagC4FgnJtYJyu1KsOQAt3c\n",
       "tJ4VG9++L8VLZEvQBWjb3SbNpb6aGBgP3BRuyRRtDkZGgUq1RrkTbUEXp6oxxxBU+oKRS2TiX9Y2\n",
       "vkqcdX4MRjoCQ+ODJlehcT7NTIbCwsrqnnD+lke+G5cfnvk9MknfRsB9JNUG56WMHyNwvKtQWOK3\n",
       "D5Ja8Q5XtcXtTznbWTt9kNxhuPeL3CkZvD/p3E+MmXuZDwxfuEQRpBYg51FQ7A8eXGyHoOfHny5I\n",
       "JsqGU78o0jyIzYSTOsMPdtnFyRzrykN8A9O35Hgo94qVB/MGPzzOc3JCj0LCVIjq7HH9QHzBexbz\n",
       "Rym8DJRKeEGBLe6KMwf1hnYTNR6PVlF4+BWn+BkCb9tambesADSaamfIrVfwYk9nlKoDQv/p2apH\n",
       "D1R2YF59ljbiTn9e0JrncP733RlM1LY8gALGhBUkyGxzNj5FZgDNA+gWO9kUkwf/ES/tV++8hTO+\n",
       "hR+53pqmedSPdZ5xhzdbjT8XiRrq51tFxDfmKLrVduVZjGc/jNJwtVrwC3gx7eSYjN67UXYgptd1\n",
       "9JI508TL2RqW/8msfe17P05ydRaWl1TDE63PPQcJeeBTpQXApj0OWN/uJO3pD+QRYegJMZcrNjRT\n",
       "x718W90GwYyuPwaA7K9Ikw4R3kNS/9KxeFPyKQqNkQFoxp1FGuaiWgHzGt4U7fa8wlGDGrf2Ohip\n",
       "neMp47DmBv9jHMiFxdktQ8c05GodosoMTCGA4TqBUyYMdkTSvCivSlCQwjQSZODAdO2nW7NMQvCU\n",
       "qXK9QXiUkumALneeRLPAmihgvJcbJVs805nQEIm7KlT4iZPY6QZaz26HkPm9kPtdihj0wW/bzmJp\n",
       "vsmf2ibPwSCfq/w/3utOX7ifYCcR+YC3lwAST4YbddpWK+XRA+4GYOnU46OJylUQAHoFeJegRi3c\n",
       "sinZt1oGfox/EKLsjNDOcIyTetTxEud1SxSo2ai9a1AOeRXckW2WEV9Knl1iFwLxipZ2yrGRTO/2\n",
       "U4E2lfLlBmtYQKw+hxWP/Nv+LYX9SyQ6+Q51o6UoNRzIsi2SDm6rPZKCuLJv7tgGPAXTsFYx5z6q\n",
       "p/nzJ11WsYb0Q00THzyR/PLQHm8uM30KOCO+GPvH4jaT0W5h9uZEWsdsHHm09wXYqFdv9Bi17NCA\n",
       "tcDOfDS1XfYOTGrN1z1hfzlJ9OdcPSi4+JbYh9iaZymwVaXPzBOb8sS29ctjZVPTzAEFS4Yw1GxW\n",
       "zYXg29OMSQPmJ1xtTqI3/Q/s757GJ4kKu2xJAk2V8orvfZBNV210asbIXmU8qrPNkcoYr6oPpQhb\n",
       "6ZZ0GPn/7O+cbUtYiDxoKrVPdUyoyKJKI86ODtVBMNsFxfdFUnzAHffTxKbqV6Glukspx7Ghlzt6\n",
       "gt8R1eISZFXj/siQrtwpI/IPzdHpawbn7PavpqlgDCKbvLsMv73PC/LruY2GWhd/CxvQc92+7AwU\n",
       "LQytUTzwsXuWppOyZjUXhliCWRHR4IvQQMSnlcWG6YAIJaJ/8U1Skw+UrgCkbv0J6IDfmNsaBaYr\n",
       "QJwTF3sVPcMsvYWYDrguEXJXuapUUgMh+HG1LwqzEdU6fHvYU94WONKN+Dn7fKhtJk/QGKcwitro\n",
       "b0IeT6PcGlNGu8JvqhRO1rr6YLemnUckivn8vqbOyXQOMHC6vXDAKZbgBlcAGyO3bcNGDKF/ZCdt\n",
       "GaVhQUzYA9u+hr89JHSV9M8T+KlCOSb6ky7QJHQFPGMoWgz+FfRAt+f/Dp3dMnbW5+4fQiK+4ybn\n",
       "65rPigSoQezpDIbtvOvhQCP7DPmQaPz52NhTAs7W96ZgPqFSgE9FD7uuSPxx+mj48j5sAUGmXwAd\n",
       "XN2NMrDkPrakH+fxuuBcY8ZPK/vlxJ1i7x6KxmOFnqKqr/tO+BmqrTxVACTN6qAkdDfhm2vE+i8p\n",
       "46D5UPYSTEvpqZrRCmYWP8kiDepUhjE2ciFGuwzGR99OGiOYd5sLYvrSViuerrGm4yKIRqeD0Nc8\n",
       "UzbJMQSCqrTA1TaiEyBgM51lLbP0eRisgSPm+1M+L5z98a678moDcSvllpkpQ/RDh6i6It5kqI9G\n",
       "jM+NZVVtmQS7lgIN7jsoRcf7gDH4HPPN0wR5eB3EBQ8B37BvNjUhOGRFrb9MEE+bHx1G3BzsVdRM\n",
       "KZ6lf4jAtKal/s+Jz7HLe3Hphl8W+NSmvJlppBXwDXII71V4xPcwhys+ulu0aVLNJydhdMLWKqwC\n",
       "Hx0zMtJklth5e+pHL9aNVpqupe0zXnGDLbpp82FC66oj5h799+7ZkSOqP8nBhDNAqDYl4Fu8ZF/A\n",
       "pdgO4yrSZ1sCki7tdEaQq+H+F9qkMkfZ+cTxJ49Q7E45QLCJHTOioeDLQ9AetfN+jx9Pz8+Q+JQT\n",
       "g/Nteb8Bv9Z6xg6GhCaJ52iW5yjOBH7wxfiQtnoaiORJ1TcJTpxt+LsKz2/rM2cNU0/KJGKA0Y0m\n",
       "lkTfVGvia8vlGfeCcdQvDKcUNZB+XqS6phOfkNYXJtGKHR+UyJWoDTImPZGDlMlJmFzaM9IJ9Im5\n",
       "1spikIUT8EoT+gB75STJnbpDEVAo5NdlO7j3XX52IBXuk93gQ6zrIQ6PE2lz3vDQqrTkrtFbp8aZ\n",
       "8OGNwX8TcIekZm+R9k+4sBYUvjOZw3eRjep9qWy0S2g9fAcRAA3GpNz1/vdZadwZzWkEYbAK1pLR\n",
       "tdM8BX5NwJpkTyKGvhRUvQnfbdRlXZ5U+8fomuJx/ghUcgZacDmcyT7FuoC0mj7wrLJrlaEfkYyJ\n",
       "rttQeTDW9YuunekjCeJg8hgBHPDqrkOEukn/iS2RUMeixj3sRt5b/M1/vF7bdoNnJqBN+Ncu8/9V\n",
       "rpK6acVTq74QxxJqzc9gshAYEEb6jlc9AgJENMqldE81pw6cdlbHJ0QlUwqB4+BkCWS7N2h/GpWQ\n",
       "iOoJpzZ6OyjKZ0NjClpNMJDqy+9k/7rs4T/QZShmr4/xEZ3A3WcpYAGTOMzV9kIdBVAyBUVBsLpN\n",
       "nZUjWqDIPLrHeUoHrn+VqKuIpyGhJcH9sXNbYBJzOhFW2fZ+oClr9eMsI0RLgNq2Ed20mNnFzUm4\n",
       "TBTqDSoz1xprSyxoignJHLsALVPeZs0AW3TLLKONB9NTnzo5QuFtr/KU6C62H54jp4W0PxG6rB9X\n",
       "Fdjr5tX4k4mXaa7TS25wOksrsaCWoL3Q6jyWgoyHzqzyKWpm3FF2830PiTjtpaktB7SgaVzB7T7x\n",
       "B2jp4jxqsSJVm6n5V8iZpNqUHDfeupQABeg8TPPttgOOZq672QAGvj0vUbhBf88HeIWQhBX1bJUY\n",
       "5/898qLTw5nZ9Zha5k0NI7Sq+fwp5JXm5MN+Ivku9QTy64B1oznkKTiFh4Gr6pNUWNkkBpZtefEi\n",
       "40lOtdDqsBiXtA/vYWibDlxs7ctFxuCYYr91wnnHnWEi878hyHNq6pqcT2A7jZwyfGouUs0Mwl68\n",
       "Q57sbqvSuxunfGDnaL6sXleNG2vfzfxL10lIMjQQzQc1JE6LYsLLc0d0p+W7u8kjoiizVe/Y9gnO\n",
       "K89m45J9r3y1slWZUK57So4rNXINzskIPrM2VrpJOXat1VrEQuc4VEKHgPfcI/9x1uWFNM1A3oye\n",
       "h70o/NtZmd0nwKwAU61Gku92xrRgtr36DKbA7K30WIPJMtRJDbtfSAyxNjs7NL6WpOWF07ym3B5y\n",
       "ab0L3o7hc6nAll0D1OWnX1WijiOF+B81KIi5A6m0jmAcX36HZEhNmAu/6V8MlkSMhNOmXjuAHRJy\n",
       "LbIXg3MTyemwUBCOF/VGIOsxNeEkLr+I3/fSATFaAMFUycvsAdXDXSEwvTay9dcr5vZ8dcIGqRDN\n",
       "CnFWJxH9koiRC0x44KlSnC4XXZkaiKPXspI1KNrM73pAa+i5vZHia10+QgOv49OhDyAz7/0RIaiU\n",
       "8ZbQB5cS94M4I20IPfjSYmfZtR/Oa1xiLCsxi8DX3Zwlyw6+cTWIkJTon6QtG0zEaZ8rqDPtFj1s\n",
       "C4hcG7IBDEsz+/skHkerXWGD+tucZfi387q6gPl0A97zXWOS5hm5VcmMzDmgiJ58uyB2Mr3XfHRA\n",
       "vXhmlIAVX/n6c35hc0n/CPpq7WZFBpx9ymppct2GAmkNKuwVh2IIA/G7uO3xiCsjRy8gv2OhMkrL\n",
       "jh8pWLQt18tWBJ/EcTd3BSUlkiXDJuK6zZ3r7UrKz3ws9qidrWawvhTOnyxLVQZ3hpvWoDYJ/+yN\n",
       "ikFGW7idJvIOTku9njnvh8dqYeRdUt4bs/z5I0VG3MLf8gUBlR2UnthDgMOXRtFnbZxPQOX/wiTo\n",
       "tOq4i5HAxWjoqQEYT/OPpKfPCVnvZ9Xlj4aK7fJpVWkrwd2YxvEvYnEY1VXse/dUZXU/hpjZDiFP\n",
       "PfMf7u7tfPM3xbD7KIiAaorr/Qhwu3DW12MIqFd4sT3MgAAAD1NBmkI8IZMphD///qmWAAK780KA\n",
       "C5KFg3BaarrC5ZvMfEV1nDIgY0pKDZ194h8NFuif6xU3HDjBkCeN30KVud56T3fVM8mB2T2t/hlv\n",
       "zWcfkncxwxDN3z+uAr56ircFSx20vcnLEDZCni8fH2Q9z1HbPPCT37QSV/fxqTK4gBX/A04b0SNE\n",
       "gku4yUw84reRa8HywsFxsrnL7KkV9PYZTZCDCUz4vXsmeh3//vxQQFAxWh9FWIMIMvOlPugxJF3Y\n",
       "iJFoJPe5UT5preCAY2LF1HU8RpxYWyIuTb75V8acDY/Rfa9PfhPOTyAG4ekMrg1J/D6l1MWgiIIt\n",
       "903A+aKulGMg9Uueyf9xKFCd2hobT1g4GLByL8BAbTgStn39+nVnvHNgvjplnAOUOxC5j8sk1umO\n",
       "yG/s4vo9kAf7tss3Dr10PH+Ajj8KQymrAKvog2TaG/6mUrj0/NtA+wXSfJ7rM6z17n3i+GhVzWxf\n",
       "e+GUe22BGBxsxTu+lHCySHAcv6bO3Hbg27lezdoi/2y41PkHDctTN737aauppSEv9WVWUOfP4ifa\n",
       "4wpoS4q1N6QRdcd8EUpFdM7BnW5JH1sFBKOoSx+sDsL0nsPJ1q6nUGq3OAcOsrYfCudYRzDkpeXP\n",
       "Vps8wdYHwyusTq/r2XDkwOgpFd0VOZdzoiYBEj3a9FjSYpEuTWwIvV9MxySz88jiNrfjxtttFiZ0\n",
       "eUVX3UMereA7Q7uZdxyCo0Pq7Ewf0KQpqK7UFH4vUQJWyOP8JENSYCtTqEf9gxWtp5Qhy4/+vY6q\n",
       "6npop9KUeodXzONUvCOC25U3tlpUhTA8ByQydTrL/QZ5EaI9/IWWiQbNMf4sAJOwJEgdrWTSqD2X\n",
       "1b+pzF4gI5ZT728WJFguB4xm5ilRIPQGDnTn/8rws4iGmz1ctc8U7yS7iyBKvOvZnIemVe6gD1uF\n",
       "IwGgFzP7kZHsn7jI23P5zfkK/6ly9ZWkiKZC96nZjv9iWzcXzGnXSWAyVTn/hteCP7Rf1ORVFk5B\n",
       "i/swmb1syrSk0M31H+pffBlxlhG5/jbFz+HgFXVuECDeh8eJFK1RlMAYFFSsd538d46+IHW6RTBH\n",
       "XcCXSLmy+GnFzicO5EgH+UdcbkgP3yFDtFPt9N1DyJwFQb5JKGP7QiVPQcz6VBoVZ+Aj40UlZug7\n",
       "wMiDS7lmu1OkwbXqS2YYfpI7gnK+aFspDPllg65LQZ4WrQWsNe+bxkA9jfg7GaaRrXCA3Ey07DDS\n",
       "jLOi6Lob+02rin5/dYMUgSAL0+YIGR7JgCO0DDJgREETdgFCqZHoRZBJwHdmNDwYL5vZJqQdzyXd\n",
       "uYXFNQFVA9AvlyqW2NE1hMrHtu1tHrefKliNmZj1C110eRL9LEPlvGiaSwTWZOs1AsitHnZFpqHa\n",
       "/FOXnazqxAiooTizOtjvS932U9zEvb6B8WIpsDXpoq8wSRaovlpP8GrH43TkcRGfCh6x6BCMRSQq\n",
       "4pyvSeMvthBL/5psvhyj25f8lStotiT3z4oY/gYbQZTXr6YW8NwJGpeBGBcHaB6/52RL615cxGyr\n",
       "XFhWxkxJsA8rje583HeFLuB2YksvqgvYBF/B+61IIq+6yJ3MvuIrZAf23OO/3dxgU1j6f5vSkvxS\n",
       "XxD/5tGg4NEBwf5knW9gPpaEX6nobSeov86QXSvPf6c4XRCtANz+CX6i4Ynvt7JLycqXY/CINjK+\n",
       "asKk5RAmGDHRLTV7SXlggvhFrcNl9uB3Zk70MPxmnskVJwDMU2oETtmlOsY8wXKY1AERqCs5T+ml\n",
       "0oMnXSwko3UNQLifpR1/ICw7F8H8ETcZHYC1WCahVsL7rdpIhS47niVX5e43nV/RPacRyIdM9N02\n",
       "hwtFBuX4Ok9DSK9+FlJjKrh1UGwVtidY9tXbqZBR2PQYI0i/2ewxqWw8mtFhL74haKUapvpFdt/3\n",
       "qW95DrMSVFPQVIQIIWA7zjE06inH+GLTu5sD75moe+zQIMtEgkDxkKt8bJWMwK/KZmeEJQsIAYhp\n",
       "7yvDUoHPNiMNy4Lu3PbB2xKhZdJEqxinqKIflMgfWwa+WqfHjnwTNgLDcJLm5OTLhh3xzBnPMdfx\n",
       "Lbsg7jJ1+Ae4rTilUqY5uBdw9X+6mfsspK0nTnMdL1Ez09dWofm+2akg+InlPjUOo59OGT2YmLhn\n",
       "VPt3I2UaG2hidhLjjHKp8kgS5tIZbFHsgasdBaQBl/9egX40NJL1IEPcBkYzH4o+/U4x4mcao3o1\n",
       "ewJoMDyS5hErWl8dQTRTWD1kgT66aXB2pFpKqOqanm8pfiDsyrX/x2LmYBq0LWJgX0NYEb2YYT6X\n",
       "JHI2rh+sXz8b0JP2ZyX7PFDObgswcuaESOu2lK8XcpVRFjobXVLOJMWBA4s3QkL9qbMQqUDET5nV\n",
       "Dp83atZaaonMQxfPU7y29onIC+g99TgwtNvrpfCGeKH9/h/w+G5/pSeH26LVgVYN/ZQ6A73DNbYA\n",
       "mRnTqGUIUKQJJ0zHPXcAR23ksFa1n52i0HGNejSZp6pYRo73e9jp0SpZiBqV60fs2zAQ6fSm4Z1U\n",
       "GqJT8lZKlghZKTtjlw2exnFw75rQ4Hjks2v8FvRpKgKK4GiC92nXvvqITyCSslkKxC4xUMuaE/td\n",
       "nfj1zX465KcvvBmdimFenWOm+a+ulNFuOVeouCnde8h4PtjSHZY0hlB+hUz3goH/FLu3HK5KC5GF\n",
       "rSv6UVsDP7/ohI2J+YLz2pqx5bZt353cKqXDTrO7SW4L/xbA+N1rOBhBMSfSjHDAcQAKX8oEdTv2\n",
       "6k2l4QfWlC4T0WKIypvpWRbG934Y4AamSFYPUbnShPCzRBLuWRXnAbZe0dsQenLKN+YZLrqRbDxk\n",
       "cXWyh2tKSvyHqvcmo8A0KSYN/Pinbwp/AzuBQ81H4kB49EkX0suo089h29YiK4uhlRfqUXhjmiq1\n",
       "9uhvMnQKQQPICs+QCZxMaj5QxJg/DjbofJLAGcJlT1Mf1BzloRZnFetEGi4XPeOyTkvMkbyt5NjX\n",
       "faueWb+zY7PgXiYIb4qmmxJC6WIZWAYrPP9/tgOR9tSQb+MWZ0rJc1s0ELpdYdVahwebl0AHkHP8\n",
       "IWoezTxkAmrKeCPHcUe0PopcMZh3hoBq0/5y703gZ4XlxiEDtB5GPOa7ZRseQhE4wV6xq+EcQvR/\n",
       "S2s685JCsF50OnQv1P4u1IzczZ7JFbxhFq8TmmWKLePAsrQu3MmCa4dQbX2Jb1hohHu0loxZmk2J\n",
       "+h3zrEvElH7LQLvDSrU5KjkDUzpy2nWAgVSWNaJfGRsi+2QdbqIE16YmNcPawkMH5TJ38+LyQEww\n",
       "MIbyYDDtrMczneK5+MyAMbz0Z6I1dOwe7IVNC+wn3rJoeHQ6pn1Va63GIq4VO8HaOqtN85jXBihp\n",
       "eqJBC4RAHQE7/FHDDmbYsksjGEog3PpIKsypViu8RFBnsjEN9+LU5VDxI7459let6j5ZcONrkOqC\n",
       "/Wc7Wmhk5P0VlYsXIr2C4sA7iCYtOtjB/xYwqJdS96LIrexM0oO7ty1Ud2l7a0obe7z5Z4vxQ9hK\n",
       "Ibny3ks9lfONfQWx8skUcR+raAIqEUeQ0X5/gnCPd7Xjz/l1zl6G8yxtQZ1iaIQBFWr64Rc3D7b3\n",
       "pIVYIAF2zc11mAN/44TIK5/hdE+P9rTDc+qpor/g7z3uGrulx1k4QLUCfdTqZwG5xC36QLcifRin\n",
       "o+uyhAyqPDSs5ax65Axfz646/ttqWN+WaM5Ck9fftE7LB6urNcezZpv96JYwdkEXMoo0owcNzHxn\n",
       "SbKfsaZmTZ4dToiMDEHrYFjMNdd27wuHf0FQAVN8TFPeS/i5xj+ZNirz9ACnl3WFDlUiKHgjaSq3\n",
       "hY96JZs3MFrbWORBK2uikyQvgx/GdHFeMcO35CrHwpRzc6z4ovu0/RNlcg1GV7Opmv/1tu2xn53i\n",
       "b5AHKblUZ4RjnMNwD9sE2eMCfqPmKwMnI0zC1f/L3b/xHXUNmTQd6I3nBJRKfLxp0Zq2qpZOM/KN\n",
       "yxPz2igysG49iheU78jHfgXaarIbI4L4Ch2Ia1UbAyFAnLP1VZ9Q8AXovEdWia3u4qigOfc9es7j\n",
       "IOF4X6eZugGGO1yw8STN3lCCV3/8jAVWW5FL7Fpo044uU7Wj3hHByE7e8TKjLfA8Cs5kQ05at+8y\n",
       "YGtiYGtVq1BMclSJPUHwT8JzM1VEexUfkbi2iGRiMSdN0EVzfwf+sJOxnotDlymQQvKO8xgoxBex\n",
       "Memm/qRti6KFVLGuqPurA39Wy+mlI5ptB1EuJmyRe/U69Izgm7xGJDxKa/RYpePTaMjfg44rwE77\n",
       "Q/fTdNO82XdzqlDkJx1yWVoRXjLcpuOvoE9kC0c6mdlkFOvVgZkECiEDPaE/dWLNj7U+FuJ51z+c\n",
       "vymFQQCfnQL5HcpdrPShtXcDkqo4oijiZupNMSqxBCld5JMWhlih1PJc3MNJCVnd0n+QU9tEo2aW\n",
       "3j4fEulZMNtLCKkJTuN7DqfWikfPbN3eZj5+zMFdw4BTnyFwTHbDQ8+r4Ik6cn9eUnd49woyssXn\n",
       "Yba0NqdryhbKRcoqkQhzgj6k0cog55O02CPiYnUoY1RLkSmpEtFg+kYo9+lQuSiMHbmdzUPy6GyL\n",
       "wd7gUGEYkRJ+IqT1jBcLiJ/6hnyJo/NVQNkUTAHlnjF1W5qcCw9x5/V+tl6uUkDcRraqLXZzKPxm\n",
       "zuJoH223P5QZQNWJvmWpV0KldloaV+bzsLVCJe4mU+fX7nur1YgIr3+0G11VWubiC0PtgQ5zzjoC\n",
       "KkBDy/NsU1d+wwGcbOtqdhOlpaG04fEYp6iB2LOsh+a+HW1KxumsLuV8oxBvIdzSXN6Z0ESNEzc/\n",
       "vI7kbimLr1FuHE38Dr/7R44IlAflZrcpEjaEbqH/2VD04MdLzciK5gd33z1srt0hysDzEdIYGleO\n",
       "fCTDahyTRxhPR/msN0zcDOzAAhtPPLBkHnNqa/afzwPNbsLvQ+cwTm8ARc+QTviGAZhiQ8wjihRT\n",
       "RnbpkYzsu6foCoBvnaIGIdsvSTWYTC5gbQ00vsTLF/rH9nKU6zPTlNlhmbvX1UIIfJEr04XPjn3R\n",
       "8am4o3yQ0q3E9x7czqQ9riT7Bs+rqx4rHL6qQ/zMBhkv3SDwmflYN1S2ZQ7VGmsS9bzibW0JVODq\n",
       "aDY4LKKPRDc3tMeDdJfme7lU1wV0FEu+esbeSMbUwYn6kRODSxeMDiPjUJHjLbJPCSEoaADn3iWz\n",
       "q446uuGQ0u8L83vOyIMD+TsE3LhN6AFdPv+EPQAADnpBmmNJ4Q8mUwIf//6plgAC3C6hiACx1FZg\n",
       "76oH0Dd4uNdWt99SmBSeez1zt2OnRrl6Csd2N/KFHUltPuAHpZDNldBKS12C+SUKA4ySqv4R4O/1\n",
       "POOk55irb3pamY3sIXxP1Bt7Gp0PoAPS5DsPnAu+AbFZYQItQJZxlC+B33jQvtCDSrME9IGRpd/O\n",
       "mcGEHYwNJKzsryccUHk0son0HfGS3ZGpKnz4XRXHqF9LQ+VsryUc3xLEjct1LnY8DbRCmq1Canli\n",
       "/Gmy0PP2x2xkPk9HKEMLKKXCDPa+xQOY5JsgeuLd1rLG9hhZ+GETc8RJ/Zh9H+d8TcMSqKoonPG3\n",
       "/dY9L48Imjqh28bDsjQz2hcy4kzp3Rv8C3zZaK03fDBzHGpJoBDFIXGSqjXwK52QhP9UDvT56O1L\n",
       "ilbDhuKEZndc9w5WcBswM6nfdpSXpQiELzom0wbZa0DzHddBW+/CpyF2XonB4Mhtzi9JXBprLFJt\n",
       "J5wiMivAWK2JUUhdNv0bR3fZpZ0ci7Fx936Wkm12Ks1lrB0cNaqvsqnuslyERTq2+pDkYA4qWT1E\n",
       "JCbrRMREeEf13PjJwWDty7tyTYZYoo34IuSIBK6B+ZdR5QhVUGYfIBPUs8PCY42gTzB0jCkpjX1j\n",
       "fWNQEih/uBWSYnC2pI8Td/Kw9wE5rVbEomRgdiXtaIIAF0F1CQLvZHOc3ZbHC8UQF08InMv58ksH\n",
       "43jLiofzGSBtbKLIcPQbLujIi4+qWkX0WApdg+HJIwH8u3XCzgOGC8NAwUbCJjSXs6DpUtcit2zc\n",
       "903mKB5IKGKRxUpeLo3d7REGJEff//g10tjIyJRQMeo+vw46bqrgwltw9HXmyUS2zaSUNbfRd1+P\n",
       "5Z6dTFQOtfaK5wzJZ/+zz7sUub4tviWEPgP+7c8zvzfhem4n0/HKEwV4MDlg8FJiOHd8MIjpiZTE\n",
       "P1J11adMuxK8sdxc9VN8FL1U3GwbaO/edlbgsA2XBZuy9htRJKajOtuM2r0QB8g6OCH02ObM0iCw\n",
       "bGWSc1t9IWeoRCBXHRG8WzkwGINLxEoGX48VkJtbm+N35l4H149U2bEANM1SQKCwZiJr1AlAu/RL\n",
       "lPq/7GfO5R0fDpcSndr3bWlp8mIRfTDIWhoTlIwBG6wdGIUVBgWpxgAMB0t5lw1nmT3A94ik2oew\n",
       "AEfwC/pv+8zAxlxjcYVwqHoZRFFcWc+szgwX+dEJFzL5Yu7t5OQA6d76ZdIXBmmKXQ/Blvzcuu3D\n",
       "+Y6043qg47h4wmk5k8EQ8loPzcI1tvvd9Q+ul48OejDByWGcIsetDuAaoscDSEiVwav46yIzlAfT\n",
       "VW3OyNkNP2rmR0QvwvlIGFGRavL4O0fr7bqBfwsUdkxhXFxfIJExI1PTKNmHmr4zU9Ems8XlK8Vd\n",
       "QKQJCeRZrVAcr9bt8rKPl5rKhiuBjGh2QEnBsEBa4zGMmvDdgYq1fVX/R7BkGHJWDBSUO5fvRtKs\n",
       "4JPa2MYmFBSrc9TYWKaiis5OnmXG8e1egyt59vfY6tm7Bm0aFWfVMxWb0Lwr2Xn2sKYDOiisQRAw\n",
       "ZfdJUDCeG+1W4O6OrGdwvXfHr3BzcSVDvxd51BZAhmaapNSfSX9Aw4EdKQpjp6o4M+ccR9IIqgxT\n",
       "bstc2QON7nDfh6hrBDDod/51cquNBJS/LMicgo9aKzZ9CjXT0piakBfuBnIecselDISiMr02/+QU\n",
       "DeXo/XAGatuUjU6HJ699fCcMARzdOt/g0V6qSt94yOeVqI86WYF4yP1dR0YOPo8orHsK3TXgrZT5\n",
       "e/m2NSnxnZfI6l1VeK4ARxauk/X4m0rZTaVuUO9330U4e7kXGboFBEeV2HoiW++UHJHV+LqUpaU0\n",
       "lA2iuxpa0nDJt0q4DVadpb7dMRzhpwtwGu+5w41VLzbjFS2ixJndFOb4AOsjfY77fl5q5bwOrrfX\n",
       "RhMCjKK50Y5gpjkUMuQ5CWT65As70YLQCph6/tvvzMgQbnGYt5abDPqpmqDsEjBeB0Vt/P/egWeP\n",
       "vq45tdyThdUYf9fzaPmcWz02jvbHklm/WBmSBD05TXnUR9VeIdJuAMGRBYYiP3Y6hr4LiqT8UTU2\n",
       "KK8XkeSvdE40r0Kz2jzCkHhgscGyGPTj/qftFk6400WkqpnOKDSRSu/GMxcqcdABC+reVwLyNiV9\n",
       "NW4/H06ds3zsGDsiEmfx3Matzt9hjJ+Kbpa7yRYEDUWBPOXJ5u3KPRCLP/jj1YWN1RhK5IuuCPJE\n",
       "GUuGPu/v6bjPQr67dDJfcnjkVMor4ryXOXeHuAXxMwvBYfJ+MS6aElESteW2CwC+FtfTcN4T1Ky3\n",
       "t+fn17gO20iFCZDXmUl07Y7gGpdqZOOVGKIC3ghLOqp2t5CamRTZ890kH6GiOv+9EYqUgAgBOk8R\n",
       "cMlCDFZk9gmTp7R3PAUOW1M1nXFQD2v6z9JvftA1mFwTOM9J1EyvcV3QWhLYcOW1Ejr5j4cb8gkj\n",
       "L1L90/4lgPkncaNNZBX9/gYm1map3AKUljQPKV+cyXgI1xiUvXBP8W1WrifV2WJ4r3H278f9BmxC\n",
       "tXucRmoSAPij16MHERDPutJKUhqdzT1NVe2qg1l19U2lf09PcueUZ6dXQqvo553ew+cHAZdSCLUG\n",
       "XH1AQhmIXkvuLOJTbWqCNqh2P2kRZ4Rr79uZTVWPuOPo8ixBxu41sztqRmq8bYWAHkXrJfpwxdem\n",
       "8TZ/BS9APxHipZdNiHFdZWM/kUja+BhfNP03Sw1UX2Y+5XFzcWSe9Em1EV2nguI36ldIn3AJdMOl\n",
       "Z3Rh4IT5r5sfNxY26Cyb0Cd/tEr89c1eD25QRiihM+e9kno8zhNnIzv4iwshgpZQKBAGGwlQVXpW\n",
       "Pg1f+ArnXLd5DO7tBJRS1j0jc62OM2MEOgZq0SsoKV1ZofQEJvN+bX6oVkmU3FGL3fKsmpoZUntN\n",
       "ROrSZpmcDQN8nXFgpsb6vWuk3RvBZLNEY3StA/O/7kOk/rfAdZfT1z9q/J1MXXVqK4oCuBjytWNG\n",
       "vw/GZCWNUBCRKoG7pZWVNX1xv+Woq+ogIjSmEeAHV0VgMF1udLPDG+cTAXofbq7qWyHGvmtzBe84\n",
       "GuvN5xn+Y9CE5KX+1Y+NszCqe7BonZE4T0A9+1QXwdLq90HpZigBrrOZIOFCEl4G9+VetfIvP1zd\n",
       "+RUYl1m0I45MJ0QRqDIfcoV5GxDAU//xh4zkYqyQ0Qkg2a8/nlvFEA2Ve8GmU9/l8mgLSNm5PrNu\n",
       "bWtdIIj5HUhD1rVHzm1TQo4hPtjikzX3n5vb+rI3dK5nHeoBreqdw/9Zg76sRyigAEVvUndZ6MiG\n",
       "WE2nvgNbXM2pMcAcXEOfaAgxsQxl8lDrLzpnVqToFHKhL+lmmhlSCdf6YvVQUOfo9IRa/sYjNSsA\n",
       "EakC8NkWBfa2vOpsOJAoBUUxwBhS5qcrFQzog/kpj9aUXux8RDDykR9S5hUEzdFgbc0sJxEBsWr/\n",
       "oavuIVuXQvycyUT4Bk9KmuAQ5MBQtw18FEpU5Jo5c2q4T1RyUn2+j0O16NrnfEd0HxhEvggHHalA\n",
       "xzI8O/x/Z4Hgjr8u3l11zW+fUmV326aZbHq/IkwpxQ3rC7T3g6kGHjrVAzLe9n9rtV/e/hwQ3Csq\n",
       "YAgFro0EMLuyf0Bdqhqc7/wGLaDXy0sPWOn8sPoV8cnw++AuX5MSNEUe12oyYzavPnAK4OEmNIiA\n",
       "5p6rvGjMON4UEjp9HHn5jJH2lSZPnPDO9oXPPCM2RqN8vZ0h+WnWZxRxNFZjAANa4fjIvsIcRv91\n",
       "WL5ef/U92nErfyr+D3aSItf08M1MGpJIf98rKXWmd0OoCZcpz8dsEML0yy1n9esprORTnkfV0VuS\n",
       "/ofvpRz5qJDR7NKDahTa4+OUkC88OKuVWPQJGO+g/TjgdJxeZfwvx41+G8+0nGXrLEqAzuXTs9pp\n",
       "q9yzV9VmlcAUZmFUHGJyHbgYmv1U7uA0W4Z5aNMyePQYeFBJIyHSw75BsDebGTVJcozvCCkgPHB0\n",
       "s4KQumWw3MqiAZvvjjoSEloeCNDGYNKPgoRAHWWwVNDYYgW6JbPushv+EJJBi1OUcAeqlRWgtEgL\n",
       "1Nljy1UEkTnxLr3FDN0RkVxHXUMZO6li76HHWkedLLQdozhDsgBmTx8pmDKtDVyjaH9BoaL3uTEq\n",
       "atUio2zBCWph3987+V3/yLRWYEl9r9dRPMqrGqm0JBdgyvnw/499BOuKwcvYpPrdBi92FhzP7x0C\n",
       "uvC1I6w1zJmAkfteUNtsDTFRTgVGHNNUCW8rcOstoIP87n31wsH9kZibgBkZ8Eac/PddCZMhLaR5\n",
       "X0a3d/9gHQHBGjJxorvGUzlzrU6Vi3UuqBaUSWiHB/FgKe8CyUvg5CJ2P/odPwWU1DVh7mW5b908\n",
       "2VgAsJHzJ/tmjIzOr+hXOek8Uczho5v5j47G7LAlyGroXnnXpC4hmJfbi5S8eD5Z6gE2HnWiT3cj\n",
       "2tFAgnZ+yZyTAr0+ZvDXb7cSY6P0mBaP4cbb+4Yp/l9xJhKJjU5WfAuSvnIocVDb2m7opbE0a1mW\n",
       "9M0YaXKxv1s7d5bLuWA4sh6rPeG8SUCdh0NI5WbuOWdELAG4/guVi7Cu7AfwQdlY+Nk8+7Ccvk/V\n",
       "Lk6wpdiTnQYauTU5gxJnDsWIBfP6+WV0aHMqO8/3LREzoNMSYwDC/HV5hqJnyVVqnZsg9i9xf7i9\n",
       "Kqy2X6j86s/gMtntTHYou8mjTcxvNYQ46ZDi42kH2YQ+TTJ7re2+FhrbaJ6cYBaKY2lA2zP4ggMs\n",
       "vNneCacd64HP0qyVY8qKtI1IXpHonX21nw3tRBn74BWmexJh9pq/5kv3W2XxgcTfxrYfEWkeTiEi\n",
       "Fzh/tN6fvJwOQd48kYq8zBoBf3cyN7HS+hiz8Qmt5+Izrey2zDSKJLzFqA6D7/hubFJgphyiTN1A\n",
       "keekqr7IeRQzRC8BJStaMD3WiL96a9kQNKL3h7c6ABoQAAAN5kGahEnhDyZTAh///qmWAALt1NUA\n",
       "F1Rg7rECy4bsq77wzOfz0TDZJcoQ2ucHDgbv//Q11CPpl2ellmbd1mskI57PAXnKxMvLZRQbq6qI\n",
       "SneCo7ug/U9yw88WI9Asi2pbyvzy9Dak2cBKCMq+z0qdQmbmP35TmoVWo+W+5vqGfGTWzafaqnHn\n",
       "gzPVToR/x4LsOnEiPoRoO3IORqeL4/tJ+22+0OavfueMjnxQiDFC9Ay85w9dAYWZQkUj13LVQt7b\n",
       "1wAZnEKbd1e5W/Tcq3CvLRz3uj3aDlZHQHiF4JUX3zJsLF4ZkgOexjiC9poBdrk2YTDbipKobZ+Z\n",
       "6RVoaXdcXgvpz+nidsNQQ58qqrFxo7lbX/R9I58EL5LKwCddJNQxiimaNCT9m2baYF9aJc3ADfIi\n",
       "pFIOJ9UrcwhsdcpkOTsGIv8Fr+R76p9r5sWd4KbDcUF5701KHomzyGJ7D7n1CXPinliYI+RqNQC7\n",
       "eFVVnd/ZnNUW7/HHP5we4qiC8ET0iycDdnkBRv+JV+PsANlkAy8CQwopJztupFNDRVRgh1ngKRXI\n",
       "Mb32lpMaqazVdWliRFLYq8Wiz3uq2AA7De04LbwMRf5Zj6qHfmMffoAULu1i4IQox/4zARhDd+VB\n",
       "jFUwTeJk8NmxHVnvYO2pA2QV86zVOZzKn6f7RcmMWBr4tWfL3+svbUA2MsuBOxluc5InBiEsgnmi\n",
       "28d+/FDTXRfWMUoW9Fw++3YHH3ZwK/y5T4g90A9Kn+lgaH8yQT773DLjUaTff/4tAf5Rohe8vxPX\n",
       "rxQKrN8rDSMR0snp0RlOB1mgatM7L22lEIDMkLVsuBwIpdxCk5mkF3xulTFR3WhDXJJK3oKEWUbB\n",
       "ErNKjMLPPFq/UoTlMRMECoiv6aU1ufSF87V8xqIFsJTvVY579HNqq/6nUUqeMtg/bqJxqT2blJgp\n",
       "MRzsAvv86kEIqQI7poLIGhfqG+RHN7eiRjqbxj3d2XNtddRhy2AAyS1+SLuP846KUvdQM9xnRPcz\n",
       "pK4jrqSQ1ewQlmi7TpYuQwmlTWwQ6smhwKDNVwn6wde5ptQG+lnWNVj+OLeKWZrm70l046hqknb1\n",
       "oOvXdql1/Q86pc79wwONY/Tht/OldXyfi3Vu5bP5BZlN/C7+DfFOmFzZ7PqnQ+XTGsFCPl04UJ55\n",
       "VZxY/OmkhUzGFTzqHb3f4Ke6p1os7rkN6u54Mp2spR0TqcfxmHOBKIQV6UJxEr1cSkvcykDinGjs\n",
       "Feu5heNsWbJTyA2dY8SQTaIEGl39T6EnXXEXzpCnN3hwBUOQPpeDQPke+X/wf8DzHZyXFkcndHrN\n",
       "coN5FgIHYY4HmjvZzd+4tJ6wlxy9x5kM5qgut+r6rdKvhcXMypJIfbcv9BBDn2V2T50KpFWBsMa5\n",
       "dpD6uYhTEakH+oxYb4ccLW4fKrjEQGlG/sM/4dQzJrO6ql5M2eM2NElK7afBLD8+nDvxvmGsRO8n\n",
       "YcrvB1EUMpr28PHtA/dG2/50qNpBccUOnqMmfdF1ZhH23pRoNVmSR1YOBCZYqXmrbZDG20o+uWuj\n",
       "0Z+JRuxgjhdrTW7fuKjI8kLUP+oWY5Uujq1LOWXAvnHcFuOXBUAgtNDi8oEJLjTKjm5H+XvSfSrQ\n",
       "Ae3GsdNSuSS7aVeA/WyIfW6+sigSirRAtaGkPpZE6mJ2r3qhIr2eOskh6Bo+neNXkLEdvGhoC8C9\n",
       "K6ThiW9DQE9GVi2DD0d1IrD0/1LrJhSVyYEJwa1swBjvCEn4XsAR3b4yqX/S7qTww22VLjipkzwH\n",
       "08A5ykM/C/a4mjVvySRQJO3v0JDYBYroRJZAPXQTCS2fGTYFOxD8uQegns19BO+VPV1xpIMAAf7h\n",
       "VA4d0lY7m/y3FFMvk0l1JLPbT9jLAV+PkPPgWp4iO9r0tjdOax8vDLjbBOQ4fMo0YJwe50qCWwsN\n",
       "QkaLeAEGUQTvCwjUIF7REBMXE4A9jYh8ivNFEqBlOsx4eOpJBhNLnXps/fqhMPI5MTm12GQG76bS\n",
       "G+7eUg21niQZMsZZbmSZqAZEXriPQhtFxC+XzM2XD3BhCeE6HJfj74701fJG4gZSPAXn6YwndxG/\n",
       "fyZkIwWYz91/4O7MXEFJimPH8JPhVoTmYEGJnjdfu/lAhQsprUzfu3PPSWM/VQx7lrqubHFu5MML\n",
       "IuEScyupfFphhlYECJjpDMk+bMOdIozxRTbuy/fwahZ8f8u8b7hwy31t4feiMsQN1LDR36f+vd64\n",
       "PNmHSzGGziWkjyrthjF516A0d39migQavHmzZuoB7voI2cOQaMIjJ2xpknhzms6pXxTIS8CMw3yB\n",
       "+LfjpZRYe5fio8FqN6cdupjCWgc3pC5z8qeBftTKMzAkIpYlOGnurAFhP4VbL5Jz7apAx/dulC/5\n",
       "/KU2sJ0+CEUL18DaRuncwPZ3iTrQa1Wa6ZRZscq72ixbDU/mFmC1HWCPZy6jycueNKCQfbommTLR\n",
       "r3D1EaeLHIflM9Z0PhFj2aa6Juvg1OBy60ffyXShgOY5KNmoqFhS7IdnJKfr1Mqgqg9Rc80d7ZuQ\n",
       "vqGOTaZm0+13qOAcSeL/6yHiC50SeWxMBvsuq8lkal6+uttikk+8WCTT1UJocvCwHkYotOf64A3l\n",
       "GIB+qy17seTGGvIvfNbsBY0HuBBj7qFVEmYbRs4fnF5IbJGIUwA2gxZESMMbBzBSgitjDqnDm/bY\n",
       "n+ZWUSf1QOILho8i5NaS+6SZ0+KTMuxeCWosbw1LhGoMr0aUcrFiuOLTu1IttIg9VeHlAJkCA7dR\n",
       "Or/eoUytQdSYBq+I0I9aQbM/o2voOMEPDPg/US8wk5YX47sFaWspzMAKqdY8hHlrjKgi5Irp/W8y\n",
       "UyrBbwQ0fZoeIt2vWLy+Amo7krUvlwU/BfjsWe56gSkGX9biZi2joLs1Y1T2iU8kYZJUHarXDP2T\n",
       "4IC2GBC/1m/qfdtyokRwGQ0Ih90D9ZyRNtmwmxueRSUr2eIfx2cxBz8u5p8bzuf3Ucg0O28I+9/E\n",
       "nwxb9HsKbrl0Pa1xk+utvHw36qb9/ztiaG0LyqSTDfpUhdcBkiEkP6284nrlXMK6yMfnYkkXsllM\n",
       "7ltPlhxGrhIkeE2jqRcS3YM5vYYoTPNwR4mFlwiYt78BkvqE4INJKO7bGfmEFaTRwUU5HIY8EtGM\n",
       "AWjcMCxIcNaqsVqF02aYT0ZesNnzU6YJN/4Lbp3kk2qGofyyInr3yamsKRZ+ZxzNU7gmXbVOrtZZ\n",
       "EWr7f6gQNPvG14DbzZAx4yCUEN6fd+NXwEU58NZsSvY+ePalSwM3ZrUjaJms33TcnKnAKHp8C5fW\n",
       "Vn5HCWsvhjK5oDC6X85PQ2bLswSudGV65+NIcBiZdW2hVw3RXzk/5aVMldw9HE6+Yh9SM8xpUOXJ\n",
       "hlh9trHVdJOjt7LdNDEzTiPvjUgWJ8piHzvqzOcxKUJNMPhqfEtmlyJYr6R+2aJYY3175J/r00iA\n",
       "O7UkLkegdeX4dkpcGLLrQDdZ7HBWrSfvFJCSFvJSxgzH1b2L3KSKk2vDuoTwoiYeGFWmAu3MB20Y\n",
       "fYhLRfcdkd40zahDjT+PJMLXPVaGo1CyiONTLO+i/w9iDaCxx/xu4z0Fz6LgF1f12XJfWHQlNg3s\n",
       "482B2VzIVtxsb3QBPMORBgqPFvbMR0Tuhh4rfOmgeRyM/e3ARiooq6xT4HZeabD9cKLRBaRVv1rn\n",
       "Oe9o0tONPGg93+3CuTlKBvLOMmHhxgLSLOEi31r0/hp6LjJAh2o0IZRbqFc+/zz5uzdc7+fg1ieg\n",
       "T+gvBK4QrMmaBRZA14YgEcyCvmf5wBD1XG7dfFZldGsmh+FDPR88tZb4h8KdZS0IdpMIdxGB2cje\n",
       "0mweXKHbO4n+dHVSdWdv8E/EB+bn4sySbWm2n4gtCS/duBQmL+mElVhGh/dQkQXQ3SvRXAE6rYEu\n",
       "Egx3N8Bgo+pRNUjTKOyZRiX/fLHaLfVhAeTaTw7Q3UowzqoHkfvYG6DWA4mVNEZYfkvh/XA6Km3P\n",
       "iEEqRTlj0WzufCcSauXcI/Xvr0skTHngzOxltbbtnW0KPq2RVfX4FRjhMXnYjHvUDiUujIIqLc43\n",
       "YNpbylRzb3z/m+zayozlkoRw9LNVI52W6XPB/oVKUTcIWumlr+TK1TEMiDg2oheRzw1RWd1Ve/fd\n",
       "3G9ta7h/0+VUTYpW0U+76llNFI16k81em4wumT+wAKKObs+Znzh0HNLVdUOzXa/dRiSjOKlQ3CNy\n",
       "rRc8VM0Cdcbrqy5XFZKDh3ZYz2iSVkhUy65ge2vARC3L2k5qsuR8plT0/vwlnKx8+CXz8iND20+8\n",
       "6Ft1rl1qOR8EVaduieiWJ2LZCsm+zjtxl0cQsQvWfhtQvIck33KnUriH51HVcLjPvhoOs1Z1Jp9o\n",
       "Ojt5OkcPP2A9hh1/KR3Bld56wi1Ucxz92boGh+KcR4oY/b7YiOXSQC1U5tT9UJZ3ZxG8ivrxb3+B\n",
       "F5ejKjh8HL5tm18Qmc5Kc/LSYhbqDLoIBfzmeoJI/iEqXy7gft1p0HVDExhiAhIw9XDKhJP63u6p\n",
       "NNrj6Ssa2pJPflD8c9kx4eOMkuyAfoWNibCukTpLsXdX/bPBv6OdcdAwuaFxXj0E2p2cEuBirouj\n",
       "XBWCbsuz6XsYSsys8cJXNub+ZtlrgpoD5Rgk1/zERBnjN+no2DF/RxCMuk2VqEW9GkChcGUWGOZi\n",
       "QIh9w5CtL9x3IDACDC/AxZ/pYagyGJohr6WV6M53rlMdvZpBLI8CKZckAr8BDGXtelqIAHYL6XzD\n",
       "EADegQAADvZBmqdJ4Q8mUwIb//6nhAAZP2VDv96zo6YjLe3FNHgxgA5zmp5taqG0Cm2HJ0p88B7P\n",
       "FyOUzBkCK24k1wVx9KxoN4NT/xjHB+x6vnkC7bC8BfFypdngMxbaQk0XjOrKOTnNpql5ywg+1AAu\n",
       "CVf98/wXKRZvWSVOp7vxQQILwwdUddlUuC53khBchaF0JK+OkPhhhFl3SEVnm9CtmAG9CxUiUmQx\n",
       "mTfJWQO2NJ0b3sj4rsml14ftWGCHXdJPJGyTfBC4PIEIPaxBYN+QHQCLe5/AnssI8OPpZ2l1FsRK\n",
       "a7paLIaUFBjJnAW+RYoqZ6dRdZ/5zOQBN6KMxa+W4EtEJX2Q1XM89eIv7Pl51ZLen8Ax86WRGsWB\n",
       "CZh6vf3Jav88GgHiYvHRHHuyNvKHSiQH2ZOIN3dGy6M8jLK2KS/tbuPB5D+UDavqcsbapXk/jOI+\n",
       "nP2xAijpJEcvdhdmWUNEG3Y4xKbAZ7YFf9x7N3f4eeMoZJRsiYnmo9LFk8DlH5nqtQ9g0bKySrTJ\n",
       "bTvYeMmQ3GmEaEJ0TpUoDU3PSy1Rd3aZbXaeRuhtpJSCuZk2Kf18RthXdXscioe0nxdbcM727J0T\n",
       "ZfUKVjWmlLNlHuyKQBr/KD7b/RZu1zD12VqBdtCj/6ZsaiOX0YurC305MNIsZLE/2R0ZYloRXYm7\n",
       "kkjwVCwsZao+41y1/3WjWojUt6Iw6iYYHvEe9TrhjtgKOiQzsVz88Sm1pRBT8FJ4H7ZAGsHYunyG\n",
       "coCHjOfIoARwFRbS1TKs7NtOzVv+kxShkKQpQ5I3wrRM5K6UGUIbIgntHl+Ug2NVhwYjXAMI7eJn\n",
       "OtMwLf/27EPHLoaimlz7Jypxml+fQ2cSBtGZSt4IkohR+JpoA+cKyF+HPb4NpTPYoMjQxaRaMtNg\n",
       "pHp9t0VT6f00GuI0MkV3F4Pw1x620FrHL9SWn4HBE2Hh3GAmoym2lkP3rcpPJtV1b6cFyfBeCIkc\n",
       "I//1WY02XZOnpVAq0ufaEa8IqrhQqKY2JrMIfN5DqP5GEvjSRvcfZgxk7qWU3C5l7lkhC/p+JVq6\n",
       "bPktFcX8VWPwvRDdrAr2Qk7y3zuHDRK0GOdIYOzTcJ8p+KbbOYB5RbFgqWKszDH4zIbQm/07D6Ul\n",
       "kElkeGXIEroRq3R+TcUJhhicJ+iBg32JN9eBtftesUra/nL+pDpb4aCpLNkGIJHaZBrxKuJmZABO\n",
       "VtpNexuC3nsLN6xObMJpG8ymhekWMiDWXsd7wnkVkIwbU9uiuXT/hGRTXWOQvwfBm0gbxBSrovTO\n",
       "zOgEixFnyHqy3/LXlNW7ssbWeVa9yUKobvtVTQaeT+PgzTuuN7GhXq3Z0Tp74bC/6NGx1776SiWF\n",
       "Ccp4qwXeULBZ2lZa5sXxPqTXPIERmP86+LSJsLeKyv4cvy5dzX8EzwS77MiBYrpc1EBHiwaCqT49\n",
       "cEVU3ezKghMGG7uo/Wi0ulZc9GN73JUh278Rnv3EmnDYVJp+GbQe9L7nLVeWIIPAV6c3Bz+tYuPP\n",
       "fPw5uZlfDMzNPzKftkL1Ay6I96lJES+q8VD3lLkX4VK3ufgPZ386/TIyN0gA8ZWgzO9Z7xlprXGW\n",
       "Lt9DqoN+rwF7LYNvmlPwXCj+jxiGtUbuBI6c25PnvWLFIsbP8MvHV4e/Cpfk1h+x1gaHBgnNee7o\n",
       "6qJtJl8ebbdHpIfG9PQtA4/JtSP+lB7k+BnWD2wirGGTR1NUXrjkEd3LaSxMfGhmsLvcEHJhCfM/\n",
       "7IIPR0UvP5LfumaA6kiucCFIeY9kRWVp5s5I9DaOpMO/G24wJzMVorF6/hf8QE5OPzFL3wRLdU/R\n",
       "NkNX2PMfBvq4LGq2/t2Ld4MaP0SdeWHMQOxY1ubwe+eZMe3y6L7uNqdHAHzAHpDo7fUBg3Jrdakm\n",
       "bauXSUqVZPOM4srDv0Sl8brFryY4PI3MHkbGeYvvEfUr7ze08wQjNUnFpFW7IsewgGr7d7EQnIch\n",
       "/Sc9Tmfw4l+0ilerOYWfrzZrfZ7GprO3WnPF+dE+HbNrUKAd3m2+kCUbHVhtJLkXv+JnaXZLo3nU\n",
       "KIqdVE++cYBS4pXRa6XwjuJ9RZXI59t+hEiK0fLS+omvz88tV32oMnOy1gvsG4zD165WXPRHWdFB\n",
       "C9VVQ6sosOfeZbSTTxtsfVgygTA1a+1/JQdcgDnMiVCZ35jAadAZF0p8qhJR5/J3j9nP9wB52a2i\n",
       "/4r0IeVn5yu1kkZcZyDZOADbANdLTaZopAXqgc98bM2VY32+fy9u+DQoKCKCUa8U25G+Qm/vgTse\n",
       "13jL8zeuoXPxpkAEaEiG1OXNM//1jNCIK/z/gIMUxklj3pHenI84ZSdxoWU/mjUQKOsas+nn0a4T\n",
       "cwRIKWXiU0iuvxugHwPEnw4VCv++pisX++R7OquSCR8WG102B1luyyKolnpYOdHIPstFUwMhcbSu\n",
       "jn8G1jrbbe3tPaxwkkROKAJk1yZlPoJ/+CGS/UOuHlZG0ka03Gpp5oLn3DQeMCzQBQWRWiif4lwc\n",
       "pYLUgbFkezl1xC3JkVCXoTm/HF3vySn2pwwNa5bXCCcctnHEZSTUno+n1Iq+NXih8YGZoHQXSshv\n",
       "/qB/YjPL+dN0X6x7D6dxMnEGkj2FvDn2EppcnO0d1LQnkVgJn9msAkx+U3srSRbYbrP3tNaRy3fV\n",
       "yh16H+Ujs63LIK900PanN4vS4nL9qxK68DK9svF+PeigrNSsiwL9rs2w/jYISEZzVB1gtomoo1XS\n",
       "YWNPTLXvJ5p2RKV8/vd24qP4Wi9veyM5slwExNUtaADG0wAGSIYgrTEsHyBcT0ay3mseI0NIWAR6\n",
       "GmWuopLCYpUEDdKPUMQqDgMX+fwTYZNCBVnGjMUS3nkwC0pPuyMYLjAwf7sE5ZSkSDdaHxdolDdp\n",
       "LKjubKxgGFHVH0oXgnw3TSwlWOoce0lw0KCNJETBJPvWLYPZNnyIi05YL+QklbrCxAQqf/vFX5cj\n",
       "/Ei29stcSIQBnFJuMnHhLtPTpe0YlTX2Bz3NjPmp3LNiU2XBbbzxbqimHKd03TDEfz0wmFquRv8C\n",
       "0dLkqmQVJcdyjo9/FQQ2dsHzRc0h3OHeqbQ3d+AGyTQT7iCZrgH/5wfQoLby3S6TruOczke77ZLW\n",
       "cku1nsNJr4RRAMimCVwsfZDhwb7teRlw5VYKVogcYPSxbpkF5v7nylpyaCGPg0bE2mfV5VTjb9k0\n",
       "o1WwHCFnesybud3jUCVbHV8khzc1b7no/AZQOwy5knRl2GqS7tyr9Jz/qXVQfI69tkty83dZaGlk\n",
       "W+Ci7p0PJIo28PSNOpPe9e3M4/UtqnMRR5rFRpKGpwhwlq9Co494OmzyMVxTCA8smG/fQ59rqspd\n",
       "kh2dhOkHU3hLcRv319msd2ub+dZpQ2kfWc1QmKKSDN2X0LOT3r0cIbRxxFnlSWxVptdgUSmRjumo\n",
       "/mA/ogCzpgmO9qdMmmc7jNl4ApvFgvfelnx4qAo3arL4pBhPrsgc1T3thZLfjqv2x1KkKwr8Kd6C\n",
       "stAJKiLHTCrDTZgEj5j9aI4abwXPVDnz+3IOVCKgx7tRTkXlOAopePzxvkcVoOe2jny+72sdY1Mn\n",
       "g0i5LN5o9uDQ5KP9ZK7lb78Xoe1umi+rrhFYoD1TLpIuuqXRR2Bb4JjB4e0PM9unCIihv8/wG6ex\n",
       "CyI8W/kQifAEH45BaU/LepWFihmsUd/mNdgp+PJxhJe+6/BURWu2StS+8HOmXTG9WwhK6BiaraXv\n",
       "ju44MKBtwtKmj94kNGtqVLoabHDG652T19CO0/vfqAKcSHmawuDsRe5hjeMfbF9HEbDhJK3m0d9Z\n",
       "UGYLtmnsSgz8tB+LuyQ15whReUDC6AkHAx/VST52zfCW2hOKGu5U5IoGEHnLEh8TzKLz/UxPYGJB\n",
       "QkKgb8bYwTdccuEbh6WfmLJN5ybxqhD3GDoYi9VGPgH554eHvXcrysE26Z9eT7NpkNrTH7X1zAMg\n",
       "bwsD7drvsOldF3i3YeJYSpfdEkDs1VYHKh9MRbgYza+shQYrhLOSJ6D9RCRCNAf4ww+ITTR/1DoB\n",
       "RFJ6UEglXqNDSqJPMk4m6czxbvXKHO7pDAjiQ0rhwmSmPACxcBWG/dKPsF/++mu6C3SphJ2T0kRh\n",
       "XwbBksAvjGA7Kx19vJfsAP49gLwSmuE8uPtW2oGnnsrqnO2kvhPlF3bBN6nyzuW97ybWgn6qEU8B\n",
       "WvGd2q4vo1QPq+a7qT4nVPyqjIMEBGw2k018E1j7XtEbl8pygunoCpIHQLd4Y1Pz0n2VxKHWh1nF\n",
       "rFERnxJvzxZbIX7EO8V56Yb35yhl93lgekWlj82YCsoO+7oWV1M+g5wbCAaAoF8wFlX0tzhhzEgR\n",
       "i2MSLbAzunKpQA1TT6wSbcyoBb4s8GGW3bYmx+y692aWB8qKsV+zmNouuDW2vyKN1yNWcEnXSqvJ\n",
       "vV7qvXsDbHjAZO0LcPgRnA33BnlegwpGdeDfjEcsqD8ovRfqh2QP0cV9/oTU+HXpOwuXlkxveNvg\n",
       "ga+0a1xsrW6Dn4C7fxfTGQ0VQmgBGg///qRY+8GPdZ7nqcP8TyDhPN8E/HEZGs4okDwEKQdI7Tie\n",
       "W1HzYkgijSsvbvt3/ubbc8MQ8aHiiXobbguq3aRGLWAln4anqgJhpGH1zN+lgObar/l3agatpjVx\n",
       "22+0U7DBtg5AE3vD2EIj4op2OzaB01D1if/veSCBS2RyUwMb8cJLE+JiKsLGecnLHbl5gEL2vRcu\n",
       "S7KTFtRgcPeZU4NF3+731bs7qNHdvOasQyTH5ywzAD5U9DPoB4rLU09blgVm/L/iZoT4HRVEFArB\n",
       "WfcYZoYxvxjbT9WeTEjkrReI9ApVou3rK2KDoayYffbQq68Z9OcQPBRUmMmU9ho+n9OCGJy3aiAW\n",
       "FmyM0uQzye2XIgO040suLcUTrV6Z+FbY/8VNm/cKDkPmNTGvOcKXLK1bqoq3qEBEaGfXGKDJzZO4\n",
       "7K2h5hXvVRu1GhUAKpo8kqd8sr45SKNxa0XcrFSVVzCdOT9zB9y5Z9TM8pc3nZpLlARfPiq9+GSl\n",
       "vZnjtvTKfTI1u6aScYBXqpCb4rryLKdSx7IFLbp07wB9BoWnYHr3TJJZ3GxMe8wJzi3l12S1HZkD\n",
       "NRIwX2/iniTZ4f/5TzaQMnOKyQAAD3xBnsVFETw3/wAI7Gbe8rZzh9b2AAhA6G19sNTNmcsw78j4\n",
       "CJfhsQ1aAZcNZA7jmJ5mSWZbgITSybJC3F7OSvlX/c/hVW/PzQEqGcpVkOVos6apDYwyoqvM3L0d\n",
       "ezr6ck0Uw2P7mhNrnivbRA/AsSU5AQq7WYkp8Wm/iNeJ7ptCTGdQLcD5R535VQ1TxbnuVS1NlU5q\n",
       "3ODsE6ysXCJDiYGkpxihYEappBcHpAXa11gskJp/co6dUj9CnSCjrMkTYM9Be609Y6/tOvryafYS\n",
       "afR3FQlRwUaUAmqyWt8zp2qI5AQfcfeHFtoGmHR843wjJkXE7MAU6BowNFrTlRbXcB9r0N2CkWKM\n",
       "EgMd471k3OOnlkam6e/LB3PZt5iCyNmvlt1g25BvgHhtLBwHEFITNNbZQniWxsXbb+nWXPg+lVqB\n",
       "WB/bqfofUiqvkTJ/4mqbABlCu2zs+evWtWMygrp730jbAru0hhBwHQYtKjU4q5+WdlOgGbghtN/T\n",
       "m54aVHFEgJP+x9e1sp7//5Jm+PfF2sj7niedzEYUlUAfKBOfWO6edZ732zp4tIXAHZlazov3dCl5\n",
       "dWsh6/LC7Xtv+suh236I/ITrj33VS4OD362jkSF6abTdRPH/kJi3uihsOygtEjR/kBoj21iOMsvZ\n",
       "PiR+ssdAz8GRMOphVBdYyEn8W/NwlzlaI4DddPmyaENnhJUEyly1qvbyJIAAxmirBpm/rdJgnUZo\n",
       "Uyv962G5eVhSa/1HQCBdpgfZByUs1Koz//9n7+8HyN0vm/CIvQqgtr+bpvmmnv4c22khAbvbRO1y\n",
       "0JHDXUvAFgk0RwAHwPId3t9rCgVGgmj9jZjXi3tryGevRxauCDKZgXXoTurRdDX6qQ3Ui3r3BN3q\n",
       "+x4S0Nim+h7IfMvUZKHdffb2XsUXeEQ+spQOgTtK8diuU5PXUMxAfRSO3/MyhqXzjWoLM8oxkEnI\n",
       "6Yzn8Ss0eE5+zFfTYTP+mvLMhmmwF27yP1ths579y5h0fC2hy+iCkCGGeFbb2K+ExAwa485rZi5B\n",
       "B4KXle/94W+vDHpbPcM0ZU1LJ4+B8fTbke2sfcvD78129BKMrIOrXaDzDHCbgLbJ8vmI8yTlWCl3\n",
       "E/UD5MOwpB4iC1xtbyG7YzqtEdhERNpiY4HfsP39kL3onkt0O3b/8Y/c+sGAIuhSI64wj7HYnvOv\n",
       "R7LuQXVXBCnDZRdoYqAr0AqxligY+2jbLPudGhyUb8ZJlIByb+d398CJspdBqoovd1T096SdOCKy\n",
       "hFHeXa3stoe5jaCaIFdwm2NeiGtFlUdJRk8cl75+wwr1W3O8WFugmG9LsinMhm9TGuuxiIMwZot0\n",
       "1dqamWhnubGgFVaBDoN/6J46vhe55lImfjDdF3gacYHSSG+TMCuD+eG89GW9OQrSrL0he7CXmwEm\n",
       "yonHVSMqJ3XMKUBkQnM0nPD5fvnCVNnSGVDNF2o2Y8vWL6+T0+F2md27bstSQWbYwU72nKAldkP1\n",
       "zA+COolFhdnMFM2oihNnc6tvO0sVGV3oKxq7sFtfhErLkeDUr3X3rjpWCV6goq8lD8qBBK9GwHxN\n",
       "ncNlZx6SAM4XjD69HHXnfSFv6OV+kXSxanOikS6d3eFetTDy+E/KAoeYqgc1P1gS096BzR8GZVBP\n",
       "YQ+U62cluxsfja01Nq/EapsvaT6Z5Z345wlOYnHXGpJAethxRcBkpHaWLGAhmhMPJSc0xN91lEqc\n",
       "EoP8Z0HH35s9udsgYzHBA+8KFnhct0nA3NsFVFOlXewO70Uuim4s/gzkFLKiE7IpwU9ESGRvpi7K\n",
       "NL0sZIkJtSbdTGS6wFKbjD1NFnoQ4RevC6Xl9l1K3h8CPv6JX1yNAj7BG3YwSzCd5k2lWwIK7D9p\n",
       "VAZcjFudqXQS/aBYgmgUne7gn2y8cWczG4vIxKaFFePvasr1QwCvPJQDdn9Wupyu8gPCjqdUTbuj\n",
       "iR57T0q/uHpjQWkMbQMzabsjLFGZ7MnYRwfSgeSYF88kFYexAmnKnNhRLQL7pfxGWJ9lZFdNnCND\n",
       "wRb46q127/+V76HAY2CCcoEnDAzEzE/KfhYValsvIgjQtMpYlyyoYoZe81AQD7C3t5swME/zPckz\n",
       "zqrdUouFlH8389XtXg4zHNcYGifeKt20wwjfn3/FQvc+T7bWyw1E+Vb8xTpTyv2bBrPzKWZ8knx0\n",
       "0jZr2aGR48vLYGJtb9P+uzEi3nVvyhx6Do9RW5pvAGl5OUwWoZC6XKIqAG9V0QamTpS/Re7V0Cxe\n",
       "eYLG6jadu4bZCejx2+dHu+qxJsmlm3/h/MS3e5Qdz5lwBje9TB5gyykx1W2jb/HvbGWJCiXRx+fe\n",
       "E0VBGOowgHz5XKAucWzNXeOJ8RYm6AUeOi4Vx9BJfBvcuFZWfHut3SSNKbzxIvvlTkJ2TYHZCac8\n",
       "RLizut1OfDsqevVqCPlESfiNYR1+W3/gf9AA6cfajG/HbyCGME1RPN4yDqHsv0bW5lnQNRA2vTk7\n",
       "l4UmkSLlO3XhOuMP3EBRtLIsdYoEBNtZ+ieIL//+doYCcoMi/pzyY5cf9rahpQ9FAb+s7gqSyDqc\n",
       "nfnkgEJ+ehBrCjDbSB28LtX7nwNGyJuhD+Y65gpCIhxrKBFq+HVfkMY8YzE6cseS4V5Sdd2jWZIa\n",
       "6r5fXJ8woEUoFNPoE/mr/obsZmCBcxIaoyZSqtJLx44cMQDliQsmf8VD93Av7rZb7NHY4hnaxFRc\n",
       "cHw6exCwk+rwUqdCfRPTbD/8aMBo7pyWDnsTbTAnDZobBFgnwbsBis4LwCy1j/n9x3iWcf+afP/C\n",
       "FiqrCQOADY0cD+Yjn6KuNo9O7z21gUp67psSu3Rc6KdXgz9t4d7tpyRp9d7OQ7/CBfKClMCrVfBW\n",
       "9chujR5Fnp0pJPpqSxYtibHkyeUZTrkes6zOI0zPWF9XIlkXO1j5K9dS4XWZA6SCzV8h+KdQqusM\n",
       "qMVQjZatiIoJRM6qAc3ITutZDRGXx/IW0VTJYP3vN3zRpntY+4zld2Z0i+Sdvl3UXWUfkHErc5Na\n",
       "aIozV8Kkh+I4oRMqAxwEWWzTWVzF7ZnD9W1Tl4BnkpN/nNJy9mIGxoDqTj7+jBXadeKq0l/dUMkP\n",
       "fdf8sKs7Tw6ajnMApug+oXEMJ0aXmMZCzMV7/YEyHep+VJhcLCvFKRDmNo7FG3or/KznAnsP+Ygz\n",
       "z26cI26zkXmfm86Om86nv9Ltj9ExkIwNEEYIJF+NN57ClRi7aqK8KwUEjn9FpZmf7EsAo5uFjjQ0\n",
       "jPpp8FACPtA+UR0ZamsvstrEU8VgTeBjq+3BrfeID7mwwcE8VrcX28CMfC4k2r1oZcV66cFEap5v\n",
       "1uRJ5/Z6susjWaPyaCIJzacLClGf6jKg7jfbEnWXR1g32aDIyTvgMH+EWoD9J109AFClF9BtGzxO\n",
       "SRs8GkDm4tT+/72aKUFLQvsZLismu9nG++n8kLg9ANP8iyU0440KIazMEFyhVM3V+8Aq1ad09pGv\n",
       "0aT0QWEsIEwaqoIUmcxsWwZGeusW9ecfiSPyO+0xA7Uaa2Y+/gSvKYI5HVl9DU/TqSlCOMUrT2hT\n",
       "wscDS0ls/0SspRrqdLuFfMxgLZkfrtC6q1IZpqXzC+dLhUCCHTfTNrN9QqQITunyVFQs0IDBC0R7\n",
       "hYctcEsR3Yr+Fijxw3Y3yJXkns1gBoduD2LjpB4ogCDgkzk1JiuDDMLigGklARF7viN8n4t5lxEW\n",
       "r87/mFbNOGmn0TCPb7Slf91T8sZJyDtqzRt6++jM0P6RjUUElAjuE/g/mESaIa9dElgkpwmM/MhO\n",
       "gnLSVOwssBRf1lwjazuOvwMvBtxttvfNCL6CbJz0EB67m7VbWsqf434Ah2rkF1y91DN1TtHo1Ol1\n",
       "7L1VmtGtx0BqFj45qBjfczNIELqLH5b//eS02PW9IY/9zvd1gAWr5dEGXLKqXiApAKtCaAM0q6Z5\n",
       "uPGZwN0Uv870wj65i/eH97fuXa6LL3awwxSVhVIUmvcfkZp4zcTWvkq+gcw4V0k7SAthZuALCfMN\n",
       "mlMQ1SSGYO5YbJMUhR8TTfAhjmk7dIZShqPTPgJNzw6oouxbjoMIOj5FmndNrJJIEDeXF1fz1x6r\n",
       "QL4I5pOowFF+CBQ1sCQdmWaZkJbxWPlrKdUQhvDf+ToQ7bAAffwlzEecrixD9leoKHKg9SITslhF\n",
       "2LbD/Uxs1C5kTynuNxuhan7BAVGVCIzzdkU4Tahg42yjNTUpU/ulVc0EKh+7zZaiB+Phq4ejHKWM\n",
       "dTesxHQhhL7BeHGHGhcHqbvDPlbzsTIuKT7PNE/f2d4ohHjN+ngOaw3d1furpH1o8JSdUNWuuXz6\n",
       "1Y/PgAeZPnqXhQ9Sp9s7l5KajGDO4IXhvGewdVRQxwq8odI+qi1BCubGcFaM9z7vTIGb5nPmD4Mn\n",
       "cQZafjKMje2nkAp2tvWjjexMOHNFuBQlyFHSxdW98gtyc1aNntduyYchSO2V9tmdO7kj74VsJZHj\n",
       "FD3kj8q334m3yyhVjdmbDxqlgAukrSlIet/oZIxojAhN6xpthABfsL8ckGlZCsVG7yu93oli5d80\n",
       "FklQxFNNNpY2k3dswIaNhXVi47p3FoGj0iP0NMENUGwHcuFoPfXX99SgOtHnQCHqoNsvk8yBIKV7\n",
       "OEUY/bSvoFVteUihkB3YXnuvinh4JmD5cFZssv4FvCE74iIxo5b8WrA/IHLcrTdaRfamrv1f+S3f\n",
       "BF6B3vgNrjdUAKITuqUsVZ5US/C2PQpohRPjY3DmuaT/BsfZQkxLc1GX9d52FoCdya+/e6vkx9bg\n",
       "3Kt2clPaj99uvdTH0sf52N1jSW8CtsfYtLpuLUyPGu4KtEgYkVSvAzRfHqbugcudVs4CcBj3jk63\n",
       "fpp0+5Id3vGyjZ7nmuvrIHP9hhL4o7VCOF4nFb+qd/x2H9hGaE2n/z0UFGm0j9I00INWjgWw0QXO\n",
       "eQjA6EBUfcyWcP0otHbcWr1BZ8+FW8rZEponYWmp+/yInFMCiZSFqCzdRWJDw+juOldyUu3Fzmfv\n",
       "yUMyoSq1dKirRvpfqX7yJ0/Xgt1J5xvLrLkFjTmAuGXzyhLCGbdgVezZdlDWxcrbrs+7qQT/EdNm\n",
       "0AKMDJzICozeq+uL8RHgKd5IqWEQ6wBG/vRPLv5rmjc3i9DndMN/dtseu5aiJhSFseTsEfwxdpD/\n",
       "uCA+V211O0GMTHu1RW+u/jb0aSZbjbkQ7g7SWVL/UC5z/xnpeNGTnt4Lq8V07P05nPULrTzpg5vO\n",
       "+90Fv2NN/ls3pbA5X01a6qiD39GL+TdhePyN25SaXg1CewHij15Nvk9f7uPhX67KH5JONgE3AAAO\n",
       "pgGe5mpDfwAB8LME2uNCsiBDHtkjCvVTLa04bvykgCxCXZ2iOYsBFm+TssB40F1i2Dx/HEAYwPA9\n",
       "Y8qOStmecT4b9/9oK945rRTPLCx8/21eYT587Li27rNUrdpN3/sjYBW6fWmPDuLx6gtPzztzEbAc\n",
       "REmQ24EH15Nl5BZf0C4ag2jB8VnBS6Pnrq8dMxb2BKghOKWV9i/PtkjwP36N+PUdhKxofbhNfRBp\n",
       "O99/2aY359OEXN3UxkpDeyUBubWRG13j/ouGVkn1Id+XaDger+A7LIVwrgS9z+9Rv4szcmfDvar7\n",
       "0rDRrhciFLRL9K5jwO6PhD67AYcgGjSuMPzsxrhBOEqAWXB/YmL3ZPuWizknwuEwQvOa6aN6SVlP\n",
       "A4PVfH3+jyv/3a9UDEZN11fxhRJrJ4wvDjzrZ02oUTXMN6R2jksO7UH9q2uBHQv3jOsR8+RULC2F\n",
       "6bxBZjv0wiYf2hEdklsY2d13b8sOfbn7kxKP23tVYnD0pQKDn5CssQoeRFT8iA9+j0gtrZ/BjkaL\n",
       "wUYllFRrq3CbRYxwvYJcgUPv4ihcOXUCRHjWGQg3RNnnz0FODBToj44Lq1P8x8ux1WNpzPGU5sEG\n",
       "Nxs0RCTyoRS8Usstn6zOXH7VzOFgAFMSRRZ3yXBacck0fDpTRpQ8PKD2zZyHML/7nKI2FPXlvLXC\n",
       "CWJgBtySwg0vqj5UOnTzfmmb+1fh4resM5cKgWn/7k/jdwl0vXiYS5ZjBPre32+rYmBfDP52BgVp\n",
       "0PmGivmttVKqya5o76a4lhP9xmfuhYPxMjNpH6to98dxN4hUCap9Djuigt/9wyfeOGLIgwXV7Q1u\n",
       "G7NJz7BhffRk7JPoCXdUMHqzyemRxyYDW6Pn0fvKWhPPyKt9hibeKOEHwSFker5ky66kEurD5WrT\n",
       "MgrslKWHCj+AjL4ycdYeBlhsAjeJmZQ/DHs01KUGCfJDjR5fG0bWwgrwi0J87NK6KZuN5U6d10K6\n",
       "E7AA5GpmyZwfqbn6cjiU9HzjidSGSnPwVYsg6GQ/fv0xp4PLCzlhHHxN8fezECzJ6056ngINR/qY\n",
       "RHFRdyu3IKCbO37EuAhtA6jhH0B74rxek5qQPWrFDOVY0R6FAqjpI1Q9KDI+IteLyTsEKfVIS30T\n",
       "ZINOW2c7LZH4npJpak0lSs7/uWMcMAGRJV7m4PjbMeiCNMkQkRNPRMvcHPOW4/7ixlk1ofYlDqnr\n",
       "T1VRx63bhHQaBPoKD8ADCYZp9MowmIapQe9Izs5K0tSXGF8VT1f/kVeYH5XJzb/zr+0GexmrcuJD\n",
       "1XUbYIPt24TWxlNAxVMnWBkCAKc9UUeYdMB6xCnu2mE5OP7vG7phYhTt4QxJcQcchiZVpAHjzga+\n",
       "bMzYBp51A/44sfSk+M5kKUQt8M3AzUMEGVjmTX7aFeSMUI75aCK4AjkpiLPMRTEKf20lGHSD2ZKM\n",
       "RJSZdXlh61r5HpXqitT057lpDYIjhbUhyhnJHfDrxtR2j/y04nyxzI0v0LXZrPDIiHD0campp9fo\n",
       "Yt0b3VCYC8rwyJCUfj3LwMLT3SGR4rlhPQsKTc1uZ7lSRJUYak4DB+qYZLh5qdo3hD3homvkQjrl\n",
       "2nB1U2QKEe9bP01BihL7L0+vCGUTWO/DNUOWoSfEhuMA7YRClZC617ghUOZrAuZK86XHr0cXw1Mu\n",
       "/V+7a3m6xlLfYSXjYlN2Uo6EfbgmcyT82E3v1r/qvlC5IlS8yR/Sq1T+4K1bTgr42mWwg3ECETGC\n",
       "WhsTWSVK7QBvPmrUrV8MTIuU/GvUFLlDGpcoaeazJm7ZzwGRLobaRTmuOetMBb02PsOokflHNGDS\n",
       "wGRtinWMUJ8nGqYOQlLHEgkXlXouqEK3yvMPBYRhom+bPtoAe75LwraBWEr/kMyHWyTZWZ+VQ+vS\n",
       "CT+Py2XLKSVGH2kORo0+pJf3agGyRCa2vgCPCYueooUCqBzuA+xBFmmooAvzkAA/40/vsK2KmUrS\n",
       "4IP5SGBGnCZhGzI6bj5yRbSq2qqz2if4AaYzUal8czZ/ajZyJyjfG3QRE8EmD/C4Ku/fItjdfqU7\n",
       "YnGW+ETdxl9G96Q4AYCoYjKxJrmc+/kna9rGCEqxPLq4BXHT3HclAusFusqupomiy/byLRfx7Ry5\n",
       "HYQ063rNGfEN8jIQj+bcDE357FNdcFkU6MH4wui8+Q7D8gkk2d9yswcLwGtRCmZ1LwVh4An9bp2U\n",
       "knDnwXivZJ+SciH0TGLNbWvDqTzIptBtBRc5NH1INgdhSQ24avVLS26kn795ZdEW/tnWf2QT6w43\n",
       "Az+MS9s9S/0W8th2A3CR7+KBXBMdJJUTIS66ViGs9Yq9D//H9YdctX2sLKWcyh+kqfKekCB/k3M1\n",
       "vaISvbJyLBbjYypoVntan5x4AcTZPmVZZOHU6+IY5nxzjKU3djYlnnF96ZHX6YPvGnx6FbbjoR0z\n",
       "kP/BtoGYHDoKXXFrqy+LlZMv5IqPteeW5Gy/X4bwdKpBfY/Xp/QDA8DZjpBwgEqhXAb6zjiWb4dM\n",
       "Kqm6txBPb/2+tWTwOZr6sSMok5G+yd65KNzFpF385KLg1jzpxJOOyYBSJ0DoiNOyElkyA16wj1AN\n",
       "qEDDiz7dzaG+R/7a/b1J9dD6LF/ePizM7Y7gXonOjglEZhAQtcPPMYagoMUinUWjL6a7LLwuqkdA\n",
       "kCecv2GWTvh6ADfE1v5idQqQtGx0HjUSmqD4hBfREzJBwFE8p7qTbw73U5wn9In85QgdPf/yn7b4\n",
       "1ZzkmrB2w87RwkMfVGGC3JTWCNm5szofspa5pX5i54Ro0KCGEJGrJDP9KrROG53v/UCxFhwVhNOO\n",
       "XTHJGjvLTHMFzg0JVqhnomIe3p4yXtsHHIvn5kcvXQCnT77A1qJ/bAZO5F3m7synnNeePHWYRffS\n",
       "D1LWT/QvaDp+y9gzCcjG531/IWrdUyDBCXvxfMn0E6xcU3mejTI9+MCsPq//oWPknDaL2T6nmkmx\n",
       "wQ3l4cgsTPB4m6OFMqCGZ+hkEU7mtQ8J7FPr3nZTWSJLv3kM3x84sfkfyTVDFpOqQvPrhokv/gLz\n",
       "IJt/rwjUKn2T7tH8inc9zug1uu//vdAuqLtkQsSbiFoF/NoD6MQbK2TauGl8DLkAwaxqy2/A46AB\n",
       "058bwYsgngN6fm8c96iuv9D2TaJo6RQ1MzSHl6gow+463p7RDfjt5pEzwd4/l5BCkh2Sk1Hb9Uo6\n",
       "sidTCBrLsYw86bsFkqu29CjGqAdMFpAR9D0QjNFoOzj7eZzVNFY+rdXQtQCQQhpBF5e+nbH2OnpI\n",
       "UIpkWalFYMDh1worB/gTdddsW7d80ceAV4h8iNKOJB9F+OgClIbrWo3t4osplpB1k0+JmDLWCcyC\n",
       "JNBj3jVYpYZhHBBBrTmUyHhuzryTl39yBUyE6Ola8WFRMMWZJ55XXMSp4vRHUcdORSqH+ka1fWdn\n",
       "nIROAmhu489gJXmkSfkN6fSVCF2hFAfgsplHFU+TRlbS64OmJ3c4P8LM4VmC88koNrrIZuI8OgWE\n",
       "OyPpctOH61B4Ps5MCYhyr1ZTxqD4w39/M10MwamJMUL842sP6cIsDo/pJhqbrzZFPPtuizWYNZCm\n",
       "x5uOY58hddRKUqyRNLerCWHvq2P7WH54angwkqa/inCLFfLspjY1QS7ociPjhdbnDHEJy+A6kD/w\n",
       "dxBrZgToS2mA0c3oLypZB+Rs1wvi98umtLa3B7mB2u575bmYMZbnapSc6S6R8qOtl5M5mF/Mpxdb\n",
       "QrAUjgGDG2aCtlgJwljLKdTb0p8dAG8QsovUep+LaHBJrzTSlehkUrLU3+UmbxTBJZFF2zYcgE3H\n",
       "OSlGPHvL8AbvgyVCvHHD6+u1EWNFB6pEFZtJXSBfc+a3hkQ4uRqrwmisjPvmqExXl2cg6bFq9gXu\n",
       "NhXF1G7tDvcdX7k7nnwgv6b2/+BM0yE5uTS7Ebjf9P9mbG0gc0CvRAOawH4QHwc9KjNUsLZjNeF4\n",
       "zWIxPAcL9YCe/zyhKF5rdXshrDszWQOTtkdH1tHWb2dLCTE2idq9wk9TqRILlUvR98tDZAYWUV64\n",
       "kW7rj6wg0fxoe+j/3kQZY02CKAsB4k5APNS9sJRD29i7FLj6JSfErboRzq1ubdgKNLUOVQrm+wfi\n",
       "/CL7ol9oTf66ATDdFLSuatPEDkmLXdLlJHerQ4u9FTH2/m7Xhj6iZK/XjjnEJvd4PF5wm9+ctdT4\n",
       "1CP49eEbcV/qQDov5P242CAjMqCezxjpn31NB2hBTxomK1S/eLAA2NxuaMcFfjFxW7FDqlx84caT\n",
       "m+fmGfdzoWvh4Erd58kTeIq0QU2vcwLMR11Y5ZEQ+LOgjPB7uGvAEt7HXh+wtGonUyvVF5L6Nsmk\n",
       "/a9IYtQHmeddCdl5o/nYDyN4ZHXsLCheIBQ2ywAFtrl7PfteKNjEA/86ZzISoIudbmatH2PYED8u\n",
       "Y5CXnE1rsEXlHkxy+UYNnvQo8wJYW1bGGB2MehFaVeUF0fBsyB7Q7qGnHeiZVXJSNSkrEIDxfA5D\n",
       "Y8J5wbtx/bDWNrhXd5VpysOPZEfhctdJ426n4MW3xDESjIBqSEnAq8m4Kt4Q3yawM6zEbbfEkmc4\n",
       "lzQPy4pUpxHAvIcXHYgqNzD0ITN7LuO0FPabK6J94aI/hfZDuvZB+kbFa9s7KjbdlWxGF2zMQORI\n",
       "fDS5Xhl4cetEJuJsJnWO9KiQhX41d7DoyRJ1ef/SPvoza5EP4TZlyoCQFrP7gRrQ6kLi9i495X6h\n",
       "6jJk3aBizSY7TT8p33CeKCkcABVOGP+624g39JjC5Qc18E0u1blWZvt4bsgth+kiZe1nMniI7J/1\n",
       "rBMUGVfbRisRBMDQguBWeJND2U/9DGZMrY22ZLZHfYeEsJNB1aDRZm+PRcSwlWl2VmFbOPJYAons\n",
       "T2bn+pjjsEQIZ+SfD7e+0Yu6+zyxpXZBMILHK+wAtKSqj2cBtB6BYpoXw854Fl19GKjULWRIQqRp\n",
       "FCFfqUyKBmOI6J0gWmv2H60p4LaH2TFZRuvXogaZTyVtmG8jRWdqGM8BbBBewQAAA2htb292AAAA\n",
       "bG12aGQAAAAAAAAAAAAAAAAAAAPoAAADIAABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAA\n",
       "AAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAACknRyYWsA\n",
       "AABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAADIAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAA\n",
       "AAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAEOAAAAfgAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAA\n",
       "AQAAAyAAAAgAAAEAAAAAAgptZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAACgAAAAgAFXEAAAAAAAt\n",
       "aGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAG1bWluZgAAABR2bWhk\n",
       "AAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAABdXN0YmwA\n",
       "AAC1c3RzZAAAAAAAAAABAAAApWF2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAEOAH4AEgAAABI\n",
       "AAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAzYXZjQwFkAB//\n",
       "4QAaZ2QAH6zZQEQEHllhAAADAAEAAAMAFA8YMZYBAAZo6+PLIsAAAAAcdXVpZGtoQPJfJE/Fujml\n",
       "G88DI/MAAAAAAAAAGHN0dHMAAAAAAAAAAQAAAAgAAAQAAAAAFHN0c3MAAAAAAAAAAQAAAAEAAAAo\n",
       "Y3R0cwAAAAAAAAADAAAABQAACAAAAAABAAAQAAAAAAIAAAQAAAAAHHN0c2MAAAAAAAAAAQAAAAEA\n",
       "AAAIAAAAAQAAADRzdHN6AAAAAAAAAAAAAAAIAAAXhQAADy8AAA9XAAAOfgAADeoAAA76AAAPgAAA\n",
       "DqoAAAAUc3RjbwAAAAAAAAABAAAALAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAA\n",
       "AG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTgu\n",
       "MjkuMTAw\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original, _ = next(iter(val_dataset.skip(2)))\n",
    "encoder_out = autoencoder.get_layer(\"encoder\")(original, training=False)\n",
    "decoder_out = autoencoder.get_layer(\"decoder\")(encoder_out, training=False)\n",
    "plot_volume_animation(original[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb9ace16af0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_index = 0\n",
    "z_index = 3\n",
    "fig, ax = plt.subplots(ncols=3)\n",
    "plot_slice(original[batch_index, :], index=z_index, ax=ax[0])\n",
    "plot_slice(encoder_out[batch_index, :], index=z_index, ax=ax[1])\n",
    "plot_slice(decoder_out[batch_index, :], index=z_index, ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset, labeled_samples = classification_dataset(\n",
    "    LIDC_SMALL_NEG_TFRECORD,\n",
    "    LIDC_BIG_NEG_TFRECORD,\n",
    "    LIDC_SMALL_POS_TFRECORD,\n",
    "    LIDC_BIG_POS_TFRECORD,\n",
    "    return_size=True,\n",
    ")\n",
    "labeled_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_perc = 0.1\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, None, None, None, None), (None, None, None, None, None)), (None, 1)), types: ((tf.float32, tf.float32), tf.int8)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset = train_test_split(\n",
    "    labeled_dataset,\n",
    "    test_perc=val_perc,\n",
    "    cardinality=labeled_samples,\n",
    ")\n",
    "val_dataset = (\n",
    "    val_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_3d_cnn():\n",
    "    input_small = keras.Input(SMALL_PATCH_SHAPE, name=\"input_small\")\n",
    "    x_small = SeluConv3D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        name=\"small_selu_conv3d_1\",\n",
    "    )(input_small)\n",
    "    x_small = keras.layers.MaxPooling3D((1, 2, 2), name=\"small_maxpool_1\")(x_small)\n",
    "    x_small = SeluConv3D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        name=\"small_selu_conv3d_2\",\n",
    "    )(x_small)\n",
    "    x_small = keras.layers.MaxPooling3D((1, 2, 2), name=\"small_maxpool_2\")(x_small)\n",
    "    x_small = SeluConv3D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        name=\"small_selu_conv3d_3\",\n",
    "    )(x_small)\n",
    "    x_small = keras.layers.MaxPooling3D((1, 2, 2), name=\"small_maxpool_3\")(x_small)\n",
    "    x_small = SeluConv3D(\n",
    "        filters=256,\n",
    "        kernel_size=3,\n",
    "        name=\"small_selu_conv3d_4\",\n",
    "    )(x_small)\n",
    "    x_small = keras.layers.Flatten(name=\"flatten_small\")(x_small)\n",
    "\n",
    "    input_big = keras.Input(BIG_PATCH_SHAPE, name=\"input_big\")\n",
    "    x_big = keras.layers.MaxPooling3D((2, 2, 2), name=\"big_maxpool_0\")(input_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        name=\"big_selu_conv3d_1\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.MaxPooling3D((1, 2, 2), name=\"big_maxpool_1\")(x_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        name=\"big_selu_conv3d_2\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.MaxPooling3D((1, 2, 2), name=\"big_maxpool_2\")(x_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        name=\"big_selu_conv3d_3\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.MaxPooling3D((1, 2, 2), name=\"big_maxpool_3\")(x_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=256,\n",
    "        kernel_size=3,\n",
    "        name=\"big_selu_conv3d_4\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.Flatten(name=\"flatten_big\")(x_big)\n",
    "\n",
    "    x = keras.layers.concatenate([x_small, x_big], name=\"concatenate\")\n",
    "    x = SeluDense(128, name=\"selu_dense\")(x)\n",
    "    x = keras.layers.AlphaDropout(dropout_rate, name=\"alpha_dropout\")(x)\n",
    "    x = keras.layers.Dense(1, activation=\"sigmoid\", name=\"final_dense\")(x)\n",
    "\n",
    "    cnn_3d = keras.Model(inputs=[input_small, input_big], outputs=x, name=\"3dcnn\")\n",
    "\n",
    "    return cnn_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    keras.metrics.AUC(name=\"auc\"),\n",
    "    keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "patience = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "43/43 [==============================] - 1s 34ms/step - loss: 0.2719 - auc: 0.5891 - accuracy: 0.5641 - val_loss: 0.1946 - val_auc: 0.8571 - val_accuracy: 0.7333\n",
      "Epoch 2/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.2048 - auc: 0.7511 - accuracy: 0.6981 - val_loss: 0.1616 - val_auc: 0.8829 - val_accuracy: 0.7867\n",
      "Epoch 3/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.2036 - auc: 0.7542 - accuracy: 0.6996 - val_loss: 0.1392 - val_auc: 0.8932 - val_accuracy: 0.7867\n",
      "Epoch 4/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.1878 - auc: 0.7916 - accuracy: 0.7231 - val_loss: 0.1314 - val_auc: 0.8996 - val_accuracy: 0.8133\n",
      "Epoch 5/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1858 - auc: 0.7938 - accuracy: 0.7187 - val_loss: 0.1396 - val_auc: 0.8964 - val_accuracy: 0.8000\n",
      "Epoch 6/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.1794 - auc: 0.8072 - accuracy: 0.7364 - val_loss: 0.1293 - val_auc: 0.8993 - val_accuracy: 0.7867\n",
      "Epoch 7/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.1594 - auc: 0.8481 - accuracy: 0.7644 - val_loss: 0.1361 - val_auc: 0.9004 - val_accuracy: 0.8133\n",
      "Epoch 8/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.1626 - auc: 0.8421 - accuracy: 0.7541 - val_loss: 0.1370 - val_auc: 0.9046 - val_accuracy: 0.8000\n",
      "Epoch 9/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.1515 - auc: 0.8629 - accuracy: 0.7703 - val_loss: 0.1298 - val_auc: 0.9182 - val_accuracy: 0.8400\n",
      "Epoch 10/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1529 - auc: 0.8590 - accuracy: 0.7703 - val_loss: 0.1287 - val_auc: 0.9207 - val_accuracy: 0.8267\n",
      "Epoch 11/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1450 - auc: 0.8746 - accuracy: 0.7894 - val_loss: 0.1276 - val_auc: 0.9132 - val_accuracy: 0.8400\n",
      "Epoch 12/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1385 - auc: 0.8855 - accuracy: 0.7879 - val_loss: 0.1293 - val_auc: 0.9111 - val_accuracy: 0.8267\n",
      "Epoch 13/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1371 - auc: 0.8861 - accuracy: 0.8174 - val_loss: 0.1270 - val_auc: 0.9104 - val_accuracy: 0.8133\n",
      "Epoch 14/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1328 - auc: 0.8927 - accuracy: 0.8247 - val_loss: 0.1264 - val_auc: 0.9182 - val_accuracy: 0.8133\n",
      "Epoch 15/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1335 - auc: 0.8927 - accuracy: 0.7968 - val_loss: 0.1269 - val_auc: 0.9161 - val_accuracy: 0.8400\n",
      "Epoch 16/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1399 - auc: 0.8810 - accuracy: 0.7982 - val_loss: 0.1427 - val_auc: 0.9207 - val_accuracy: 0.8133\n",
      "Epoch 17/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.1304 - auc: 0.8977 - accuracy: 0.8071 - val_loss: 0.1489 - val_auc: 0.8943 - val_accuracy: 0.8267\n",
      "Epoch 18/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1225 - auc: 0.9078 - accuracy: 0.8336 - val_loss: 0.1282 - val_auc: 0.9164 - val_accuracy: 0.8133\n",
      "Epoch 19/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1231 - auc: 0.9076 - accuracy: 0.8174 - val_loss: 0.1465 - val_auc: 0.8946 - val_accuracy: 0.8267\n",
      "Epoch 20/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.1186 - auc: 0.9137 - accuracy: 0.8439 - val_loss: 0.1275 - val_auc: 0.9243 - val_accuracy: 0.8133\n",
      "Epoch 21/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1126 - auc: 0.9228 - accuracy: 0.8336 - val_loss: 0.1327 - val_auc: 0.9236 - val_accuracy: 0.8000\n",
      "Epoch 22/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1045 - auc: 0.9327 - accuracy: 0.8527 - val_loss: 0.1379 - val_auc: 0.9254 - val_accuracy: 0.8133\n",
      "Epoch 23/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1022 - auc: 0.9363 - accuracy: 0.8630 - val_loss: 0.1291 - val_auc: 0.9246 - val_accuracy: 0.8400\n",
      "Epoch 24/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.1141 - auc: 0.9217 - accuracy: 0.8395 - val_loss: 0.1238 - val_auc: 0.9189 - val_accuracy: 0.8267\n",
      "Epoch 25/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1040 - auc: 0.9331 - accuracy: 0.8645 - val_loss: 0.1364 - val_auc: 0.9093 - val_accuracy: 0.8133\n",
      "Epoch 26/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1048 - auc: 0.9338 - accuracy: 0.8498 - val_loss: 0.1269 - val_auc: 0.9189 - val_accuracy: 0.8533\n",
      "Epoch 27/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.1012 - auc: 0.9386 - accuracy: 0.8454 - val_loss: 0.1219 - val_auc: 0.9304 - val_accuracy: 0.8133\n",
      "Epoch 28/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0946 - auc: 0.9450 - accuracy: 0.8763 - val_loss: 0.1329 - val_auc: 0.9186 - val_accuracy: 0.8267\n",
      "Epoch 29/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1002 - auc: 0.9387 - accuracy: 0.8837 - val_loss: 0.1343 - val_auc: 0.9239 - val_accuracy: 0.8000\n",
      "Epoch 30/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0949 - auc: 0.9465 - accuracy: 0.8675 - val_loss: 0.1305 - val_auc: 0.9236 - val_accuracy: 0.8267\n",
      "Epoch 31/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0873 - auc: 0.9528 - accuracy: 0.8763 - val_loss: 0.1301 - val_auc: 0.9257 - val_accuracy: 0.8133\n",
      "Epoch 32/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0919 - auc: 0.9483 - accuracy: 0.8733 - val_loss: 0.1344 - val_auc: 0.9168 - val_accuracy: 0.8400\n",
      "Epoch 33/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0834 - auc: 0.9575 - accuracy: 0.8910 - val_loss: 0.1350 - val_auc: 0.9282 - val_accuracy: 0.8267\n",
      "Epoch 34/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0829 - auc: 0.9599 - accuracy: 0.8895 - val_loss: 0.1294 - val_auc: 0.9314 - val_accuracy: 0.8267\n",
      "Epoch 35/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0884 - auc: 0.9539 - accuracy: 0.8851 - val_loss: 0.1199 - val_auc: 0.9211 - val_accuracy: 0.8400\n",
      "Epoch 36/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0761 - auc: 0.9636 - accuracy: 0.8999 - val_loss: 0.1356 - val_auc: 0.9211 - val_accuracy: 0.8133\n",
      "Epoch 37/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0775 - auc: 0.9631 - accuracy: 0.9043 - val_loss: 0.1492 - val_auc: 0.9161 - val_accuracy: 0.8267\n",
      "Epoch 38/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0756 - auc: 0.9658 - accuracy: 0.8999 - val_loss: 0.1409 - val_auc: 0.9129 - val_accuracy: 0.7867\n",
      "Epoch 39/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0723 - auc: 0.9676 - accuracy: 0.9116 - val_loss: 0.1387 - val_auc: 0.9168 - val_accuracy: 0.8267\n",
      "Epoch 40/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0694 - auc: 0.9697 - accuracy: 0.9161 - val_loss: 0.1351 - val_auc: 0.9143 - val_accuracy: 0.8133\n",
      "Epoch 41/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0698 - auc: 0.9687 - accuracy: 0.9072 - val_loss: 0.1376 - val_auc: 0.9125 - val_accuracy: 0.8133\n",
      "Epoch 42/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0698 - auc: 0.9730 - accuracy: 0.9102 - val_loss: 0.1332 - val_auc: 0.9189 - val_accuracy: 0.8133\n",
      "Epoch 43/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0595 - auc: 0.9819 - accuracy: 0.9381 - val_loss: 0.1331 - val_auc: 0.9161 - val_accuracy: 0.8133\n",
      "Epoch 44/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0618 - auc: 0.9798 - accuracy: 0.9249 - val_loss: 0.1300 - val_auc: 0.9143 - val_accuracy: 0.8267\n",
      "Epoch 45/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0592 - auc: 0.9819 - accuracy: 0.9323 - val_loss: 0.1417 - val_auc: 0.9111 - val_accuracy: 0.8267\n",
      "Epoch 46/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0556 - auc: 0.9818 - accuracy: 0.9411 - val_loss: 0.1329 - val_auc: 0.9168 - val_accuracy: 0.8133\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0529 - auc: 0.9846 - accuracy: 0.9426 - val_loss: 0.1399 - val_auc: 0.9236 - val_accuracy: 0.8267\n",
      "Epoch 48/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0571 - auc: 0.9813 - accuracy: 0.9264 - val_loss: 0.1345 - val_auc: 0.9211 - val_accuracy: 0.8267\n",
      "Epoch 49/1000\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0544 - auc: 0.9853 - accuracy: 0.9278 - val_loss: 0.1348 - val_auc: 0.9225 - val_accuracy: 0.8267\n",
      "Epoch 50/1000\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0538 - auc: 0.9847 - accuracy: 0.9278 - val_loss: 0.1404 - val_auc: 0.9100 - val_accuracy: 0.8133\n",
      "Epoch 00050: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = build_3d_cnn()\n",
    "cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=metrics,\n",
    ")\n",
    "num_epochs = get_best_num_epochs(\n",
    "    cnn,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    patience,\n",
    "    \"val_loss\",\n",
    "    verbose_training=1,\n",
    "    verbose_early_stopping=1,\n",
    ")\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, None, None, None, None), (None, None, None, None, None)), (None, 1)), types: ((tf.float32, tf.float32), tf.int8)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = (\n",
    "    labeled_dataset.cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for more epochs because the training set will be bigger\n",
    "extra_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = build_3d_cnn()\n",
    "cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "model_fname = f\"models/3dcnn-lidc.h5\"\n",
    "log_dir = f\"logs/3dcnn-lidc\"\n",
    "cnn = train_model(\n",
    "    cnn,\n",
    "    train_dataset,\n",
    "    num_epochs + extra_epochs,\n",
    "    model_fname,\n",
    "    log_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pretrained_3d_cnn():\n",
    "    small_encoder = keras.models.load_model(\"models/autoencoder-lidc.h5\").get_layer(\n",
    "        \"encoder\"\n",
    "    )\n",
    "    small_encoder._name = \"small_encoder\"\n",
    "    for layer in small_encoder.layers:\n",
    "        layer._name = \"small_\" + layer._name\n",
    "    small_encoder.trainable = False\n",
    "\n",
    "    input_small = keras.Input(SMALL_PATCH_SHAPE, name=\"input_small\")\n",
    "    x_small = small_encoder(input_small)\n",
    "    x_small = keras.layers.Flatten(name=\"flatten_small\")(x_small)\n",
    "\n",
    "    big_encoder = keras.models.load_model(\"models/autoencoder-lidc.h5\").get_layer(\n",
    "        \"encoder\"\n",
    "    )\n",
    "    big_encoder._name = \"big_encoder\"\n",
    "    for layer in big_encoder.layers:\n",
    "        layer._name = \"big_\" + layer._name\n",
    "    big_encoder.trainable = False\n",
    "\n",
    "    input_big = keras.Input(BIG_PATCH_SHAPE, name=\"input_big\")\n",
    "    x_big = keras.layers.MaxPooling3D((2, 2, 2), name=\"big_maxpool_0\")(input_big)\n",
    "    x_big = big_encoder(x_big)\n",
    "    x_big = keras.layers.Flatten(name=\"flatten_big\")(x_big)\n",
    "\n",
    "    x = keras.layers.concatenate([x_small, x_big], name=\"concatenate\")\n",
    "\n",
    "    x = SeluDense(128, name=\"selu_dense\")(x)\n",
    "    x = keras.layers.AlphaDropout(dropout_rate, name=\"alpha_dropout\")(x)\n",
    "    x = keras.layers.Dense(1, activation=\"sigmoid\", name=\"final_dense\")(x)\n",
    "\n",
    "    cnn_3d = keras.Model(inputs=[input_small, input_big], outputs=x, name=\"3dcnn\")\n",
    "\n",
    "    return cnn_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, None, None, None, None), (None, None, None, None, None)), (None, 1)), types: ((tf.float32, tf.float32), tf.int8)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset = train_test_split(\n",
    "    labeled_dataset,\n",
    "    test_perc=val_perc,\n",
    "    cardinality=labeled_samples,\n",
    ")\n",
    "val_dataset = (\n",
    "    val_dataset.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.2415 - auc: 0.8556 - accuracy: 0.7699 - val_loss: 0.1146 - val_auc: 0.9171 - val_accuracy: 0.8400\n",
      "Epoch 2/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1982 - auc: 0.7654 - accuracy: 0.6830 - val_loss: 0.1072 - val_auc: 0.9482 - val_accuracy: 0.8400\n",
      "Epoch 3/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1681 - auc: 0.8316 - accuracy: 0.7533 - val_loss: 0.1191 - val_auc: 0.9486 - val_accuracy: 0.8133\n",
      "Epoch 4/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1668 - auc: 0.8332 - accuracy: 0.7639 - val_loss: 0.1054 - val_auc: 0.9429 - val_accuracy: 0.8533\n",
      "Epoch 5/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1536 - auc: 0.8596 - accuracy: 0.7759 - val_loss: 0.1453 - val_auc: 0.9486 - val_accuracy: 0.8133\n",
      "Epoch 6/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1607 - auc: 0.8438 - accuracy: 0.7599 - val_loss: 0.0849 - val_auc: 0.9568 - val_accuracy: 0.8933\n",
      "Epoch 7/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1443 - auc: 0.8768 - accuracy: 0.7891 - val_loss: 0.0968 - val_auc: 0.9482 - val_accuracy: 0.8533\n",
      "Epoch 8/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1469 - auc: 0.8701 - accuracy: 0.7851 - val_loss: 0.0890 - val_auc: 0.9632 - val_accuracy: 0.9067\n",
      "Epoch 9/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1342 - auc: 0.8933 - accuracy: 0.8196 - val_loss: 0.0798 - val_auc: 0.9714 - val_accuracy: 0.8800\n",
      "Epoch 10/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1346 - auc: 0.8923 - accuracy: 0.8130 - val_loss: 0.0797 - val_auc: 0.9700 - val_accuracy: 0.8933\n",
      "Epoch 11/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1323 - auc: 0.8945 - accuracy: 0.8236 - val_loss: 0.0881 - val_auc: 0.9679 - val_accuracy: 0.8667\n",
      "Epoch 12/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1217 - auc: 0.9125 - accuracy: 0.8263 - val_loss: 0.0616 - val_auc: 0.9775 - val_accuracy: 0.9333\n",
      "Epoch 13/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1259 - auc: 0.9047 - accuracy: 0.8210 - val_loss: 0.0747 - val_auc: 0.9729 - val_accuracy: 0.8800\n",
      "Epoch 14/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1207 - auc: 0.9124 - accuracy: 0.8249 - val_loss: 0.0702 - val_auc: 0.9761 - val_accuracy: 0.8933\n",
      "Epoch 15/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1201 - auc: 0.9144 - accuracy: 0.8223 - val_loss: 0.0882 - val_auc: 0.9764 - val_accuracy: 0.8800\n",
      "Epoch 16/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1243 - auc: 0.9069 - accuracy: 0.8183 - val_loss: 0.0658 - val_auc: 0.9782 - val_accuracy: 0.9067\n",
      "Epoch 17/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1099 - auc: 0.9286 - accuracy: 0.8541 - val_loss: 0.0610 - val_auc: 0.9800 - val_accuracy: 0.9200\n",
      "Epoch 18/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1102 - auc: 0.9275 - accuracy: 0.8355 - val_loss: 0.0553 - val_auc: 0.9854 - val_accuracy: 0.9200\n",
      "Epoch 19/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1039 - auc: 0.9354 - accuracy: 0.8594 - val_loss: 0.0459 - val_auc: 0.9886 - val_accuracy: 0.9200\n",
      "Epoch 20/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1056 - auc: 0.9339 - accuracy: 0.8475 - val_loss: 0.0517 - val_auc: 0.9850 - val_accuracy: 0.9200\n",
      "Epoch 21/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1081 - auc: 0.9297 - accuracy: 0.8607 - val_loss: 0.0483 - val_auc: 0.9864 - val_accuracy: 0.9333\n",
      "Epoch 22/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0929 - auc: 0.9518 - accuracy: 0.8753 - val_loss: 0.0439 - val_auc: 0.9900 - val_accuracy: 0.9467\n",
      "Epoch 23/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0985 - auc: 0.9429 - accuracy: 0.8647 - val_loss: 0.0433 - val_auc: 0.9921 - val_accuracy: 0.9333\n",
      "Epoch 24/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0971 - auc: 0.9439 - accuracy: 0.8660 - val_loss: 0.0413 - val_auc: 0.9900 - val_accuracy: 0.9467\n",
      "Epoch 25/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0963 - auc: 0.9453 - accuracy: 0.8700 - val_loss: 0.0400 - val_auc: 0.9914 - val_accuracy: 0.9600\n",
      "Epoch 26/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0957 - auc: 0.9465 - accuracy: 0.8767 - val_loss: 0.0356 - val_auc: 0.9929 - val_accuracy: 0.9600\n",
      "Epoch 27/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0914 - auc: 0.9519 - accuracy: 0.8833 - val_loss: 0.0349 - val_auc: 0.9921 - val_accuracy: 0.9733\n",
      "Epoch 28/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0904 - auc: 0.9511 - accuracy: 0.8833 - val_loss: 0.0394 - val_auc: 0.9921 - val_accuracy: 0.9333\n",
      "Epoch 29/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0839 - auc: 0.9615 - accuracy: 0.8912 - val_loss: 0.0457 - val_auc: 0.9921 - val_accuracy: 0.9333\n",
      "Epoch 30/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0892 - auc: 0.9534 - accuracy: 0.8806 - val_loss: 0.0677 - val_auc: 0.9914 - val_accuracy: 0.9067\n",
      "Epoch 31/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0855 - auc: 0.9580 - accuracy: 0.8820 - val_loss: 0.0455 - val_auc: 0.9918 - val_accuracy: 0.9200\n",
      "Epoch 32/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0852 - auc: 0.9575 - accuracy: 0.8793 - val_loss: 0.0341 - val_auc: 0.9939 - val_accuracy: 0.9467\n",
      "Epoch 33/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0775 - auc: 0.9661 - accuracy: 0.8939 - val_loss: 0.0418 - val_auc: 0.9950 - val_accuracy: 0.9600\n",
      "Epoch 34/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0782 - auc: 0.9665 - accuracy: 0.9005 - val_loss: 0.0240 - val_auc: 0.9957 - val_accuracy: 0.9733\n",
      "Epoch 35/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0814 - auc: 0.9623 - accuracy: 0.8793 - val_loss: 0.0364 - val_auc: 0.9921 - val_accuracy: 0.9600\n",
      "Epoch 36/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0852 - auc: 0.9568 - accuracy: 0.8859 - val_loss: 0.0293 - val_auc: 0.9957 - val_accuracy: 0.9600\n",
      "Epoch 37/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0766 - auc: 0.9656 - accuracy: 0.9058 - val_loss: 0.0265 - val_auc: 0.9950 - val_accuracy: 0.9600\n",
      "Epoch 38/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0674 - auc: 0.9756 - accuracy: 0.9218 - val_loss: 0.0216 - val_auc: 0.9971 - val_accuracy: 0.9733\n",
      "Epoch 39/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0643 - auc: 0.9778 - accuracy: 0.9244 - val_loss: 0.0289 - val_auc: 0.9968 - val_accuracy: 0.9600\n",
      "Epoch 40/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0663 - auc: 0.9760 - accuracy: 0.9178 - val_loss: 0.0240 - val_auc: 0.9964 - val_accuracy: 0.9733\n",
      "Epoch 41/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0672 - auc: 0.9728 - accuracy: 0.9138 - val_loss: 0.0198 - val_auc: 0.9979 - val_accuracy: 0.9733\n",
      "Epoch 42/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0659 - auc: 0.9777 - accuracy: 0.9151 - val_loss: 0.0270 - val_auc: 0.9957 - val_accuracy: 0.9600\n",
      "Epoch 43/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0640 - auc: 0.9782 - accuracy: 0.9178 - val_loss: 0.0247 - val_auc: 0.9986 - val_accuracy: 0.9733\n",
      "Epoch 44/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0670 - auc: 0.9766 - accuracy: 0.9111 - val_loss: 0.0270 - val_auc: 0.9957 - val_accuracy: 0.9467\n",
      "Epoch 45/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0645 - auc: 0.9769 - accuracy: 0.9218 - val_loss: 0.0201 - val_auc: 0.9986 - val_accuracy: 0.9733\n",
      "Epoch 46/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0602 - auc: 0.9822 - accuracy: 0.9178 - val_loss: 0.0160 - val_auc: 0.9986 - val_accuracy: 0.9733\n",
      "Epoch 47/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0589 - auc: 0.9828 - accuracy: 0.9297 - val_loss: 0.0171 - val_auc: 0.9979 - val_accuracy: 0.9867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0645 - auc: 0.9781 - accuracy: 0.9191 - val_loss: 0.0146 - val_auc: 0.9986 - val_accuracy: 0.9733\n",
      "Epoch 49/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0544 - auc: 0.9858 - accuracy: 0.9403 - val_loss: 0.0216 - val_auc: 0.9986 - val_accuracy: 0.9733\n",
      "Epoch 50/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0603 - auc: 0.9813 - accuracy: 0.9164 - val_loss: 0.0235 - val_auc: 0.9993 - val_accuracy: 0.9733\n",
      "Epoch 51/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0572 - auc: 0.9839 - accuracy: 0.9244 - val_loss: 0.0173 - val_auc: 0.9993 - val_accuracy: 0.9733\n",
      "Epoch 52/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0577 - auc: 0.9827 - accuracy: 0.9284 - val_loss: 0.0119 - val_auc: 0.9993 - val_accuracy: 0.9867\n",
      "Epoch 53/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0476 - auc: 0.9913 - accuracy: 0.9536 - val_loss: 0.0158 - val_auc: 0.9993 - val_accuracy: 0.9733\n",
      "Epoch 54/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0499 - auc: 0.9887 - accuracy: 0.9469 - val_loss: 0.0113 - val_auc: 0.9993 - val_accuracy: 0.9867\n",
      "Epoch 55/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0522 - auc: 0.9866 - accuracy: 0.9350 - val_loss: 0.0138 - val_auc: 1.0000 - val_accuracy: 0.9733\n",
      "Epoch 56/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0514 - auc: 0.9890 - accuracy: 0.9390 - val_loss: 0.0114 - val_auc: 1.0000 - val_accuracy: 0.9733\n",
      "Epoch 57/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0505 - auc: 0.9889 - accuracy: 0.9337 - val_loss: 0.0141 - val_auc: 1.0000 - val_accuracy: 0.9733\n",
      "Epoch 58/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0516 - auc: 0.9877 - accuracy: 0.9310 - val_loss: 0.0100 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 59/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0526 - auc: 0.9864 - accuracy: 0.9324 - val_loss: 0.0108 - val_auc: 0.9993 - val_accuracy: 0.9867\n",
      "Epoch 60/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0434 - auc: 0.9932 - accuracy: 0.9589 - val_loss: 0.0115 - val_auc: 0.9993 - val_accuracy: 0.9867\n",
      "Epoch 61/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0444 - auc: 0.9914 - accuracy: 0.9483 - val_loss: 0.0151 - val_auc: 1.0000 - val_accuracy: 0.9733\n",
      "Epoch 62/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0556 - auc: 0.9863 - accuracy: 0.9363 - val_loss: 0.0144 - val_auc: 1.0000 - val_accuracy: 0.9733\n",
      "Epoch 63/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0490 - auc: 0.9895 - accuracy: 0.9469 - val_loss: 0.0073 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 64/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0440 - auc: 0.9911 - accuracy: 0.9483 - val_loss: 0.0090 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 65/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0442 - auc: 0.9915 - accuracy: 0.9536 - val_loss: 0.0054 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 66/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0485 - auc: 0.9897 - accuracy: 0.9456 - val_loss: 0.0096 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 67/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0423 - auc: 0.9934 - accuracy: 0.9589 - val_loss: 0.0078 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 68/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0418 - auc: 0.9929 - accuracy: 0.9496 - val_loss: 0.0073 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 69/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0430 - auc: 0.9917 - accuracy: 0.9536 - val_loss: 0.0047 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 70/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0442 - auc: 0.9912 - accuracy: 0.9496 - val_loss: 0.0078 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 71/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0388 - auc: 0.9945 - accuracy: 0.9682 - val_loss: 0.0043 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 72/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0387 - auc: 0.9941 - accuracy: 0.9602 - val_loss: 0.0041 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 73/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0363 - auc: 0.9949 - accuracy: 0.9589 - val_loss: 0.0030 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 74/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0374 - auc: 0.9944 - accuracy: 0.9602 - val_loss: 0.0053 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 75/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0398 - auc: 0.9931 - accuracy: 0.9509 - val_loss: 0.0050 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 76/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0331 - auc: 0.9961 - accuracy: 0.9695 - val_loss: 0.0091 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 77/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0373 - auc: 0.9948 - accuracy: 0.9589 - val_loss: 0.0030 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 78/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0392 - auc: 0.9941 - accuracy: 0.9549 - val_loss: 0.0025 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 79/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0362 - auc: 0.9947 - accuracy: 0.9602 - val_loss: 0.0041 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 80/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0364 - auc: 0.9949 - accuracy: 0.9655 - val_loss: 0.0023 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 81/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0383 - auc: 0.9946 - accuracy: 0.9629 - val_loss: 0.0022 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 82/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0342 - auc: 0.9961 - accuracy: 0.9708 - val_loss: 0.0016 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 83/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0356 - auc: 0.9946 - accuracy: 0.9562 - val_loss: 0.0029 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 84/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0332 - auc: 0.9963 - accuracy: 0.9655 - val_loss: 0.0031 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 85/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0347 - auc: 0.9949 - accuracy: 0.9668 - val_loss: 0.0061 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 86/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0378 - auc: 0.9947 - accuracy: 0.9549 - val_loss: 0.0076 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 87/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0323 - auc: 0.9963 - accuracy: 0.9642 - val_loss: 0.0029 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 88/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0338 - auc: 0.9959 - accuracy: 0.9642 - val_loss: 0.0057 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 89/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0288 - auc: 0.9974 - accuracy: 0.9748 - val_loss: 0.0017 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 90/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0281 - auc: 0.9983 - accuracy: 0.9775 - val_loss: 0.0018 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 91/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0312 - auc: 0.9964 - accuracy: 0.9615 - val_loss: 0.0024 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 92/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0348 - auc: 0.9949 - accuracy: 0.9602 - val_loss: 0.0021 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 93/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0320 - auc: 0.9959 - accuracy: 0.9682 - val_loss: 0.0011 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 94/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0353 - auc: 0.9945 - accuracy: 0.9549 - val_loss: 7.7699e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0324 - auc: 0.9960 - accuracy: 0.9682 - val_loss: 0.0044 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 96/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0307 - auc: 0.9967 - accuracy: 0.9708 - val_loss: 0.0013 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 97/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0334 - auc: 0.9952 - accuracy: 0.9695 - val_loss: 0.0012 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 98/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0297 - auc: 0.9967 - accuracy: 0.9682 - val_loss: 5.5878e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 99/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0362 - auc: 0.9926 - accuracy: 0.9576 - val_loss: 6.0079e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 100/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0276 - auc: 0.9969 - accuracy: 0.9735 - val_loss: 8.7950e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 101/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0291 - auc: 0.9970 - accuracy: 0.9695 - val_loss: 7.3335e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 102/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0247 - auc: 0.9985 - accuracy: 0.9788 - val_loss: 0.0012 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 103/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0267 - auc: 0.9976 - accuracy: 0.9801 - val_loss: 4.2417e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 104/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0265 - auc: 0.9973 - accuracy: 0.9761 - val_loss: 0.0036 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 105/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0276 - auc: 0.9973 - accuracy: 0.9735 - val_loss: 6.2490e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 106/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0267 - auc: 0.9968 - accuracy: 0.9775 - val_loss: 8.4726e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 107/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0269 - auc: 0.9973 - accuracy: 0.9708 - val_loss: 9.0963e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 108/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0255 - auc: 0.9977 - accuracy: 0.9708 - val_loss: 2.9899e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 109/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0248 - auc: 0.9978 - accuracy: 0.9748 - val_loss: 4.8609e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 110/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0277 - auc: 0.9967 - accuracy: 0.9642 - val_loss: 0.0080 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 111/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0230 - auc: 0.9982 - accuracy: 0.9775 - val_loss: 0.0026 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 112/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0268 - auc: 0.9975 - accuracy: 0.9642 - val_loss: 3.6482e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 113/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0272 - auc: 0.9974 - accuracy: 0.9775 - val_loss: 4.8584e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 114/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0257 - auc: 0.9975 - accuracy: 0.9761 - val_loss: 2.7446e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 115/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0269 - auc: 0.9971 - accuracy: 0.9682 - val_loss: 1.8015e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 116/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0304 - auc: 0.9947 - accuracy: 0.9629 - val_loss: 0.0025 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 117/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0222 - auc: 0.9984 - accuracy: 0.9748 - val_loss: 0.0043 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 118/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0234 - auc: 0.9980 - accuracy: 0.9735 - val_loss: 0.0013 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 119/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0228 - auc: 0.9980 - accuracy: 0.9788 - val_loss: 0.0015 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 120/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0218 - auc: 0.9986 - accuracy: 0.9788 - val_loss: 0.0010 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 121/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0190 - auc: 0.9993 - accuracy: 0.9867 - val_loss: 1.1390e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 122/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0194 - auc: 0.9984 - accuracy: 0.9828 - val_loss: 1.2043e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 123/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0200 - auc: 0.9988 - accuracy: 0.9748 - val_loss: 1.3453e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 124/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0227 - auc: 0.9982 - accuracy: 0.9775 - val_loss: 0.0040 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 125/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0217 - auc: 0.9981 - accuracy: 0.9841 - val_loss: 5.2554e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 126/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0219 - auc: 0.9973 - accuracy: 0.9828 - val_loss: 0.0027 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 127/1000\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0195 - auc: 0.9991 - accuracy: 0.9828 - val_loss: 5.3783e-05 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 128/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0233 - auc: 0.9974 - accuracy: 0.9775 - val_loss: 1.3696e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 129/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0199 - auc: 0.9987 - accuracy: 0.9814 - val_loss: 5.5432e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 130/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0211 - auc: 0.9984 - accuracy: 0.9775 - val_loss: 6.2622e-05 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 131/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0244 - auc: 0.9973 - accuracy: 0.9708 - val_loss: 0.0032 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 132/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0224 - auc: 0.9981 - accuracy: 0.9814 - val_loss: 0.0011 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 133/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0208 - auc: 0.9986 - accuracy: 0.9828 - val_loss: 0.0039 - val_auc: 1.0000 - val_accuracy: 0.9867\n",
      "Epoch 134/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0215 - auc: 0.9980 - accuracy: 0.9735 - val_loss: 2.2863e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 135/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0198 - auc: 0.9983 - accuracy: 0.9801 - val_loss: 9.5515e-05 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 136/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0168 - auc: 0.9992 - accuracy: 0.9867 - val_loss: 3.7361e-05 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 137/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0198 - auc: 0.9983 - accuracy: 0.9801 - val_loss: 4.4820e-05 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 138/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0272 - auc: 0.9964 - accuracy: 0.9682 - val_loss: 4.6523e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 139/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0208 - auc: 0.9985 - accuracy: 0.9814 - val_loss: 0.0021 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 140/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0160 - auc: 0.9993 - accuracy: 0.9828 - val_loss: 0.0050 - val_auc: 1.0000 - val_accuracy: 0.9867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0196 - auc: 0.9988 - accuracy: 0.9828 - val_loss: 0.0018 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 142/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0201 - auc: 0.9985 - accuracy: 0.9775 - val_loss: 8.2454e-05 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 143/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0174 - auc: 0.9993 - accuracy: 0.9841 - val_loss: 1.2253e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 144/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0169 - auc: 0.9992 - accuracy: 0.9828 - val_loss: 0.0020 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 145/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0171 - auc: 0.9988 - accuracy: 0.9814 - val_loss: 3.9539e-05 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 146/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0182 - auc: 0.9986 - accuracy: 0.9828 - val_loss: 1.8886e-04 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 147/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0187 - auc: 0.9986 - accuracy: 0.9814 - val_loss: 7.1219e-05 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 148/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0175 - auc: 0.9989 - accuracy: 0.9828 - val_loss: 4.4470e-05 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 149/1000\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0146 - auc: 0.9993 - accuracy: 0.9867 - val_loss: 8.3252e-05 - val_auc: 1.0000 - val_accuracy: 1.0000\n",
      "Epoch 150/1000\n",
      "17/48 [=========>....................] - ETA: 0s - loss: 0.0158 - auc: 0.9994 - accuracy: 0.9816"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ba74fe40094b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m num_epochs = get_best_num_epochs(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ct-scan-ml/train.py\u001b[0m in \u001b[0;36mget_best_num_epochs\u001b[0;34m(model, train_dataset, val_dataset, patience, monitor_metric, max_epochs, verbose_training, verbose_early_stopping)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mTrain\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mearly\u001b[0m \u001b[0mstopping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv38/lib64/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv38/lib64/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv38/lib64/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv38/lib64/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv38/lib64/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv38/lib64/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/venv38/lib64/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/venv38/lib64/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv38/lib64/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn = build_pretrained_3d_cnn()\n",
    "cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=metrics,\n",
    ")\n",
    "num_epochs = get_best_num_epochs(\n",
    "    cnn,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    patience,\n",
    "    \"val_loss\",\n",
    "    verbose_training=1,\n",
    "    verbose_early_stopping=1,\n",
    ")\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, None, None, None, None), (None, None, None, None, None)), (None, 1)), types: ((tf.float32, tf.float32), tf.int8)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = (\n",
    "    labeled_dataset.cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for more epochs because the training set will be bigger\n",
    "extra_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = build_pretrained_3d_cnn()\n",
    "cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "model_fname = f\"models/3dcnn-lidc.h5\"\n",
    "log_dir = f\"logs/3dcnn-lidc\"\n",
    "cnn = train_model(\n",
    "    cnn,\n",
    "    train_dataset,\n",
    "    num_epochs + extra_epochs,\n",
    "    model_fname,\n",
    "    log_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.models.load_model(\"models/pretrained-3dcnn-lidc.h\")\n",
    "cnn.trainable = True\n",
    "\n",
    "model_fname = f\"models/finetuned-3dcnn-lidc.h5\"\n",
    "log_dir = f\"logs/finetuned-3dcnn-lidc\"\n",
    "cnn = train_model(\n",
    "    cnn,\n",
    "    train_dataset,\n",
    "    10,\n",
    "    model_fname,\n",
    "    log_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spie_dataset, spie_samples = classification_dataset(\n",
    "    SPIE_SMALL_NEG_TFRECORD,\n",
    "    SPIE_BIG_NEG_TFRECORD,\n",
    "    SPIE_SMALL_POS_TFRECORD,\n",
    "    SPIE_BIG_POS_TFRECORD,\n",
    "    return_size=True,\n",
    ")\n",
    "spie_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 4ms/step - loss: 0.4091 - auc: 0.6239 - accuracy: 0.5616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.4090675413608551,\n",
       " 'auc': 0.6238738894462585,\n",
       " 'accuracy': 0.5616438388824463}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = keras.models.load_model(\"models/3dcnn-lidc.h5\")\n",
    "cnn.evaluate(spie_dataset.batch(1), return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3896 - auc: 0.6351 - accuracy: 0.5753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.3895767331123352,\n",
       " 'auc': 0.6351351737976074,\n",
       " 'accuracy': 0.5753424763679504}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = keras.models.load_model(\"models/pretrained-3dcnn-lidc.h\")\n",
    "cnn.evaluate(spie_dataset.batch(1), return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4307 - auc: 0.6400 - accuracy: 0.5479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.4307129681110382,\n",
       " 'auc': 0.6400150060653687,\n",
       " 'accuracy': 0.5479452013969421}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = keras.models.load_model(\"models/finetuned-3dcnn-lidc.h5\")\n",
    "cnn.evaluate(spie_dataset.batch(1), return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
