{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from model import conv_block, deconv_block\n",
    "from data import example_to_tensor, normalize, add_channel_axis, train_test_split\n",
    "from plot import plot_slice, plot_animated_volume\n",
    "from config import data_root_dir, seed\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (48, 256, 256, 1)\n",
    "# tfrecord_glob = \"LUNA16/*.tfrecord\"\n",
    "tfrecord_glob = \"covid-*/*.tfrecord\"\n",
    "\n",
    "encoder_filters = [32, 64, 128]\n",
    "epochs = 500\n",
    "patience = 10\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.0\n",
    "batch_size = 4\n",
    "val_perc = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(scan):\n",
    "    \"Normalize the values in [0, 1]\"\n",
    "    min_value = tf.reduce_min(scan)\n",
    "    max_value = tf.reduce_max(scan)\n",
    "    return (scan - min_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: (None, None, None, 1), types: tf.float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrecord_fnames = [str(p) for p in Path(data_root_dir).glob(tfrecord_glob)]\n",
    "dataset = (\n",
    "    tf.data.TFRecordDataset(tfrecord_fnames)\n",
    "    .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(add_channel_axis, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "# num_samples = sum(1 for _ in dataset)\n",
    "# num_samples = 1018  # LUNA16\n",
    "num_samples = 500  # covid\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, None, None, 1), (None, None, None, 1)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# duplicate the dataset to perform unsupervised training\n",
    "duplicated_dataset = tf.data.Dataset.zip((dataset, dataset))\n",
    "duplicated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, None, None, None, 1), (None, None, None, None, 1)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset = train_test_split(\n",
    "    duplicated_dataset,\n",
    "    test_perc=val_perc,\n",
    "    cardinality=num_samples,\n",
    "    seed=seed,\n",
    ")\n",
    "val_dataset = (\n",
    "    val_dataset.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.batch(batch_size)\n",
    "    .cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=64, reshuffle_each_iteration=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_autoencoder(filters, dropout_rate, learning_rate):\n",
    "    \"\"\"Build the autoencoder with the specified number of filters.\n",
    "\n",
    "    The decoder is a mirrored image of the encoder plus a dense layer.\n",
    "    Compile the model with the Adam optimizer and MeanSquaredError loss.\n",
    "    \"\"\"\n",
    "    encoder_inputs = keras.layers.Input(input_shape)\n",
    "    x = encoder_inputs\n",
    "    for f in filters:\n",
    "        x = conv_block(x, filters=f, dropout_rate=dropout_rate)\n",
    "    encoder_outputs = x\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
    "\n",
    "    decoder_inputs = keras.layers.Input(encoder.output_shape[1:])\n",
    "    x = decoder_inputs\n",
    "    for f in reversed(filters):\n",
    "        x = deconv_block(x, filters=f, dropout_rate=dropout_rate)\n",
    "    decoder_outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    decoder = keras.Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    autoencoder = keras.Sequential([encoder, decoder], name=\"autoencoder\")\n",
    "\n",
    "    autoencoder.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "    )\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 48, 256, 256, 1)] 0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 48, 256, 256, 32)  896       \n",
      "_________________________________________________________________\n",
      "alpha_dropout_6 (AlphaDropou (None, 48, 256, 256, 32)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 24, 128, 128, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 24, 128, 128, 64)  55360     \n",
      "_________________________________________________________________\n",
      "alpha_dropout_7 (AlphaDropou (None, 24, 128, 128, 64)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 12, 64, 64, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 12, 64, 64, 128)   221312    \n",
      "_________________________________________________________________\n",
      "alpha_dropout_8 (AlphaDropou (None, 12, 64, 64, 128)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 6, 32, 32, 128)    0         \n",
      "=================================================================\n",
      "Total params: 277,568\n",
      "Trainable params: 277,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 6, 32, 32, 128)]  0         \n",
      "_________________________________________________________________\n",
      "up_sampling3d_3 (UpSampling3 (None, 12, 64, 64, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 12, 64, 64, 128)   442496    \n",
      "_________________________________________________________________\n",
      "alpha_dropout_9 (AlphaDropou (None, 12, 64, 64, 128)   0         \n",
      "_________________________________________________________________\n",
      "up_sampling3d_4 (UpSampling3 (None, 24, 128, 128, 128) 0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 24, 128, 128, 64)  221248    \n",
      "_________________________________________________________________\n",
      "alpha_dropout_10 (AlphaDropo (None, 24, 128, 128, 64)  0         \n",
      "_________________________________________________________________\n",
      "up_sampling3d_5 (UpSampling3 (None, 48, 256, 256, 64)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 48, 256, 256, 32)  55328     \n",
      "_________________________________________________________________\n",
      "alpha_dropout_11 (AlphaDropo (None, 48, 256, 256, 32)  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 48, 256, 256, 1)   33        \n",
      "=================================================================\n",
      "Total params: 719,105\n",
      "Trainable params: 719,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Functional)         (None, 6, 32, 32, 128)    277568    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 48, 256, 256, 1)   719105    \n",
      "=================================================================\n",
      "Total params: 996,673\n",
      "Trainable params: 996,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = build_and_compile_autoencoder(\n",
    "    encoder_filters, dropout_rate, learning_rate\n",
    ")\n",
    "autoencoder.get_layer(\"encoder\").summary()\n",
    "autoencoder.get_layer(\"decoder\").summary()\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "      2/Unknown - 1s 630ms/step - loss: 0.0872WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4643s vs `on_train_batch_end` time: 0.7954s). Check your callbacks.\n",
      "    100/Unknown - 127s 1s/step - loss: 0.0202\n",
      "Epoch 00001: val_loss improved from inf to 0.01114, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 150s 2s/step - loss: 0.0202 - val_loss: 0.0111\n",
      "Epoch 2/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0101\n",
      "Epoch 00002: val_loss improved from 0.01114 to 0.01007, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 141s 1s/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 3/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0088\n",
      "Epoch 00003: val_loss improved from 0.01007 to 0.00807, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 4/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0079\n",
      "Epoch 00004: val_loss improved from 0.00807 to 0.00740, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0079 - val_loss: 0.0074\n",
      "Epoch 5/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0073\n",
      "Epoch 00005: val_loss improved from 0.00740 to 0.00681, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 6/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0068\n",
      "Epoch 00006: val_loss improved from 0.00681 to 0.00642, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 7/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0064\n",
      "Epoch 00007: val_loss improved from 0.00642 to 0.00619, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0064 - val_loss: 0.0062\n",
      "Epoch 8/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0062\n",
      "Epoch 00008: val_loss improved from 0.00619 to 0.00583, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 9/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 00009: val_loss improved from 0.00583 to 0.00565, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 10/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 00010: val_loss improved from 0.00565 to 0.00560, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 11/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0055\n",
      "Epoch 00011: val_loss improved from 0.00560 to 0.00530, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 12/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0054\n",
      "Epoch 00012: val_loss improved from 0.00530 to 0.00525, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0054 - val_loss: 0.0052\n",
      "Epoch 13/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0053\n",
      "Epoch 00013: val_loss improved from 0.00525 to 0.00507, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 14/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0052\n",
      "Epoch 00014: val_loss improved from 0.00507 to 0.00496, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0052 - val_loss: 0.0050\n",
      "Epoch 15/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0050\n",
      "Epoch 00015: val_loss improved from 0.00496 to 0.00492, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 16/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0053\n",
      "Epoch 00016: val_loss improved from 0.00492 to 0.00482, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0053 - val_loss: 0.0048\n",
      "Epoch 17/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0050\n",
      "Epoch 00017: val_loss improved from 0.00482 to 0.00479, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 18/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0047\n",
      "Epoch 00018: val_loss did not improve from 0.00479\n",
      "100/100 [==============================] - 139s 1s/step - loss: 0.0047 - val_loss: 0.0048\n",
      "Epoch 19/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0047\n",
      "Epoch 00019: val_loss improved from 0.00479 to 0.00457, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 20/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0046\n",
      "Epoch 00020: val_loss did not improve from 0.00457\n",
      "100/100 [==============================] - 139s 1s/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 21/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0046\n",
      "Epoch 00021: val_loss improved from 0.00457 to 0.00444, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 22/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0046\n",
      "Epoch 00022: val_loss improved from 0.00444 to 0.00439, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 23/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0045\n",
      "Epoch 00023: val_loss improved from 0.00439 to 0.00432, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 24/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0044\n",
      "Epoch 00024: val_loss did not improve from 0.00432\n",
      "100/100 [==============================] - 139s 1s/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 25/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0044\n",
      "Epoch 00025: val_loss improved from 0.00432 to 0.00424, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 26/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0043\n",
      "Epoch 00026: val_loss improved from 0.00424 to 0.00421, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 27/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0043\n",
      "Epoch 00027: val_loss did not improve from 0.00421\n",
      "100/100 [==============================] - 139s 1s/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 28/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0043\n",
      "Epoch 00028: val_loss improved from 0.00421 to 0.00412, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 29/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0043\n",
      "Epoch 00029: val_loss did not improve from 0.00412\n",
      "100/100 [==============================] - 139s 1s/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 30/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00030: val_loss improved from 0.00412 to 0.00409, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0042 - val_loss: 0.0041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0041\n",
      "Epoch 00031: val_loss did not improve from 0.00409\n",
      "100/100 [==============================] - 139s 1s/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 32/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0041\n",
      "Epoch 00032: val_loss improved from 0.00409 to 0.00398, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 33/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0041\n",
      "Epoch 00033: val_loss did not improve from 0.00398\n",
      "100/100 [==============================] - 139s 1s/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 34/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0041\n",
      "Epoch 00034: val_loss did not improve from 0.00398\n",
      "100/100 [==============================] - 139s 1s/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 35/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 00035: val_loss improved from 0.00398 to 0.00386, saving model to models/autoencoder-20201103-111931.h5\n",
      "100/100 [==============================] - 140s 1s/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 36/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 00036: val_loss did not improve from 0.00386\n",
      "100/100 [==============================] - 139s 1s/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 37/500\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 00037: val_loss did not improve from 0.00386\n",
      "100/100 [==============================] - 139s 1s/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 38/500\n",
      " 35/100 [=========>....................] - ETA: 1:21 - loss: 0.0039"
     ]
    }
   ],
   "source": [
    "autoencoder = build_and_compile_autoencoder(\n",
    "    encoder_filters, dropout_rate, learning_rate\n",
    ")\n",
    "monitor_metric = \"val_loss\"\n",
    "\n",
    "start_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "best_checkpoint = f\"models/autoencoder-{start_time}.h5\"\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    best_checkpoint, monitor=monitor_metric, verbose=1, save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric,\n",
    "    patience=patience,\n",
    ")\n",
    "log_dir = f\"logs/autoencoder-{start_time}\"\n",
    "file_writer = tf.summary.create_file_writer(log_dir)\n",
    "with file_writer.as_default():\n",
    "    tf.summary.text(\n",
    "        \"Hyperparameters\",\n",
    "        f\"{input_shape=}; \"\n",
    "        f\"{encoder_filters=}; \"\n",
    "        f\"{epochs=}; \"\n",
    "        f\"{patience=}; \"\n",
    "        f\"{batch_size=}; \"\n",
    "        f\"{dropout_rate=}; \"\n",
    "        f\"{learning_rate=}; \"\n",
    "        f\"{val_perc=}\",\n",
    "        step=0,\n",
    "    )\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=False,\n",
    "    profile_batch=0,\n",
    ")\n",
    "autoencoder.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = keras.models.load_model(\"models/autoencoder-20201029-125142.h5\")\n",
    "original, _ = next(iter(train_dataset.skip(1)))\n",
    "encoder_out = autoencoder.get_layer(\"encoder\")(original, training=False)\n",
    "decoder_out = autoencoder.get_layer(\"decoder\")(encoder_out, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index = 3\n",
    "z_index = 20\n",
    "fig, ax = plt.subplots(ncols=3)\n",
    "plot_slice(original[batch_index, :], z_index, ax[0])\n",
    "plot_slice(encoder_out[batch_index, :], encoder_out.shape[1] // 3, ax[1])\n",
    "plot_slice(decoder_out[batch_index, :], z_index, ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animated_volume(original[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
