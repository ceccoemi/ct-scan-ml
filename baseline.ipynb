{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import example_to_tensor, train_test_split\n",
    "from data_augmentation import random_rotate, random_flip\n",
    "from plot import plot_slice, plot_volume_animation\n",
    "from config import (\n",
    "    SMALL_NEG_TFRECORD,\n",
    "    SMALL_POS_TFRECORD,\n",
    "    BIG_NEG_TFRECORD,\n",
    "    BIG_POS_TFRECORD,\n",
    "    SMALL_PATCH_SHAPE,\n",
    "    BIG_PATCH_SHAPE,\n",
    "    SEED,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(volume):\n",
    "    \"Normalize the input volume with values in [0, 1]\"\n",
    "    min_value = tf.reduce_min(volume)\n",
    "    max_value = tf.reduce_max(volume)\n",
    "    return (volume - min_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative samples: 370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, None, None, None), (None, None, None, None)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_x = tf.data.Dataset.zip(\n",
    "    (\n",
    "        tf.data.TFRecordDataset(SMALL_NEG_TFRECORD)\n",
    "        .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE),\n",
    "        tf.data.TFRecordDataset(BIG_NEG_TFRECORD)\n",
    "        .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE),\n",
    "    )\n",
    ")\n",
    "num_neg_samples = sum(1 for _ in neg_x)\n",
    "print(f\"Number of negative samples: {num_neg_samples}\")\n",
    "neg_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: (((None, None, None, None), (None, None, None, None)), (1,)), types: ((tf.float32, tf.float32), tf.int8)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_dataset = tf.data.Dataset.zip(\n",
    "    (neg_x, tf.data.Dataset.from_tensor_slices(np.int8([[0]])).repeat(num_neg_samples))\n",
    ")\n",
    "assert sum(1 for _ in neg_dataset) == num_neg_samples\n",
    "neg_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples: 379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, None, None, None), (None, None, None, None)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_x = tf.data.Dataset.zip(\n",
    "    (\n",
    "        tf.data.TFRecordDataset(SMALL_POS_TFRECORD)\n",
    "        .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE),\n",
    "        tf.data.TFRecordDataset(BIG_POS_TFRECORD)\n",
    "        .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE),\n",
    "    )\n",
    ")\n",
    "num_pos_samples = sum(1 for _ in pos_x)\n",
    "print(f\"Number of positive samples: {num_pos_samples}\")\n",
    "pos_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: (((None, None, None, None), (None, None, None, None)), (1,)), types: ((tf.float32, tf.float32), tf.int8)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dataset = tf.data.Dataset.zip(\n",
    "    (pos_x, tf.data.Dataset.from_tensor_slices(np.int8([[1]])).repeat(num_pos_samples))\n",
    ")\n",
    "assert sum(1 for _ in pos_dataset) == num_pos_samples\n",
    "pos_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: (((None, None, None, None), (None, None, None, None)), (1,)), types: ((tf.float32, tf.float32), tf.int8)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_samples = num_neg_samples + num_pos_samples\n",
    "dataset = neg_dataset.concatenate(pos_dataset).shuffle(\n",
    "    buffer_size=total_samples, seed=SEED, reshuffle_each_iteration=False\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ksplit_dataset(k, dataset, cardinality=None, seed=SEED):\n",
    "    \"Split a dataset into k datasets and drop the remaining elements\"\n",
    "    if not cardinality:\n",
    "        cardinality = sum(1 for _ in dataset)\n",
    "    assert 2 <= k <= cardinality\n",
    "    dataset = dataset.shuffle(\n",
    "        buffer_size=cardinality, reshuffle_each_iteration=False, seed=seed\n",
    "    )\n",
    "    split_size = cardinality // k\n",
    "    splits = []\n",
    "    for _ in range(k):\n",
    "        splits.append(dataset.take(split_size))\n",
    "        dataset = dataset.skip(split_size)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfolds(k, dataset, cardinality=None, seed=SEED):\n",
    "    \"Generator of training / test set with k fold\"\n",
    "    if not cardinality:\n",
    "        cardinality = sum(1 for _ in dataset)\n",
    "    folds = ksplit_dataset(k, dataset, cardinality, seed)\n",
    "    for i, test_dataset in enumerate(folds):\n",
    "        train_folds = [f for j, f in enumerate(folds) if j != i]\n",
    "        train_dataset = train_folds[0]\n",
    "        for d in train_folds[1:]:\n",
    "            train_dataset = train_dataset.concatenate(d)\n",
    "        yield train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, train_dataset, val_dataset, patience, monitor_metric, model_fname, log_dir\n",
    "):\n",
    "    \"Train the model and return the best model found with early stopping\"\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=1000,\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                model_fname, monitor=monitor_metric, verbose=0, save_best_only=True\n",
    "            ),\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=monitor_metric,\n",
    "                patience=patience,\n",
    "            ),\n",
    "            keras.callbacks.TensorBoard(\n",
    "                log_dir=log_dir,\n",
    "                histogram_freq=1,\n",
    "                write_graph=False,\n",
    "                profile_batch=0,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    model = keras.models.load_model(model_fname)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "val_perc = 0.1\n",
    "k = 10\n",
    "patience = 30\n",
    "batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "dropout_rate = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeluConv3D = partial(\n",
    "    keras.layers.Conv3D,\n",
    "    padding=\"same\",\n",
    "    activation=\"selu\",\n",
    "    kernel_initializer=\"lecun_normal\",\n",
    "    bias_initializer=\"zeros\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeluDense = partial(\n",
    "    keras.layers.Dense,\n",
    "    activation=\"selu\",\n",
    "    kernel_initializer=\"lecun_normal\",\n",
    "    bias_initializer=\"zeros\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model():\n",
    "    input_small = keras.Input(SMALL_PATCH_SHAPE, name=\"input_small\")\n",
    "    x_small = SeluConv3D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        name=\"selu_conv3d_small_1\",\n",
    "    )(input_small)\n",
    "    x_small = keras.layers.MaxPooling3D((1, 2, 2), name=\"maxpool_small_1\")(x_small)\n",
    "    x_small = SeluConv3D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        name=\"selu_conv3d_small_2\",\n",
    "    )(x_small)\n",
    "    x_small = keras.layers.MaxPooling3D((1, 2, 2), name=\"maxpool_small_2\")(x_small)\n",
    "    x_small = SeluConv3D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        name=\"selu_conv3d_small_3\",\n",
    "    )(x_small)\n",
    "    x_small = keras.layers.MaxPooling3D((1, 2, 2), name=\"maxpool_small_3\")(x_small)\n",
    "    x_small = SeluConv3D(\n",
    "        filters=256,\n",
    "        kernel_size=3,\n",
    "        name=\"selu_conv3d_small_4\",\n",
    "    )(x_small)\n",
    "    x_small = keras.layers.Flatten(name=\"flatten_small\")(x_small)\n",
    "\n",
    "    input_big = keras.Input(BIG_PATCH_SHAPE, name=\"input_big\")\n",
    "    x_big = keras.layers.MaxPooling3D((2, 2, 2), name=\"maxpool_big_0\")(input_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        name=\"selu_conv3d_big_1\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.MaxPooling3D((1, 2, 2), name=\"maxpool_big_1\")(x_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        name=\"selu_conv3d_big_2\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.MaxPooling3D((1, 2, 2), name=\"maxpool_big_2\")(x_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        name=\"selu_conv3d_big_3\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.MaxPooling3D((1, 2, 2), name=\"maxpool_big_3\")(x_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=256,\n",
    "        kernel_size=3,\n",
    "        name=\"selu_conv3d_big_4\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.Flatten(name=\"flatten_big\")(x_big)\n",
    "\n",
    "    x = keras.layers.concatenate([x_small, x_big], name=\"concatenate\")\n",
    "    x = SeluDense(128, name=\"selu_dense\")(x)\n",
    "    x = keras.layers.AlphaDropout(dropout_rate, name=\"alpha_dropout\")(x)\n",
    "    x = keras.layers.Dense(1, activation=\"sigmoid\", name=\"final_dense\")(x)\n",
    "\n",
    "    cnn = keras.Model(inputs=[input_small, input_big], outputs=x, name=\"3dcnn\")\n",
    "\n",
    "    cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.TruePositives(name=\"tp\"),\n",
    "            keras.metrics.FalsePositives(name=\"fp\"),\n",
    "            keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "            keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "            keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0176s vs `on_train_batch_end` time: 0.0274s). Check your callbacks.\n",
      "{'accuracy': 0.7297297120094299,\n",
      " 'fn': 12.0,\n",
      " 'fp': 8.0,\n",
      " 'loss': 0.7366518378257751,\n",
      " 'tn': 25.0,\n",
      " 'tp': 29.0}\n",
      "{'accuracy': 0.7702702879905701,\n",
      " 'fn': 5.0,\n",
      " 'fp': 12.0,\n",
      " 'loss': 0.5845815539360046,\n",
      " 'tn': 27.0,\n",
      " 'tp': 30.0}\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0178s vs `on_train_batch_end` time: 0.0273s). Check your callbacks.\n",
      "{'accuracy': 0.7837837934494019,\n",
      " 'fn': 5.0,\n",
      " 'fp': 11.0,\n",
      " 'loss': 0.7872048616409302,\n",
      " 'tn': 27.0,\n",
      " 'tp': 31.0}\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0177s vs `on_train_batch_end` time: 0.0272s). Check your callbacks.\n",
      "{'accuracy': 0.7432432174682617,\n",
      " 'fn': 9.0,\n",
      " 'fp': 10.0,\n",
      " 'loss': 0.5746363401412964,\n",
      " 'tn': 27.0,\n",
      " 'tp': 28.0}\n",
      "{'accuracy': 0.7432432174682617,\n",
      " 'fn': 9.0,\n",
      " 'fp': 10.0,\n",
      " 'loss': 1.1722667217254639,\n",
      " 'tn': 26.0,\n",
      " 'tp': 29.0}\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0181s vs `on_train_batch_end` time: 0.0273s). Check your callbacks.\n",
      "{'accuracy': 0.7297297120094299,\n",
      " 'fn': 8.0,\n",
      " 'fp': 12.0,\n",
      " 'loss': 0.8728437423706055,\n",
      " 'tn': 24.0,\n",
      " 'tp': 30.0}\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0181s vs `on_train_batch_end` time: 0.0273s). Check your callbacks.\n",
      "{'accuracy': 0.7567567825317383,\n",
      " 'fn': 4.0,\n",
      " 'fp': 14.0,\n",
      " 'loss': 1.4429597854614258,\n",
      " 'tn': 28.0,\n",
      " 'tp': 28.0}\n",
      "{'accuracy': 0.7837837934494019,\n",
      " 'fn': 9.0,\n",
      " 'fp': 7.0,\n",
      " 'loss': 0.5396794676780701,\n",
      " 'tn': 24.0,\n",
      " 'tp': 34.0}\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0179s vs `on_train_batch_end` time: 0.0274s). Check your callbacks.\n",
      "{'accuracy': 0.6351351141929626,\n",
      " 'fn': 12.0,\n",
      " 'fp': 15.0,\n",
      " 'loss': 1.4254175424575806,\n",
      " 'tn': 21.0,\n",
      " 'tp': 26.0}\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0182s vs `on_train_batch_end` time: 0.0274s). Check your callbacks.\n",
      "{'accuracy': 0.6756756901741028,\n",
      " 'fn': 10.0,\n",
      " 'fp': 14.0,\n",
      " 'loss': 1.8532426357269287,\n",
      " 'tn': 23.0,\n",
      " 'tp': 27.0}\n",
      "{'accuracy': 0.7351351320743561, 'fn': 8.3, 'fp': 11.3, 'tn': 25.2, 'tp': 29.2}\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "from pprint import pprint\n",
    "\n",
    "metrics = {\"tp\": [], \"fp\": [], \"tn\": [], \"fn\": [], \"accuracy\": []}\n",
    "for train_val_dataset, test_dataset in kfolds(k, dataset, cardinality=total_samples):\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "    train_dataset, val_dataset = train_test_split(train_val_dataset, test_perc=val_perc)\n",
    "    val_dataset = val_dataset.batch(1)\n",
    "    train_dataset = (\n",
    "        train_dataset.batch(batch_size)\n",
    "        .cache()  # must be called before shuffle\n",
    "        .shuffle(buffer_size=128, reshuffle_each_iteration=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "    cnn = build_and_compile_model()\n",
    "    start_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model_fname = f\"models/baseline-{start_time}.h5\"\n",
    "    log_dir = f\"logs/baseline-{start_time}\"\n",
    "    cnn = train_model(\n",
    "        cnn, train_dataset, val_dataset, patience, \"val_accuracy\", model_fname, log_dir\n",
    "    )\n",
    "    test_metrics = cnn.evaluate(test_dataset, return_dict=True, verbose=0)\n",
    "    pprint(test_metrics)\n",
    "    for metric_name, metric_value in test_metrics.items():\n",
    "        if metric_name in metrics:\n",
    "            metrics[metric_name].append(metric_value)\n",
    "mean_metrics = {\n",
    "    metric_name: mean(metric_values) for metric_name, metric_values in metrics.items()\n",
    "    if metric_name in metrics\n",
    "}\n",
    "pprint(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches, label = next(iter(test_dataset.skip(6)))\n",
    "print(f\"label: {label[0][0].numpy()}\")\n",
    "prediction = cnn(patches, training=False)\n",
    "print(f\"prediction: {prediction[0][0].numpy()}\")\n",
    "plot_volume_animation(patches[0][0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
