{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from data import kfolds, train_test_split, classification_dataset\n",
    "from train import train_model\n",
    "from layers import SeluConv3D, SeluDense\n",
    "from plot import plot_slice, plot_volume_animation\n",
    "from config import (\n",
    "    LIDC_SMALL_NEG_TFRECORD,\n",
    "    LIDC_SMALL_POS_TFRECORD,\n",
    "    LIDC_BIG_NEG_TFRECORD,\n",
    "    LIDC_BIG_POS_TFRECORD,\n",
    "    SPIE_SMALL_NEG_TFRECORD,\n",
    "    SPIE_SMALL_POS_TFRECORD,\n",
    "    SPIE_BIG_NEG_TFRECORD,\n",
    "    SPIE_BIG_POS_TFRECORD,\n",
    "    SMALL_PATCH_SHAPE,\n",
    "    BIG_PATCH_SHAPE,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "val_perc = 0.2\n",
    "k = 10\n",
    "patience = 15\n",
    "batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "dropout_rate = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidc_dataset, lidc_total_samples = classification_dataset(\n",
    "    LIDC_SMALL_NEG_TFRECORD,\n",
    "    LIDC_BIG_NEG_TFRECORD,\n",
    "    LIDC_SMALL_POS_TFRECORD,\n",
    "    LIDC_BIG_POS_TFRECORD,\n",
    "    return_size=True,\n",
    ")\n",
    "lidc_total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_small = keras.Input(SMALL_PATCH_SHAPE, name=\"input_small\")\n",
    "    x_small = SeluConv3D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        name=\"small_selu_conv3d_1\",\n",
    "    )(input_small)\n",
    "    x_small = keras.layers.MaxPooling3D((1, 2, 2), name=\"small_maxpool_1\")(x_small)\n",
    "    x_small = SeluConv3D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        name=\"small_selu_conv3d_2\",\n",
    "    )(x_small)\n",
    "    x_small = keras.layers.MaxPooling3D((1, 2, 2), name=\"small_maxpool_2\")(x_small)\n",
    "    x_small = SeluConv3D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        name=\"small_selu_conv3d_3\",\n",
    "    )(x_small)\n",
    "    x_small = keras.layers.MaxPooling3D((1, 2, 2), name=\"small_maxpool_3\")(x_small)\n",
    "    x_small = SeluConv3D(\n",
    "        filters=256,\n",
    "        kernel_size=3,\n",
    "        name=\"small_selu_conv3d_4\",\n",
    "    )(x_small)\n",
    "    x_small = keras.layers.Flatten(name=\"flatten_small\")(x_small)\n",
    "\n",
    "    input_big = keras.Input(BIG_PATCH_SHAPE, name=\"input_big\")\n",
    "    x_big = keras.layers.MaxPooling3D((2, 2, 2), name=\"big_maxpool_0\")(input_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        name=\"big_selu_conv3d_1\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.MaxPooling3D((1, 2, 2), name=\"big_maxpool_1\")(x_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        name=\"big_selu_conv3d_2\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.MaxPooling3D((1, 2, 2), name=\"big_maxpool_2\")(x_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        name=\"big_selu_conv3d_3\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.MaxPooling3D((1, 2, 2), name=\"big_maxpool_3\")(x_big)\n",
    "    x_big = SeluConv3D(\n",
    "        filters=256,\n",
    "        kernel_size=3,\n",
    "        name=\"big_selu_conv3d_4\",\n",
    "    )(x_big)\n",
    "    x_big = keras.layers.Flatten(name=\"flatten_big\")(x_big)\n",
    "\n",
    "    x = keras.layers.concatenate([x_small, x_big], name=\"concatenate\")\n",
    "    x = SeluDense(128, name=\"selu_dense\")(x)\n",
    "    x = keras.layers.AlphaDropout(dropout_rate, name=\"alpha_dropout\")(x)\n",
    "    x = keras.layers.Dense(1, activation=\"sigmoid\", name=\"final_dense\")(x)\n",
    "\n",
    "    cnn_3d = keras.Model(inputs=[input_small, input_big], outputs=x, name=\"3dcnn\")\n",
    "\n",
    "    return cnn_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    keras.metrics.AUC(name=\"auc\"),\n",
    "    keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.76000, saving model to models/baseline-lidc.h5\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.76000\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.76000\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.76000 to 0.78000, saving model to models/baseline-lidc.h5\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.78000\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.78000 to 0.78667, saving model to models/baseline-lidc.h5\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.78667\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.78667\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.78667 to 0.80667, saving model to models/baseline-lidc.h5\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.80667\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.80667\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.80667\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.80667 to 0.81333, saving model to models/baseline-lidc.h5\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.81333 to 0.82000, saving model to models/baseline-lidc.h5\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.82000\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = train_test_split(lidc_dataset, test_perc=val_perc)\n",
    "val_dataset = val_dataset.batch(1)\n",
    "train_dataset = (\n",
    "    train_dataset.batch(batch_size)\n",
    "    .cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=64, reshuffle_each_iteration=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "cnn_3d = build_model()\n",
    "cnn_3d.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "log_dir = f\"logs/baseline-lidc\"\n",
    "model_fname = f\"models/baseline-lidc.h5\"\n",
    "cnn_3d = train_model(\n",
    "        cnn_3d,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        patience,\n",
    "        \"val_accuracy\",\n",
    "        model_fname,\n",
    "        log_dir,\n",
    "        verbose_checkpoint=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spie_dataset, spie_total_samples = classification_dataset(\n",
    "    SPIE_SMALL_NEG_TFRECORD,\n",
    "    SPIE_BIG_NEG_TFRECORD,\n",
    "    SPIE_SMALL_POS_TFRECORD,\n",
    "    SPIE_BIG_POS_TFRECORD,\n",
    "    return_size=True,\n",
    ")\n",
    "spie_total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 37, 1: 36})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(label.numpy()[0] for _, label in spie_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 4ms/step - loss: 2.6478 - auc: 0.6329 - accuracy: 0.5890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 2.647796154022217,\n",
       " 'auc': 0.632882833480835,\n",
       " 'accuracy': 0.5890411138534546}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_3d.evaluate(spie_dataset.batch(1), return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metrics = {\n",
    "    metric.name: keras.metrics.Mean(f\"{metric.name}_mean\", dtype=tf.float32)\n",
    "    for metric in metrics\n",
    "}\n",
    "fold_id = 0\n",
    "for train_val_dataset, test_dataset in tqdm(\n",
    "    kfolds(k, dataset, cardinality=total_samples), total=k\n",
    "):\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "    train_dataset, val_dataset = train_test_split(train_val_dataset, test_perc=val_perc)\n",
    "    val_dataset = val_dataset.batch(1)\n",
    "    train_dataset = (\n",
    "        train_dataset.batch(batch_size)\n",
    "        .cache()  # must be called before shuffle\n",
    "        .shuffle(buffer_size=64, reshuffle_each_iteration=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    cnn_3d = build_model()\n",
    "    cnn_3d.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=metrics,\n",
    "    )\n",
    "\n",
    "    log_dir = f\"logs/baseline-lidc-{fold_id}\"\n",
    "    model_fname = f\"models/baseline-lidc-{fold_id}.h5\"\n",
    "    cnn_3d = train_model(\n",
    "        cnn_3d,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        patience,\n",
    "        \"val_accuracy\",\n",
    "        model_fname,\n",
    "        log_dir,\n",
    "    )\n",
    "\n",
    "    test_metrics = cnn_3d.evaluate(test_dataset, return_dict=True, verbose=0)\n",
    "\n",
    "    print(f\" {fold_id=} \".center(40, \"=\"))\n",
    "    for metric_name, metric_value in test_metrics.items():\n",
    "        if metric_name in mean_metrics:\n",
    "            print(f\"{metric_name}: {metric_value}\")\n",
    "            mean_metrics[metric_name].update_state(metric_value)\n",
    "\n",
    "    fold_id += 1\n",
    "\n",
    "print(\" mean \".center(40, \"=\"))\n",
    "for metric_name, metric_value in mean_metrics.items():\n",
    "    print(f\"{metric_name}: {metric_value.result().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_perc=val_perc)\n",
    "val_dataset = val_dataset.batch(1)\n",
    "train_dataset = (\n",
    "    train_dataset.batch(batch_size)\n",
    "    .cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=64, reshuffle_each_iteration=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "cnn_3d = build_model()\n",
    "cnn_3d.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "log_dir = f\"logs/baseline-lidc-{fold_id}\"\n",
    "model_fname = f\"models/baseline-lidc-{fold_id}.h5\"\n",
    "cnn_3d = train_model(\n",
    "    cnn_3d,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    patience,\n",
    "    \"val_loss\",\n",
    "    model_fname,\n",
    "    log_dir,\n",
    "    verbose_training=True\n",
    ")\n",
    "\n",
    "dataset, total_samples = classification_dataset(\n",
    "    SPIE_SMALL_NEG_TFRECORD,\n",
    "    SPIE_BIG_NEG_TFRECORD,\n",
    "    SPIE_SMALL_POS_TFRECORD,\n",
    "    SPIE_BIG_POS_TFRECORD,\n",
    "    return_size=True,\n",
    ")\n",
    "print(total_samples)\n",
    "dataset = dataset.batch(1)\n",
    "cnn_3d.evaluate(dataset, return_dict=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "i = 2\n",
    "\n",
    "for fold_id, (train_val_dataset, test_dataset) in enumerate(\n",
    "    islice(kfolds(k, dataset, cardinality=total_samples), i, i+1), i\n",
    "):\n",
    "    print(Counter(label.numpy()[0] for _, label in test_dataset))\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "    cnn_3d = keras.models.load_model(f\"models/baseline-lidc-{fold_id}.h5\")\n",
    "    print(cnn_3d.evaluate(test_dataset, return_dict=True, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches, label = next(iter(test_dataset.skip(5)))\n",
    "print(f\"label: {label[0][0].numpy()}\")\n",
    "prediction = cnn_3d(patches, training=False)\n",
    "print(f\"prediction: {prediction[0][0].numpy()}\")\n",
    "plot_volume_animation(patches[0][0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
