{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from model import conv_block\n",
    "from data import example_to_tensor, normalize, add_channel_axis, train_test_split\n",
    "from plot import plot_slice, plot_animated_volume\n",
    "from config import data_root_dir, seed\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (48, 256, 256, 1)\n",
    "neg_tfrecord_glob = \"covid-neg/*.tfrecord\"\n",
    "pos_tfrecord_glob = \"covid-pos/*.tfrecord\"\n",
    "\n",
    "epochs = 1000\n",
    "patience = 10\n",
    "batch_size = 8\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.0\n",
    "val_perc = 0.12  # percentage from the already splitted training test\n",
    "test_perc = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative samples: 250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: (None, None, None, 1), types: tf.float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tfrecord_fnames = [str(p) for p in Path(data_root_dir).glob(neg_tfrecord_glob)]\n",
    "neg_x = (\n",
    "    tf.data.TFRecordDataset(neg_tfrecord_fnames)\n",
    "    .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(add_channel_axis, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "# num_neg = sum(1 for _ in neg_x)\n",
    "num_neg = 250\n",
    "# num_neg = 254\n",
    "print(f\"Number of negative samples: {num_neg}\")\n",
    "neg_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples: 250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: (None, None, None, 1), types: tf.float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tfrecord_fnames = [str(p) for p in Path(data_root_dir).glob(pos_tfrecord_glob)]\n",
    "pos_x = (\n",
    "    tf.data.TFRecordDataset(pos_tfrecord_fnames)\n",
    "    .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(add_channel_axis, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "# num_pos = sum(1 for _ in pos_x)\n",
    "num_pos = 250\n",
    "# num_pos = 856\n",
    "print(f\"Number of positive samples: {num_pos}\")\n",
    "pos_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, None, None, 1), (1,)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_y = tf.data.Dataset.from_tensors(tf.constant([0], dtype=tf.int8)).repeat(num_neg)\n",
    "neg_dataset = tf.data.Dataset.zip((neg_x, neg_y))\n",
    "neg_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, None, None, 1), (1,)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_y = tf.data.Dataset.from_tensors(tf.constant([1], dtype=tf.int8)).repeat(num_pos)\n",
    "pos_dataset = tf.data.Dataset.zip((pos_x, pos_y))\n",
    "pos_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, None, None, None, 1), (None, 1)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = neg_dataset.concatenate(pos_dataset)\n",
    "dataset, test_dataset = train_test_split(\n",
    "    dataset,\n",
    "    test_perc=test_perc,\n",
    "    cardinality=(num_pos + num_neg),\n",
    "    seed=seed,\n",
    ")\n",
    "test_dataset = test_dataset.batch(1)\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    dataset,\n",
    "    test_perc=val_perc,\n",
    "    cardinality=None,\n",
    "    seed=seed,\n",
    ")\n",
    "val_dataset = (\n",
    "    val_dataset.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.batch(batch_size)\n",
    "    .cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=64, reshuffle_each_iteration=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels:\n",
      "\t{0: 201, 1: 195}\n",
      "Validation labels:\n",
      "\t{0: 23, 1: 31}\n",
      "Test labels:\n",
      "\t{1: 24, 0: 26}\n"
     ]
    }
   ],
   "source": [
    "def count_labels(dataset):\n",
    "    \"Return a dictionary of the label count.\"\n",
    "    return dict(Counter(label.numpy()[0] for _, label in dataset.unbatch()))\n",
    "\n",
    "\n",
    "print(f\"Train labels:\\n\\t{count_labels(train_dataset)}\")\n",
    "print(f\"Validation labels:\\n\\t{count_labels(val_dataset)}\")\n",
    "print(f\"Test labels:\\n\\t{count_labels(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(learning_rate, dropout_rate):\n",
    "    inputs = keras.layers.Input(input_shape)\n",
    "\n",
    "    x = conv_block(inputs, filters=32, dropout_rate=dropout_rate)\n",
    "    x = conv_block(x, filters=64, dropout_rate=dropout_rate)\n",
    "    x = conv_block(x, filters=128, dropout_rate=dropout_rate)\n",
    "\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(\n",
    "        512,\n",
    "        kernel_initializer=\"lecun_normal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        activation=\"selu\",\n",
    "    )(x)\n",
    "    x = keras.layers.AlphaDropout(dropout_rate)(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(\n",
    "        1,\n",
    "        activation=\"sigmoid\",\n",
    "    )(x)\n",
    "    cnn = keras.Model(inputs, outputs, name=\"baseline-3dcnn\")\n",
    "    cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[\n",
    "            keras.metrics.TruePositives(name=\"tp\"),\n",
    "            keras.metrics.FalsePositives(name=\"fp\"),\n",
    "            keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "            keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "            keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "      2/Unknown - 0s 235ms/step - loss: 147.4123 - tp: 3.0000 - fp: 5.0000 - tn: 4.0000 - fn: 4.0000 - accuracy: 0.4375     WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1804s vs `on_train_batch_end` time: 0.2898s). Check your callbacks.\n",
      "     50/Unknown - 24s 479ms/step - loss: 9.4927 - tp: 10.0000 - fp: 19.0000 - tn: 182.0000 - fn: 185.0000 - accuracy: 0.4848\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.42593, saving model to models/baseline-20201031-132941.h5\n",
      "50/50 [==============================] - 502s 10s/step - loss: 9.4927 - tp: 10.0000 - fp: 19.0000 - tn: 182.0000 - fn: 185.0000 - accuracy: 0.4848 - val_loss: 0.6992 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 23.0000 - val_fn: 31.0000 - val_accuracy: 0.4259\n",
      "Epoch 2/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6950 - tp: 18.0000 - fp: 42.0000 - tn: 159.0000 - fn: 177.0000 - accuracy: 0.4470\n",
      "Epoch 00002: val_accuracy did not improve from 0.42593\n",
      "50/50 [==============================] - 53s 1s/step - loss: 0.6950 - tp: 18.0000 - fp: 42.0000 - tn: 159.0000 - fn: 177.0000 - accuracy: 0.4470 - val_loss: 0.6941 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 23.0000 - val_fn: 31.0000 - val_accuracy: 0.4259\n",
      "Epoch 3/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6958 - tp: 11.0000 - fp: 21.0000 - tn: 180.0000 - fn: 184.0000 - accuracy: 0.4823\n",
      "Epoch 00003: val_accuracy did not improve from 0.42593\n",
      "50/50 [==============================] - 53s 1s/step - loss: 0.6958 - tp: 11.0000 - fp: 21.0000 - tn: 180.0000 - fn: 184.0000 - accuracy: 0.4823 - val_loss: 0.6971 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 23.0000 - val_fn: 31.0000 - val_accuracy: 0.4259\n",
      "Epoch 4/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6953 - tp: 83.0000 - fp: 105.0000 - tn: 96.0000 - fn: 112.0000 - accuracy: 0.4520\n",
      "Epoch 00004: val_accuracy improved from 0.42593 to 0.57407, saving model to models/baseline-20201031-132941.h5\n",
      "50/50 [==============================] - 497s 10s/step - loss: 0.6953 - tp: 83.0000 - fp: 105.0000 - tn: 96.0000 - fn: 112.0000 - accuracy: 0.4520 - val_loss: 0.6889 - val_tp: 31.0000 - val_fp: 23.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5741\n",
      "Epoch 5/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6957 - tp: 81.0000 - fp: 95.0000 - tn: 106.0000 - fn: 114.0000 - accuracy: 0.4722\n",
      "Epoch 00005: val_accuracy did not improve from 0.57407\n",
      "50/50 [==============================] - 53s 1s/step - loss: 0.6957 - tp: 81.0000 - fp: 95.0000 - tn: 106.0000 - fn: 114.0000 - accuracy: 0.4722 - val_loss: 0.6919 - val_tp: 31.0000 - val_fp: 23.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5741\n",
      "Epoch 6/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6951 - tp: 51.0000 - fp: 53.0000 - tn: 148.0000 - fn: 144.0000 - accuracy: 0.5025\n",
      "Epoch 00006: val_accuracy did not improve from 0.57407\n",
      "50/50 [==============================] - 53s 1s/step - loss: 0.6951 - tp: 51.0000 - fp: 53.0000 - tn: 148.0000 - fn: 144.0000 - accuracy: 0.5025 - val_loss: 0.6983 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 23.0000 - val_fn: 31.0000 - val_accuracy: 0.4259\n",
      "Epoch 7/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6960 - tp: 105.0000 - fp: 107.0000 - tn: 94.0000 - fn: 90.0000 - accuracy: 0.5025\n",
      "Epoch 00007: val_accuracy did not improve from 0.57407\n",
      "50/50 [==============================] - 53s 1s/step - loss: 0.6960 - tp: 105.0000 - fp: 107.0000 - tn: 94.0000 - fn: 90.0000 - accuracy: 0.5025 - val_loss: 0.7126 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 23.0000 - val_fn: 31.0000 - val_accuracy: 0.4259\n",
      "Epoch 8/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6973 - tp: 4.0000 - fp: 4.0000 - tn: 197.0000 - fn: 191.0000 - accuracy: 0.5076        \n",
      "Epoch 00008: val_accuracy did not improve from 0.57407\n",
      "50/50 [==============================] - 53s 1s/step - loss: 0.6973 - tp: 4.0000 - fp: 4.0000 - tn: 197.0000 - fn: 191.0000 - accuracy: 0.5076 - val_loss: 0.6929 - val_tp: 31.0000 - val_fp: 23.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5741\n",
      "Epoch 9/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6952 - tp: 20.0000 - fp: 24.0000 - tn: 177.0000 - fn: 175.0000 - accuracy: 0.4975\n",
      "Epoch 00009: val_accuracy did not improve from 0.57407\n",
      "50/50 [==============================] - 52s 1s/step - loss: 0.6952 - tp: 20.0000 - fp: 24.0000 - tn: 177.0000 - fn: 175.0000 - accuracy: 0.4975 - val_loss: 0.6946 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 23.0000 - val_fn: 31.0000 - val_accuracy: 0.4259\n",
      "Epoch 10/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6941 - tp: 156.0000 - fp: 156.0000 - tn: 45.0000 - fn: 39.0000 - accuracy: 0.5076\n",
      "Epoch 00010: val_accuracy did not improve from 0.57407\n",
      "50/50 [==============================] - 52s 1s/step - loss: 0.6941 - tp: 156.0000 - fp: 156.0000 - tn: 45.0000 - fn: 39.0000 - accuracy: 0.5076 - val_loss: 0.7048 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 23.0000 - val_fn: 31.0000 - val_accuracy: 0.4259\n",
      "Epoch 11/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7030 - tp: 70.0000 - fp: 90.0000 - tn: 111.0000 - fn: 125.0000 - accuracy: 0.4571\n",
      "Epoch 00011: val_accuracy did not improve from 0.57407\n",
      "50/50 [==============================] - 53s 1s/step - loss: 0.7030 - tp: 70.0000 - fp: 90.0000 - tn: 111.0000 - fn: 125.0000 - accuracy: 0.4571 - val_loss: 0.6928 - val_tp: 31.0000 - val_fp: 23.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5741\n",
      "Epoch 12/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6965 - tp: 22.0000 - fp: 26.0000 - tn: 175.0000 - fn: 173.0000 - accuracy: 0.4975\n",
      "Epoch 00012: val_accuracy did not improve from 0.57407\n",
      "50/50 [==============================] - 52s 1s/step - loss: 0.6965 - tp: 22.0000 - fp: 26.0000 - tn: 175.0000 - fn: 173.0000 - accuracy: 0.4975 - val_loss: 0.6879 - val_tp: 31.0000 - val_fp: 23.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5741\n",
      "Epoch 13/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6960 - tp: 99.0000 - fp: 117.0000 - tn: 84.0000 - fn: 96.0000 - accuracy: 0.4621\n",
      "Epoch 00013: val_accuracy did not improve from 0.57407\n",
      "50/50 [==============================] - 52s 1s/step - loss: 0.6960 - tp: 99.0000 - fp: 117.0000 - tn: 84.0000 - fn: 96.0000 - accuracy: 0.4621 - val_loss: 0.6970 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 23.0000 - val_fn: 31.0000 - val_accuracy: 0.4259\n",
      "Epoch 14/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6987 - tp: 31.0000 - fp: 61.0000 - tn: 140.0000 - fn: 164.0000 - accuracy: 0.4318\n",
      "Epoch 00014: val_accuracy did not improve from 0.57407\n",
      "50/50 [==============================] - 52s 1s/step - loss: 0.6987 - tp: 31.0000 - fp: 61.0000 - tn: 140.0000 - fn: 164.0000 - accuracy: 0.4318 - val_loss: 0.7039 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 23.0000 - val_fn: 31.0000 - val_accuracy: 0.4259\n"
     ]
    }
   ],
   "source": [
    "cnn = build_and_compile_model(learning_rate, dropout_rate)\n",
    "\n",
    "monitor_metric = \"val_accuracy\"\n",
    "\n",
    "start_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "best_checkpoint = f\"models/baseline-{start_time}.h5\"\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    best_checkpoint,\n",
    "    monitor=monitor_metric,\n",
    "    mode=\"max\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=patience, mode=\"max\"\n",
    ")\n",
    "log_dir = f\"logs/baseline-{start_time}\"\n",
    "file_writer = tf.summary.create_file_writer(log_dir)\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=False,\n",
    "    profile_batch=0,\n",
    ")\n",
    "cnn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb],\n",
    ")\n",
    "with file_writer.as_default():\n",
    "    tf.summary.text(\n",
    "        \"Hyperparameters\",\n",
    "        f\"{seed=}; \"\n",
    "        f\"{input_shape=}; \"\n",
    "        f\"{epochs=}; \"\n",
    "        f\"{patience=}; \"\n",
    "        f\"{batch_size=}; \"\n",
    "        f\"{learning_rate=}; \"\n",
    "        f\"{dropout_rate=}; \"\n",
    "        f\"{val_perc=}; \"\n",
    "        f\"{test_perc=}\",\n",
    "        step=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 2s 33ms/step - loss: 0.7026 - tp: 19.0000 - fp: 16.0000 - tn: 10.0000 - fn: 5.0000 - accuracy: 0.5800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.7026064395904541,\n",
       " 'tp': 19.0,\n",
       " 'fp': 16.0,\n",
       " 'tn': 10.0,\n",
       " 'fn': 5.0,\n",
       " 'accuracy': 0.5799999833106995}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = keras.models.load_model(\"models/baseline-20201029-111058.h5\")\n",
    "cnn.evaluate(test_dataset, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 2s 33ms/step - loss: 1.1716 - tp: 24.0000 - fp: 26.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.4800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.1716235876083374,\n",
       " 'tp': 24.0,\n",
       " 'fp': 26.0,\n",
       " 'tn': 0.0,\n",
       " 'fn': 0.0,\n",
       " 'accuracy': 0.47999998927116394}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = keras.models.load_model(\"models/baseline-20201029-113438.h5\")\n",
    "cnn.evaluate(test_dataset, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: models/baseline-20201019-115235.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-82f1ab5aedbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/baseline-20201019-115235.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv38/lib64/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv38/lib64/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     raise IOError(\"SavedModel file does not exist at: %s/{%s|%s}\" %\n\u001b[0m\u001b[1;32m    111\u001b[0m                   (export_dir,\n\u001b[1;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: models/baseline-20201019-115235.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "cnn = keras.models.load_model(\"models/baseline-20201029-115235.h5\")\n",
    "cnn.evaluate(test_dataset, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_dataset.skip(5)))\n",
    "prediction = cnn(x, training=False)\n",
    "print(f\"real: {y.numpy()}, prediction: {prediction.numpy()}\")\n",
    "plot_animated_volume(x[0, :], fps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_bias(dataset):\n",
    "    \"\"\"Prediction bias is the difference\n",
    "        average_labels - average_predictions\n",
    "\n",
    "    It should be near zero.\n",
    "    Return the tuple (label_avg, prediction_avg, prediction_bias)\n",
    "    \"\"\"\n",
    "    label_avg = np.mean([label.numpy()[0] for _, label in dataset.unbatch()])\n",
    "\n",
    "    def gen():\n",
    "        for x, _ in dataset:\n",
    "            yield x\n",
    "\n",
    "    x_dataset = (\n",
    "        tf.data.Dataset.from_generator(gen, tf.float32)\n",
    "        .unbatch()\n",
    "        .padded_batch(1, input_shape)\n",
    "    )\n",
    "    prediction_avg = np.mean([cnn(x, training=False).numpy()[0][0] for x in x_dataset])\n",
    "    return label_avg, prediction_avg, np.abs(label_avg - prediction_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, p, b = prediction_bias(train_dataset)\n",
    "print(f\"Labels average: {l}\")\n",
    "print(f\"Predictions average: {p}\")\n",
    "print(f\"Prediction bias: {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
