{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from model import conv_block\n",
    "from data import example_to_tensor, normalize, add_channel_axis, train_test_split\n",
    "from plot import plot_slice, plot_animated_volume\n",
    "from config import data_root_dir, seed\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (48, 256, 256, 1)\n",
    "neg_tfrecord_glob = \"CT-0/*.tfrecord\"\n",
    "pos_tfrecord_glob = \"CT-[1-4]/*.tfrecord\"\n",
    "\n",
    "epochs = 1000\n",
    "patience = 30\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.0\n",
    "seed = 5\n",
    "val_perc = 0.12  # percentage from the already splitted training test\n",
    "test_perc = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative samples: 254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: (None, None, None, 1), types: tf.float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tfrecord_fnames = [str(p) for p in Path(data_root_dir).glob(neg_tfrecord_glob)]\n",
    "neg_x = (\n",
    "    tf.data.TFRecordDataset(neg_tfrecord_fnames)\n",
    "    .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(add_channel_axis, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "# num_neg = sum(1 for _ in neg_x)\n",
    "num_neg = 254\n",
    "print(f\"Number of negative samples: {num_neg}\")\n",
    "neg_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples: 856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: (None, None, None, 1), types: tf.float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tfrecord_fnames = [str(p) for p in Path(data_root_dir).glob(pos_tfrecord_glob)]\n",
    "pos_x = (\n",
    "    tf.data.TFRecordDataset(pos_tfrecord_fnames)\n",
    "    .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(add_channel_axis, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "# num_pos = sum(1 for _ in pos_x)\n",
    "num_pos = 856\n",
    "print(f\"Number of positive samples: {num_pos}\")\n",
    "pos_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, None, None, 1), (1,)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_y = tf.data.Dataset.from_tensors(tf.constant([0], dtype=tf.int8)).repeat(num_neg)\n",
    "neg_dataset = tf.data.Dataset.zip((neg_x, neg_y))\n",
    "neg_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, None, None, 1), (1,)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_y = tf.data.Dataset.from_tensors(tf.constant([1], dtype=tf.int8)).repeat(num_pos)\n",
    "pos_dataset = tf.data.Dataset.zip((pos_x, pos_y))\n",
    "pos_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = neg_dataset.concatenate(pos_dataset)\n",
    "dataset, test_dataset = train_test_split(\n",
    "    dataset,\n",
    "    test_perc=test_perc,\n",
    "    cardinality=(num_pos + num_neg),\n",
    "    seed=seed,\n",
    ")\n",
    "test_dataset = test_dataset.batch(1)\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    dataset,\n",
    "    test_perc=val_perc,\n",
    "    cardinality=None,\n",
    "    seed=seed,\n",
    ")\n",
    "val_dataset = (\n",
    "    val_dataset.batch(batch_size)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.batch(batch_size)\n",
    "    .take(16)\n",
    "    .cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=64, reshuffle_each_iteration=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels:\n",
      "\t{1: 677, 0: 203}\n",
      "Validation labels:\n",
      "\t{1: 98, 0: 21}\n",
      "Test labels:\n",
      "\t{0: 30, 1: 81}\n"
     ]
    }
   ],
   "source": [
    "def count_labels(dataset):\n",
    "    \"Return a dictionary of the label count.\"\n",
    "    return dict(Counter(label.numpy()[0] for _, label in dataset.unbatch()))\n",
    "\n",
    "\n",
    "print(f\"Train labels:\\n\\t{count_labels(train_dataset)}\")\n",
    "print(f\"Validation labels:\\n\\t{count_labels(val_dataset)}\")\n",
    "print(f\"Test labels:\\n\\t{count_labels(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"baseline-3dcnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 48, 256, 256, 1)] 0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 48, 256, 256, 32)  896       \n",
      "_________________________________________________________________\n",
      "alpha_dropout_4 (AlphaDropou (None, 48, 256, 256, 32)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 24, 128, 128, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 24, 128, 128, 64)  55360     \n",
      "_________________________________________________________________\n",
      "alpha_dropout_5 (AlphaDropou (None, 24, 128, 128, 64)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 12, 64, 64, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 12, 64, 64, 128)   221312    \n",
      "_________________________________________________________________\n",
      "alpha_dropout_6 (AlphaDropou (None, 12, 64, 64, 128)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 6, 32, 32, 128)    0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling3d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "alpha_dropout_7 (AlphaDropou (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 344,129\n",
      "Trainable params: 344,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.layers.Input(input_shape)\n",
    "\n",
    "x = conv_block(inputs, filters=32, dropout_rate=dropout_rate)\n",
    "x = conv_block(x, filters=64, dropout_rate=dropout_rate)\n",
    "x = conv_block(x, filters=128, dropout_rate=dropout_rate)\n",
    "\n",
    "x = keras.layers.GlobalAveragePooling3D()(x)\n",
    "x = keras.layers.Dense(\n",
    "    512,\n",
    "    kernel_initializer=\"lecun_normal\",\n",
    "    bias_initializer=\"lecun_normal\",\n",
    "    activation=\"selu\",\n",
    ")(x)\n",
    "x = keras.layers.AlphaDropout(dropout_rate)(x)\n",
    "\n",
    "outputs = keras.layers.Dense(\n",
    "    1,\n",
    "    activation=\"sigmoid\",\n",
    ")(x)\n",
    "\n",
    "cnn = keras.Model(inputs, outputs, name=\"baseline-3dcnn\")\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        keras.metrics.TruePositives(name=\"tp\"),\n",
    "        keras.metrics.FalsePositives(name=\"fp\"),\n",
    "        keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "        keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "        keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "        keras.metrics.AUC(name=\"auc\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "    220/Unknown - 50s 229ms/step - loss: 0.6360 - tp: 613.0000 - fp: 181.0000 - tn: 22.0000 - fn: 64.0000 - accuracy: 0.7216 - precision: 0.7720 - recall: 0.9055 - auc: 0.5648\n",
      "Epoch 00001: val_auc improved from -inf to 0.48494, saving model to models/baseline-20201026-155003.h5\n",
      "220/220 [==============================] - 54s 245ms/step - loss: 0.6360 - tp: 613.0000 - fp: 181.0000 - tn: 22.0000 - fn: 64.0000 - accuracy: 0.7216 - precision: 0.7720 - recall: 0.9055 - auc: 0.5648 - val_loss: 0.4845 - val_tp: 98.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.8235 - val_precision: 0.8235 - val_recall: 1.0000 - val_auc: 0.4849\n",
      "Epoch 2/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5768 - tp: 660.0000 - fp: 193.0000 - tn: 10.0000 - fn: 17.0000 - accuracy: 0.7614 - precision: 0.7737 - recall: 0.9749 - auc: 0.5240\n",
      "Epoch 00002: val_auc did not improve from 0.48494\n",
      "220/220 [==============================] - 51s 231ms/step - loss: 0.5768 - tp: 660.0000 - fp: 193.0000 - tn: 10.0000 - fn: 17.0000 - accuracy: 0.7614 - precision: 0.7737 - recall: 0.9749 - auc: 0.5240 - val_loss: 0.4854 - val_tp: 98.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.8235 - val_precision: 0.8235 - val_recall: 1.0000 - val_auc: 0.4842\n",
      "Epoch 3/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5855 - tp: 650.0000 - fp: 195.0000 - tn: 8.0000 - fn: 27.0000 - accuracy: 0.7477 - precision: 0.7692 - recall: 0.9601 - auc: 0.5459\n",
      "Epoch 00003: val_auc did not improve from 0.48494\n",
      "220/220 [==============================] - 51s 232ms/step - loss: 0.5855 - tp: 650.0000 - fp: 195.0000 - tn: 8.0000 - fn: 27.0000 - accuracy: 0.7477 - precision: 0.7692 - recall: 0.9601 - auc: 0.5459 - val_loss: 0.4963 - val_tp: 98.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.8235 - val_precision: 0.8235 - val_recall: 1.0000 - val_auc: 0.4779\n",
      "Epoch 4/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.6769 - tp: 602.0000 - fp: 181.0000 - tn: 22.0000 - fn: 75.0000 - accuracy: 0.7091 - precision: 0.7688 - recall: 0.8892 - auc: 0.5154\n",
      "Epoch 00004: val_auc improved from 0.48494 to 0.51628, saving model to models/baseline-20201026-155003.h5\n",
      "220/220 [==============================] - 51s 234ms/step - loss: 0.6769 - tp: 602.0000 - fp: 181.0000 - tn: 22.0000 - fn: 75.0000 - accuracy: 0.7091 - precision: 0.7688 - recall: 0.8892 - auc: 0.5154 - val_loss: 0.4729 - val_tp: 98.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.8235 - val_precision: 0.8235 - val_recall: 1.0000 - val_auc: 0.5163\n",
      "Epoch 5/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5523 - tp: 654.0000 - fp: 194.0000 - tn: 9.0000 - fn: 23.0000 - accuracy: 0.7534 - precision: 0.7712 - recall: 0.9660 - auc: 0.5857\n",
      "Epoch 00005: val_auc did not improve from 0.51628\n",
      "220/220 [==============================] - 51s 231ms/step - loss: 0.5523 - tp: 654.0000 - fp: 194.0000 - tn: 9.0000 - fn: 23.0000 - accuracy: 0.7534 - precision: 0.7712 - recall: 0.9660 - auc: 0.5857 - val_loss: 0.4979 - val_tp: 98.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.8235 - val_precision: 0.8235 - val_recall: 1.0000 - val_auc: 0.4815\n",
      "Epoch 6/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5456 - tp: 666.0000 - fp: 196.0000 - tn: 7.0000 - fn: 11.0000 - accuracy: 0.7648 - precision: 0.7726 - recall: 0.9838 - auc: 0.5768\n",
      "Epoch 00006: val_auc did not improve from 0.51628\n",
      "220/220 [==============================] - 51s 232ms/step - loss: 0.5456 - tp: 666.0000 - fp: 196.0000 - tn: 7.0000 - fn: 11.0000 - accuracy: 0.7648 - precision: 0.7726 - recall: 0.9838 - auc: 0.5768 - val_loss: 0.4749 - val_tp: 98.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.8235 - val_precision: 0.8235 - val_recall: 1.0000 - val_auc: 0.4978\n",
      "Epoch 7/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5551 - tp: 670.0000 - fp: 198.0000 - tn: 5.0000 - fn: 7.0000 - accuracy: 0.7670 - precision: 0.7719 - recall: 0.9897 - auc: 0.5571\n",
      "Epoch 00007: val_auc did not improve from 0.51628\n",
      "220/220 [==============================] - 51s 231ms/step - loss: 0.5551 - tp: 670.0000 - fp: 198.0000 - tn: 5.0000 - fn: 7.0000 - accuracy: 0.7670 - precision: 0.7719 - recall: 0.9897 - auc: 0.5571 - val_loss: 0.5259 - val_tp: 96.0000 - val_fp: 20.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.8151 - val_precision: 0.8276 - val_recall: 0.9796 - val_auc: 0.4995\n",
      "Epoch 8/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5431 - tp: 664.0000 - fp: 200.0000 - tn: 3.0000 - fn: 13.0000 - accuracy: 0.7580 - precision: 0.7685 - recall: 0.9808 - auc: 0.5894\n",
      "Epoch 00008: val_auc improved from 0.51628 to 0.53523, saving model to models/baseline-20201026-155003.h5\n",
      "220/220 [==============================] - 52s 236ms/step - loss: 0.5431 - tp: 664.0000 - fp: 200.0000 - tn: 3.0000 - fn: 13.0000 - accuracy: 0.7580 - precision: 0.7685 - recall: 0.9808 - auc: 0.5894 - val_loss: 0.4679 - val_tp: 98.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.8235 - val_precision: 0.8235 - val_recall: 1.0000 - val_auc: 0.5352\n",
      "Epoch 9/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5586 - tp: 658.0000 - fp: 194.0000 - tn: 9.0000 - fn: 19.0000 - accuracy: 0.7580 - precision: 0.7723 - recall: 0.9719 - auc: 0.5601\n",
      "Epoch 00009: val_auc improved from 0.53523 to 0.56244, saving model to models/baseline-20201026-155003.h5\n",
      "220/220 [==============================] - 52s 236ms/step - loss: 0.5586 - tp: 658.0000 - fp: 194.0000 - tn: 9.0000 - fn: 19.0000 - accuracy: 0.7580 - precision: 0.7723 - recall: 0.9719 - auc: 0.5601 - val_loss: 0.4774 - val_tp: 98.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.8235 - val_precision: 0.8235 - val_recall: 1.0000 - val_auc: 0.5624\n",
      "Epoch 10/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5832 - tp: 634.0000 - fp: 184.0000 - tn: 19.0000 - fn: 43.0000 - accuracy: 0.7420 - precision: 0.7751 - recall: 0.9365 - auc: 0.5595\n",
      "Epoch 00010: val_auc did not improve from 0.56244\n",
      "220/220 [==============================] - 51s 233ms/step - loss: 0.5832 - tp: 634.0000 - fp: 184.0000 - tn: 19.0000 - fn: 43.0000 - accuracy: 0.7420 - precision: 0.7751 - recall: 0.9365 - auc: 0.5595 - val_loss: 0.6501 - val_tp: 67.0000 - val_fp: 13.0000 - val_tn: 8.0000 - val_fn: 31.0000 - val_accuracy: 0.6303 - val_precision: 0.8375 - val_recall: 0.6837 - val_auc: 0.4971\n",
      "Epoch 11/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5461 - tp: 657.0000 - fp: 191.0000 - tn: 12.0000 - fn: 20.0000 - accuracy: 0.7602 - precision: 0.7748 - recall: 0.9705 - auc: 0.5876\n",
      "Epoch 00011: val_auc did not improve from 0.56244\n",
      "220/220 [==============================] - 51s 231ms/step - loss: 0.5461 - tp: 657.0000 - fp: 191.0000 - tn: 12.0000 - fn: 20.0000 - accuracy: 0.7602 - precision: 0.7748 - recall: 0.9705 - auc: 0.5876 - val_loss: 0.5758 - val_tp: 98.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.8235 - val_precision: 0.8235 - val_recall: 1.0000 - val_auc: 0.5119\n",
      "Epoch 12/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5544 - tp: 651.0000 - fp: 192.0000 - tn: 11.0000 - fn: 26.0000 - accuracy: 0.7523 - precision: 0.7722 - recall: 0.9616 - auc: 0.5798\n",
      "Epoch 00012: val_auc did not improve from 0.56244\n",
      "220/220 [==============================] - 51s 230ms/step - loss: 0.5544 - tp: 651.0000 - fp: 192.0000 - tn: 11.0000 - fn: 26.0000 - accuracy: 0.7523 - precision: 0.7722 - recall: 0.9616 - auc: 0.5798 - val_loss: 0.4840 - val_tp: 98.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.8235 - val_precision: 0.8235 - val_recall: 1.0000 - val_auc: 0.4988\n",
      "Epoch 13/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5360 - tp: 664.0000 - fp: 196.0000 - tn: 7.0000 - fn: 13.0000 - accuracy: 0.7625 - precision: 0.7721 - recall: 0.9808 - auc: 0.5925\n",
      "Epoch 00013: val_auc did not improve from 0.56244\n",
      "220/220 [==============================] - 51s 230ms/step - loss: 0.5360 - tp: 664.0000 - fp: 196.0000 - tn: 7.0000 - fn: 13.0000 - accuracy: 0.7625 - precision: 0.7721 - recall: 0.9808 - auc: 0.5925 - val_loss: 0.4974 - val_tp: 97.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 1.0000 - val_accuracy: 0.8151 - val_precision: 0.8220 - val_recall: 0.9898 - val_auc: 0.5039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5475 - tp: 655.0000 - fp: 191.0000 - tn: 12.0000 - fn: 22.0000 - accuracy: 0.7580 - precision: 0.7742 - recall: 0.9675 - auc: 0.5821\n",
      "Epoch 00014: val_auc did not improve from 0.56244\n",
      "220/220 [==============================] - 51s 232ms/step - loss: 0.5475 - tp: 655.0000 - fp: 191.0000 - tn: 12.0000 - fn: 22.0000 - accuracy: 0.7580 - precision: 0.7742 - recall: 0.9675 - auc: 0.5821 - val_loss: 0.4867 - val_tp: 97.0000 - val_fp: 21.0000 - val_tn: 0.0000e+00 - val_fn: 1.0000 - val_accuracy: 0.8151 - val_precision: 0.8220 - val_recall: 0.9898 - val_auc: 0.5078\n",
      "Epoch 15/1000\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5321 - tp: 671.0000 - fp: 199.0000 - tn: 4.0000 - fn: 6.0000 - accuracy: 0.7670 - precision: 0.7713 - recall: 0.9911 - auc: 0.6048\n",
      "Epoch 00015: val_auc did not improve from 0.56244\n",
      "220/220 [==============================] - 51s 232ms/step - loss: 0.5321 - tp: 671.0000 - fp: 199.0000 - tn: 4.0000 - fn: 6.0000 - accuracy: 0.7670 - precision: 0.7713 - recall: 0.9911 - auc: 0.6048 - val_loss: 0.5172 - val_tp: 92.0000 - val_fp: 20.0000 - val_tn: 1.0000 - val_fn: 6.0000 - val_accuracy: 0.7815 - val_precision: 0.8214 - val_recall: 0.9388 - val_auc: 0.5046\n",
      "Epoch 16/1000\n",
      " 86/220 [==========>...................] - ETA: 29s - loss: 0.5415 - tp: 258.0000 - fp: 73.0000 - tn: 4.0000 - fn: 9.0000 - accuracy: 0.7616 - precision: 0.7795 - recall: 0.9663 - auc: 0.5685"
     ]
    }
   ],
   "source": [
    "monitor_metric = \"val_auc\"\n",
    "\n",
    "start_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "best_checkpoint = f\"models/baseline-{start_time}.h5\"\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    best_checkpoint, monitor=monitor_metric, mode=\"max\", verbose=1, save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=patience, mode=\"max\"\n",
    ")\n",
    "log_dir = f\"logs/baseline-{start_time}\"\n",
    "file_writer = tf.summary.create_file_writer(log_dir)\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=False,\n",
    "    profile_batch=0,\n",
    ")\n",
    "cnn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb],\n",
    ")\n",
    "with file_writer.as_default():\n",
    "    tf.summary.text(\n",
    "        \"Hyperparameters\",\n",
    "        f\"{seed=}; \"\n",
    "        f\"{input_shape=}; \"\n",
    "        f\"{epochs=}; \"\n",
    "        f\"{patience=}; \"\n",
    "        f\"{batch_size=}; \"\n",
    "        f\"{learning_rate=}; \"\n",
    "        f\"{dropout_rate=}; \"\n",
    "        f\"{val_perc=}; \"\n",
    "        f\"{test_perc=}\",\n",
    "        step=0,\n",
    "    )\n",
    "cnn = keras.models.load_model(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn = keras.models.load_model(\"models/baseline-20201025-005657.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.evaluate(test_dataset, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_dataset.skip(0)))\n",
    "prediction = cnn(x, training=False)\n",
    "print(f\"real: {y.numpy()}, prediction: {prediction.numpy()}\")\n",
    "plot_animated_volume(x[0, :], fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_bias(dataset):\n",
    "    \"\"\"Prediction bias is the difference\n",
    "        average_labels - average_predictions\n",
    "    \n",
    "    It should be near zero.\n",
    "    Return the tuple (label_avg, prediction_avg, prediction_bias)\n",
    "    \"\"\"\n",
    "    label_avg = np.mean([label.numpy()[0] for _, label in dataset.unbatch()])\n",
    "\n",
    "    def gen():\n",
    "        for x, _ in dataset:\n",
    "            yield x\n",
    "\n",
    "    x_dataset = (\n",
    "        tf.data.Dataset.from_generator(gen, tf.float32)\n",
    "        .unbatch()\n",
    "        .padded_batch(1, input_shape)\n",
    "    )\n",
    "    prediction_avg = np.mean([cnn(x, training=False).numpy()[0][0] for x in x_dataset])\n",
    "    return label_avg, prediction_avg, np.abs(label_avg - prediction_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, p, b = prediction_bias(test_dataset)\n",
    "print(f\"Labels average: {l}\")\n",
    "print(f\"Predictions average: {p}\")\n",
    "print(f\"Prediction bias: {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
