{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scipy import ndimage\n",
    "\n",
    "from model import conv_block\n",
    "from data import example_to_tensor, normalize, add_channel_axis, train_test_split\n",
    "from plot import plot_slice, plot_animated_volume, grid_plot_slices\n",
    "from config import data_root_dir, seed\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "input_shape = (48, 256, 256, 1)\n",
    "# neg_tfrecord_glob = \"covid-neg/*.tfrecord\"\n",
    "# pos_tfrecord_glob = \"covid-pos/*.tfrecord\"\n",
    "neg_tfrecord_glob = \"CT-[0-1]/*.tfrecord\"\n",
    "pos_tfrecord_glob = \"CT-[3-4]/*.tfrecord\"\n",
    "\n",
    "epochs = 1000\n",
    "patience = 20\n",
    "batch_size = 8\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.5\n",
    "val_perc = 0.12  # percentage from the already splitted training test\n",
    "test_perc = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative samples: 938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: (None, None, None, 1), types: tf.float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tfrecord_fnames = [str(p) for p in Path(data_root_dir).glob(neg_tfrecord_glob)]\n",
    "neg_x = (\n",
    "    tf.data.TFRecordDataset(neg_tfrecord_fnames)\n",
    "    .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(add_channel_axis, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "# num_neg = sum(1 for _ in neg_x)\n",
    "num_neg = 938  # CT-0 + CT-1\n",
    "# num_neg = 250\n",
    "# num_neg = 254\n",
    "print(f\"Number of negative samples: {num_neg}\")\n",
    "neg_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples: 47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: (None, None, None, 1), types: tf.float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tfrecord_fnames = [str(p) for p in Path(data_root_dir).glob(pos_tfrecord_glob)]\n",
    "pos_x = (\n",
    "    tf.data.TFRecordDataset(pos_tfrecord_fnames)\n",
    "    .map(example_to_tensor, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(add_channel_axis, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "# num_pos = sum(1 for _ in pos_x)\n",
    "num_pos = 47  # CT-3 + CT-4\n",
    "# num_pos = 250\n",
    "# num_pos = 856\n",
    "print(f\"Number of positive samples: {num_pos}\")\n",
    "pos_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, None, None, 1), (1,)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_y = tf.data.Dataset.from_tensors(tf.constant([0], dtype=tf.int8)).repeat(num_neg)\n",
    "neg_dataset = tf.data.Dataset.zip((neg_x, neg_y))\n",
    "neg_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((None, None, None, 1), (1,)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_y = tf.data.Dataset.from_tensors(tf.constant([1], dtype=tf.int8)).repeat(num_pos)\n",
    "pos_dataset = tf.data.Dataset.zip((pos_x, pos_y))\n",
    "pos_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def random_rotate(volume, label):\n",
    "    \"Rotate the volume by a random degree\"\n",
    "\n",
    "    def scipy_rotate(volume):\n",
    "        angle = tf.random.uniform(shape=(1,), minval=-180, maxval=180, dtype=tf.int32)[0].numpy()\n",
    "        volume = ndimage.rotate(volume, angle, axes=(1, 2), reshape=False)\n",
    "        volume[volume < 0] = 0\n",
    "        volume[volume > 1] = 1\n",
    "        return volume\n",
    "\n",
    "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "    return augmented_volume, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def random_shift(volume, label):\n",
    "    \"Shift the volume by a few random pixels\"\n",
    "\n",
    "    def scipy_shift(volume):\n",
    "        shift_y, shift_x = tf.random.uniform(shape=(2,), minval=0, maxval=30, dtype=tf.int32).numpy()\n",
    "        volume = ndimage.shift(volume, (0, shift_y, shift_x, 0))\n",
    "        return volume\n",
    "\n",
    "    augmented_volume = tf.numpy_function(scipy_shift, [volume], tf.float32)\n",
    "    return augmented_volume, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def random_contrast(volume, label):\n",
    "    mean = tf.reduce_mean(volume)\n",
    "    contrast_factor = tf.random.uniform(shape=(1,), minval=0, maxval=1, dtype=tf.float32)\n",
    "    augmented_volume = (volume - mean) * contrast_factor + mean\n",
    "    augmented_volume = tf.clip_by_value(augmented_volume, 0, 1)\n",
    "    return augmented_volume, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (<unknown>, (None, 1)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = neg_dataset.concatenate(pos_dataset)\n",
    "dataset, test_dataset = train_test_split(\n",
    "    dataset,\n",
    "    test_perc=test_perc,\n",
    "    cardinality=(num_pos + num_neg),\n",
    "    seed=seed,\n",
    ")\n",
    "test_dataset = test_dataset.batch(1)\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    dataset,\n",
    "    test_perc=val_perc,\n",
    "    cardinality=None,\n",
    "    seed=seed,\n",
    ")\n",
    "val_dataset = (\n",
    "    val_dataset.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.map(random_rotate, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(random_shift, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .cache()  # must be called before shuffle\n",
    "    .shuffle(buffer_size=64, reshuffle_each_iteration=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels:\n",
      "\t{0: 751, 1: 30}\n",
      "Validation labels:\n",
      "\t{0: 96, 1: 10}\n",
      "Test labels:\n",
      "\t{0: 91, 1: 7}\n"
     ]
    }
   ],
   "source": [
    "def count_labels(dataset):\n",
    "    \"Return a dictionary of the label count.\"\n",
    "    return dict(Counter(label.numpy()[0] for _, label in dataset.unbatch()))\n",
    "\n",
    "\n",
    "print(f\"Train labels:\\n\\t{count_labels(train_dataset)}\")\n",
    "print(f\"Validation labels:\\n\\t{count_labels(val_dataset)}\")\n",
    "print(f\"Test labels:\\n\\t{count_labels(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 48, 256, 256, 1)] 0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 48, 256, 256, 32)  896       \n",
      "_________________________________________________________________\n",
      "alpha_dropout (AlphaDropout) (None, 48, 256, 256, 32)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 24, 128, 128, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 24, 128, 128, 64)  55360     \n",
      "_________________________________________________________________\n",
      "alpha_dropout_1 (AlphaDropou (None, 24, 128, 128, 64)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 12, 64, 64, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 12, 64, 64, 128)   221312    \n",
      "_________________________________________________________________\n",
      "alpha_dropout_2 (AlphaDropou (None, 12, 64, 64, 128)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 6, 32, 32, 128)    0         \n",
      "=================================================================\n",
      "Total params: 277,568\n",
      "Trainable params: 277,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 6, 32, 32, 128)]  0         \n",
      "_________________________________________________________________\n",
      "up_sampling3d (UpSampling3D) (None, 12, 64, 64, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 12, 64, 64, 128)   442496    \n",
      "_________________________________________________________________\n",
      "alpha_dropout_3 (AlphaDropou (None, 12, 64, 64, 128)   0         \n",
      "_________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3 (None, 24, 128, 128, 128) 0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 24, 128, 128, 64)  221248    \n",
      "_________________________________________________________________\n",
      "alpha_dropout_4 (AlphaDropou (None, 24, 128, 128, 64)  0         \n",
      "_________________________________________________________________\n",
      "up_sampling3d_2 (UpSampling3 (None, 48, 256, 256, 64)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 48, 256, 256, 32)  55328     \n",
      "_________________________________________________________________\n",
      "alpha_dropout_5 (AlphaDropou (None, 48, 256, 256, 32)  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 48, 256, 256, 1)   33        \n",
      "=================================================================\n",
      "Total params: 719,105\n",
      "Trainable params: 719,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Functional)         (None, 6, 32, 32, 128)    277568    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 48, 256, 256, 1)   719105    \n",
      "=================================================================\n",
      "Total params: 996,673\n",
      "Trainable params: 996,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = keras.models.load_model(\"models/autoencoder-20201029-125142.h5\")\n",
    "encoder = autoencoder.get_layer(\"encoder\")\n",
    "encoder.summary()\n",
    "autoencoder.get_layer(\"decoder\").summary()\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original, y = next(iter(train_dataset.unbatch().batch(1)))\n",
    "print(f\"label: {y}\")\n",
    "encoder_out = autoencoder.get_layer(\"encoder\")(original, training=False)\n",
    "decoder_out = autoencoder.get_layer(\"decoder\")(encoder_out, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animated_volume(original[0, :], fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_index = 20\n",
    "fig, ax = plt.subplots(ncols=3)\n",
    "plot_slice(original[0, :], z_index, ax[0])\n",
    "plot_slice(encoder_out[0, :], 0, ax[1])\n",
    "plot_slice(decoder_out[0, :], z_index, ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"3dcnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Functional)         (None, 6, 32, 32, 128)    277568    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 786432)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               201326848 \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 201,604,673\n",
      "Trainable params: 201,327,105\n",
      "Non-trainable params: 277,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.trainable = False\n",
    "cnn = keras.Sequential(\n",
    "    [\n",
    "        encoder,\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(\n",
    "            256, activation=\"relu\",\n",
    "        ),\n",
    "        keras.layers.Dropout(dropout_rate),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"3dcnn\",\n",
    ")\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        keras.metrics.TruePositives(name=\"tp\"),\n",
    "        keras.metrics.FalsePositives(name=\"fp\"),\n",
    "        keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "        keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "        keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "        keras.metrics.AUC(name=\"auc\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "     98/Unknown - 15s 152ms/step - loss: 4.8087 - tp: 2.0000 - fp: 27.0000 - tn: 724.0000 - fn: 28.0000 - accuracy: 0.9296 - precision: 0.0690 - recall: 0.0667 - auc: 0.5087\n",
      "Epoch 00001: val_auc improved from -inf to 0.50000, saving model to models/tl-20201104-194055.h5\n",
      "98/98 [==============================] - 250s 3s/step - loss: 4.8087 - tp: 2.0000 - fp: 27.0000 - tn: 724.0000 - fn: 28.0000 - accuracy: 0.9296 - precision: 0.0690 - recall: 0.0667 - auc: 0.5087 - val_loss: 2.8985 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000\n",
      "Epoch 2/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1582 - tp: 3.0000 - fp: 25.0000 - tn: 726.0000 - fn: 27.0000 - accuracy: 0.9334 - precision: 0.1071 - recall: 0.1000 - auc: 0.5123\n",
      "Epoch 00002: val_auc did not improve from 0.50000\n",
      "98/98 [==============================] - 34s 342ms/step - loss: 1.1582 - tp: 3.0000 - fp: 25.0000 - tn: 726.0000 - fn: 27.0000 - accuracy: 0.9334 - precision: 0.1071 - recall: 0.1000 - auc: 0.5123 - val_loss: 1.1234 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000\n",
      "Epoch 3/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4075 - tp: 1.0000 - fp: 9.0000 - tn: 742.0000 - fn: 29.0000 - accuracy: 0.9513 - precision: 0.1000 - recall: 0.0333 - auc: 0.5227\n",
      "Epoch 00003: val_auc improved from 0.50000 to 0.52917, saving model to models/tl-20201104-194055.h5\n",
      "98/98 [==============================] - 249s 3s/step - loss: 0.4075 - tp: 1.0000 - fp: 9.0000 - tn: 742.0000 - fn: 29.0000 - accuracy: 0.9513 - precision: 0.1000 - recall: 0.0333 - auc: 0.5227 - val_loss: 0.4037 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5292\n",
      "Epoch 4/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2931 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6502\n",
      "Epoch 00004: val_auc did not improve from 0.52917\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.2931 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6502 - val_loss: 0.5371 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4339\n",
      "Epoch 5/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2658 - tp: 2.0000 - fp: 1.0000 - tn: 750.0000 - fn: 28.0000 - accuracy: 0.9629 - precision: 0.6667 - recall: 0.0667 - auc: 0.6726\n",
      "Epoch 00005: val_auc did not improve from 0.52917\n",
      "98/98 [==============================] - 35s 360ms/step - loss: 0.2658 - tp: 2.0000 - fp: 1.0000 - tn: 750.0000 - fn: 28.0000 - accuracy: 0.9629 - precision: 0.6667 - recall: 0.0667 - auc: 0.6726 - val_loss: 0.6170 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4484\n",
      "Epoch 6/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3236 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.6446  \n",
      "Epoch 00006: val_auc did not improve from 0.52917\n",
      "98/98 [==============================] - 35s 359ms/step - loss: 0.3236 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.6446 - val_loss: 0.3492 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4854\n",
      "Epoch 7/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3494 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6358\n",
      "Epoch 00007: val_auc did not improve from 0.52917\n",
      "98/98 [==============================] - 36s 363ms/step - loss: 0.3494 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6358 - val_loss: 0.3951 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4208\n",
      "Epoch 8/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2206 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.7108\n",
      "Epoch 00008: val_auc improved from 0.52917 to 0.54948, saving model to models/tl-20201104-194055.h5\n",
      "98/98 [==============================] - 251s 3s/step - loss: 0.2206 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.7108 - val_loss: 0.3537 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5495\n",
      "Epoch 9/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2897 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7122\n",
      "Epoch 00009: val_auc did not improve from 0.54948\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.2897 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7122 - val_loss: 1.2765 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5000\n",
      "Epoch 10/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2601 - tp: 1.0000 - fp: 2.0000 - tn: 749.0000 - fn: 29.0000 - accuracy: 0.9603 - precision: 0.3333 - recall: 0.0333 - auc: 0.7998\n",
      "Epoch 00010: val_auc did not improve from 0.54948\n",
      "98/98 [==============================] - 33s 332ms/step - loss: 0.2601 - tp: 1.0000 - fp: 2.0000 - tn: 749.0000 - fn: 29.0000 - accuracy: 0.9603 - precision: 0.3333 - recall: 0.0333 - auc: 0.7998 - val_loss: 0.3664 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5333\n",
      "Epoch 11/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1892 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.8445\n",
      "Epoch 00011: val_auc improved from 0.54948 to 0.56302, saving model to models/tl-20201104-194055.h5\n",
      "98/98 [==============================] - 250s 3s/step - loss: 0.1892 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.8445 - val_loss: 0.6019 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5630\n",
      "Epoch 12/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2066 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7708\n",
      "Epoch 00012: val_auc did not improve from 0.56302\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.2066 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7708 - val_loss: 0.5710 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4484\n",
      "Epoch 13/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 0.1644 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8506\n",
      "Epoch 00013: val_auc did not improve from 0.56302\n",
      "98/98 [==============================] - 32s 323ms/step - loss: 0.1644 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8506 - val_loss: 0.4530 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4849\n",
      "Epoch 14/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1711 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.8503\n",
      "Epoch 00014: val_auc did not improve from 0.56302\n",
      "98/98 [==============================] - 32s 327ms/step - loss: 0.1711 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.8503 - val_loss: 0.5210 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5120\n",
      "Epoch 15/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1487 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.8830\n",
      "Epoch 00015: val_auc did not improve from 0.56302\n",
      "98/98 [==============================] - 35s 359ms/step - loss: 0.1487 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.8830 - val_loss: 0.3898 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5307\n",
      "Epoch 16/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1717 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8335\n",
      "Epoch 00016: val_auc did not improve from 0.56302\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.1717 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8335 - val_loss: 0.4001 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5526\n",
      "Epoch 17/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1555 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8611\n",
      "Epoch 00017: val_auc improved from 0.56302 to 0.57969, saving model to models/tl-20201104-194055.h5\n",
      "98/98 [==============================] - 248s 3s/step - loss: 0.1555 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8611 - val_loss: 0.3711 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5797\n",
      "Epoch 18/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1447 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8679\n",
      "Epoch 00018: val_auc improved from 0.57969 to 0.59063, saving model to models/tl-20201104-194055.h5\n",
      "98/98 [==============================] - 247s 3s/step - loss: 0.1447 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8679 - val_loss: 0.3807 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5906\n",
      "Epoch 19/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1362 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.8916\n",
      "Epoch 00019: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 357ms/step - loss: 0.1362 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.8916 - val_loss: 0.3944 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5578\n",
      "Epoch 20/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1359 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9043\n",
      "Epoch 00020: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.1359 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9043 - val_loss: 0.4399 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5495\n",
      "Epoch 21/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1412 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8969\n",
      "Epoch 00021: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 33s 338ms/step - loss: 0.1412 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8969 - val_loss: 0.4448 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5297\n",
      "Epoch 22/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1341 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9055\n",
      "Epoch 00022: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 32s 324ms/step - loss: 0.1341 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9055 - val_loss: 0.4637 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5167\n",
      "Epoch 23/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1577 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.8670\n",
      "Epoch 00023: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 33s 332ms/step - loss: 0.1577 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.8670 - val_loss: 0.5416 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4885\n",
      "Epoch 24/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1216 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9140\n",
      "Epoch 00024: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 359ms/step - loss: 0.1216 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9140 - val_loss: 0.4939 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5380\n",
      "Epoch 25/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 0.1160 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9237\n",
      "Epoch 00025: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 32s 325ms/step - loss: 0.1160 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9237 - val_loss: 0.4780 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5047\n",
      "Epoch 26/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1024 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9388\n",
      "Epoch 00026: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.1024 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9388 - val_loss: 0.5792 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4609\n",
      "Epoch 27/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1247 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9173\n",
      "Epoch 00027: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.1247 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9173 - val_loss: 0.5250 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5047\n",
      "Epoch 28/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1367 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8845\n",
      "Epoch 00028: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 357ms/step - loss: 0.1367 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8845 - val_loss: 0.4777 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5302\n",
      "Epoch 29/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1332 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9091\n",
      "Epoch 00029: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 32s 322ms/step - loss: 0.1332 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9091 - val_loss: 0.5883 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5406\n",
      "Epoch 30/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1054 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9301\n",
      "Epoch 00030: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.1054 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9301 - val_loss: 0.4405 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5411\n",
      "Epoch 31/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1139 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9271\n",
      "Epoch 00031: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 361ms/step - loss: 0.1139 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9271 - val_loss: 0.4679 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5234\n",
      "Epoch 32/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1122 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9288\n",
      "Epoch 00032: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.1122 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9288 - val_loss: 0.5918 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5500\n",
      "Epoch 33/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1133 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9215\n",
      "Epoch 00033: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.1133 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9215 - val_loss: 0.4578 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5312\n",
      "Epoch 34/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1160 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9227\n",
      "Epoch 00034: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.1160 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9227 - val_loss: 0.4244 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5740\n",
      "Epoch 35/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1272 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9116\n",
      "Epoch 00035: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 32s 329ms/step - loss: 0.1272 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9116 - val_loss: 0.5722 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5458\n",
      "Epoch 36/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1410 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8801\n",
      "Epoch 00036: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.1410 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 751.0000 - fn: 30.0000 - accuracy: 0.9616 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8801 - val_loss: 0.5500 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5380\n",
      "Epoch 37/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1167 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9185\n",
      "Epoch 00037: val_auc did not improve from 0.59063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 32s 324ms/step - loss: 0.1167 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9185 - val_loss: 0.4716 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5396\n",
      "Epoch 38/1000\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.1224 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9141\n",
      "Epoch 00038: val_auc did not improve from 0.59063\n",
      "98/98 [==============================] - 35s 358ms/step - loss: 0.1224 - tp: 1.0000 - fp: 0.0000e+00 - tn: 751.0000 - fn: 29.0000 - accuracy: 0.9629 - precision: 1.0000 - recall: 0.0333 - auc: 0.9141 - val_loss: 0.5632 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 96.0000 - val_fn: 10.0000 - val_accuracy: 0.9057 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5208\n"
     ]
    }
   ],
   "source": [
    "monitor_metric = \"val_auc\"\n",
    "\n",
    "start_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "best_checkpoint = f\"models/tl-{start_time}.h5\"\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    best_checkpoint, monitor=monitor_metric, mode=\"max\", verbose=1, save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=patience, mode=\"max\"\n",
    ")\n",
    "log_dir = f\"logs/tl-{start_time}\"\n",
    "file_writer = tf.summary.create_file_writer(log_dir)\n",
    "with file_writer.as_default():\n",
    "    tf.summary.text(\n",
    "        \"Hyperparameters\",\n",
    "        f\"{seed=}; \"\n",
    "        f\"{input_shape=}; \"\n",
    "        f\"{epochs=}; \"\n",
    "        f\"{patience=}; \"\n",
    "        f\"{batch_size=}; \"\n",
    "        f\"{learning_rate=}; \"\n",
    "        f\"{dropout_rate=}; \"\n",
    "        f\"{val_perc=}; \"\n",
    "        f\"{test_perc=}\",\n",
    "        step=0,\n",
    "    )\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=False,\n",
    "    profile_batch=0,\n",
    ")\n",
    "cnn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb],\n",
    ")\n",
    "cnn = keras.models.load_model(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.models.load_model(\"models/tl-20201104-150014.h5\")\n",
    "cnn.evaluate(val_dataset, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.trainable = True\n",
    "patience = 30\n",
    "learning_rate = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_dataset.unbatch().batch(1)))\n",
    "with tf.GradientTape() as tape:\n",
    "    last_conv_layer = cnn.get_layer(\"encoder\").get_layer(\"conv3d_19\")\n",
    "    iterate = tf.keras.models.Model([cnn.inputs], [cnn.output, last_conv_layer.output])\n",
    "    model_out, last_conv_layer = iterate(x)\n",
    "    class_out = model_out[:, np.argmax(model_out[0])]\n",
    "    grads = tape.gradient(class_out, last_conv_layer)\n",
    "    pooled_grads = keras.backend.mean(grads, axis=(0, 1, 2))\n",
    "  \n",
    "heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        # keras.metrics.TruePositives(name=\"tp\"),\n",
    "        # keras.metrics.FalsePositives(name=\"fp\"),\n",
    "        # keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "        # keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "        keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        # keras.metrics.Precision(name=\"precision\"),\n",
    "        # keras.metrics.Recall(name=\"recall\"),\n",
    "        # keras.metrics.AUC(name=\"auc\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_metric = \"val_accuracy\"\n",
    "\n",
    "start_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "best_checkpoint = f\"models/tl-finetuning-{start_time}.h5\"\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    best_checkpoint, monitor=monitor_metric, mode=\"max\", verbose=1, save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=patience, mode=\"max\"\n",
    ")\n",
    "log_dir = f\"logs/tl-finetuning-{start_time}\"\n",
    "file_writer = tf.summary.create_file_writer(log_dir)\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=False,\n",
    "    profile_batch=0,\n",
    ")\n",
    "cnn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb],\n",
    ")\n",
    "with file_writer.as_default():\n",
    "    tf.summary.text(\n",
    "        \"Hyperparameters\",\n",
    "        f\"{seed=}; \"\n",
    "        f\"{input_shape=}; \"\n",
    "        f\"{epochs=}; \"\n",
    "        f\"{patience=}; \"\n",
    "        f\"{batch_size=}; \"\n",
    "        f\"{learning_rate=}; \"\n",
    "        f\"{dropout_rate=}; \"\n",
    "        f\"{val_perc=}; \"\n",
    "        f\"{test_perc=}\",\n",
    "        step=0,\n",
    "    )\n",
    "cnn = keras.models.load_model(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(val_dataset.unbatch().batch(1).skip(1)))\n",
    "prediction = cnn(x, training=False)\n",
    "print(f\"real: {y.numpy()}, prediction: {prediction.numpy()}\")\n",
    "plot_animated_volume(x[0, :], fps=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
